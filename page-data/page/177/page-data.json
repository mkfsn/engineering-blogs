{"componentChunkName":"component---src-templates-blog-list-tsx","path":"/page/177","result":{"data":{"allPost":{"edges":[{"node":{"ID":429,"Title":"Jackson: More than JSON for Java","Description":"<p>Jackson is a mature and feature-rich open source project that we use, support, and contribute to here at Indeed. As Jackson’s creator and primary maintainer, I want to highlight Jackson’s core competencies, extensions, and challenges in this two-part series. Jackson’s core competency If you’re creating a web service in Java that reads or returns JSON, [&#8230;]</p>\n<p> </p>\n","PublishedAt":"2020-09-23 19:55:48+00:00","OriginURL":"https://engineering.indeedblog.com/blog/2020/09/jackson-more-than-json-for-java/","SourceName":"Indeed"}},{"node":{"ID":625,"Title":"Beyond trivago Tech Pt. 2: Four More Side Projects from Our Developers","Description":"","PublishedAt":"2020-09-22 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2020-09-22-developersideprojectspt-2/","SourceName":"Trivago"}},{"node":{"ID":800,"Title":"Pagination Updates on Our API","Description":"As part of our efforts to improve our APIs, we’re introducing updates on how we paginate over tracks. This only affects developers and apps…","PublishedAt":"2020-09-21 00:00:00+00:00","OriginURL":"https://developers.soundcloud.com/blog/pagination-updates-on-our-api","SourceName":"Soundcloud"}},{"node":{"ID":355,"Title":"The Life-Changing Magic of Tidying Ruby Object Allocations","Description":"<p>Your app is slow. It does not spark joy. This post will use memory allocation profiling tools to discover performance hotspots, even when they're coming from inside a library. We will use this technique with a real-world application to identify a piece of optimizable code in Active Record that ultimately leads to a patch with a substantial impact on page speed.</p>\n\n<p>In addition to the talk, I've gone back and written a full technical recap of each section to revisit it any time you want without going through the video.</p>\n\n<p>I make heavy use of theatrics here, including a Japanese voiceover artist, animoji, and some edited clips of Marie Kondo's Netflix TV show. This recording was done at EuRuKo on a boat. If you've got the time, here's the talk:</p>\n\n<div class=\"embedded-video-wrapper\">\n<iframe src=\"https://www.youtube-nocookie.com/embed/Aczy01drwkg?start=287\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n<ol>\n<li><a href=\"#intro-to-tidying-object-allocations\">Intro to Tidying Object Allocations</a></li>\n<li><a href=\"#tidying-example-1-active-record-respond_to-logic\">Tidying Example 1: Active Record respond_to? logic</a></li>\n<li><a href=\"#performance-and-statistical-significance\">Performance and Statistical Significance</a></li>\n<li><a href=\"#tidying-example-2-converting-strings-to-time-takes-time\">Tidying example 2: Converting strings to time takes time</a></li>\n<li>\n<a href=\"tidying-example-3-lightning-fast-cache-keys\">Tidying Example 3: Lightning fast cache keys</a> </li>\n</ol>\n<h2 class=\"anchored\">\n  <a name=\"intro-to-tidying-object-allocations\" href=\"#intro-to-tidying-object-allocations\">Intro to Tidying Object Allocations</a>\n</h2>\n\n<p>The core premise of this talk is that we all want faster applications. Here I'm making the pitch that you can get significant speedups by focusing on your object allocations. To do that, I'll eventually show you a few real-world cases of PRs I made to Rails along with a \"how-to\" that shows how I used profiling and benchmarking to find and fix the hotspots.</p>\n\n<p>At a high level, the \"tidying\" technique looks like this:</p>\n\n<ol>\n<li>Take all your object allocations and put them in a pile where you can see them</li>\n<li>Consider each one: Does it spark joy?</li>\n<li>Keep only the objects that spark joy</li>\n</ol>\n\n<p>An object sparks joy if it is useful, keeps your code clean, and does not cause performance problems. If an object is absolutely necessary, and removing it causes your code to crash, it sparks joy.</p>\n\n<p>To put object allocations in front of us we'll use:</p>\n\n<ul>\n<li><a href=\"https://github.com/SamSaffron/memory_profiler\">memory_profiler</a></li>\n<li><a href=\"https://github.com/schneems/derailed_benchmarks\">derailed_benchmarks</a></li>\n</ul>\n\n<p>To get a sense of the cost of object allocation, we can benchmark two different ways to perform the same logic. One of these allocates an array while the other does not.</p>\n\n<pre><code class=\"lang-ruby\">require 'benchmark/ips'\n\ndef compare_max(a, b)\n  return a if a &gt; b\n  b\nend\n\ndef allocate_max(a, b)\n  array = [a, b] # &lt;===== Array allocation here\n  array.max\nend\n\nBenchmark.ips do |x|\n  x.report(\"allocate_max\") {\n    allocate_max(1, 2)\n  }\n  x.report(\"compare_max \") {\n    compare_max(1, 2)\n  }\n  x.compare!\nend\n</code></pre>\n\n<p>This gives us the results:</p>\n\n<pre><code class=\"lang-term\">Warming up --------------------------------------\n        allocate_max   258.788k i/100ms\n        compare_max    307.196k i/100ms\nCalculating -------------------------------------\n        allocate_max      6.665M (±14.6%) i/s -     32.090M in   5.033786s\n        compare_max      13.597M (± 6.0%) i/s -     67.890M in   5.011819s\n\nComparison:\n        compare_max : 13596747.2 i/s\n        allocate_max:  6664605.5 i/s - 2.04x  slower\n</code></pre>\n\n<p>In this example, allocating an array is 2x slower than making a direct comparison. It's a truism in most languages that allocating memory or creating objects is slow. In the <code>C</code> programming language, it's a truism that \"malloc is slow.\"</p>\n\n<p>Since we know that allocating in Ruby is slow, we can make our programs faster by removing allocations. As a simplifying assumption, I've found that a decrease in bytes allocated roughly corresponds to performance improvement. For example, if I can reduce the number of bytes allocated by 1% in a request, then on average, the request will have been sped up by about 1%. This assumption helps us benchmark faster as it's much easier to measure memory allocated than it is to repeatedly run hundreds or thousands of timing benchmarks.</p>\n<h2 class=\"anchored\">\n  <a name=\"tidying-example-1-active-record-code-respond_to-code-logic\" href=\"#tidying-example-1-active-record-code-respond_to-code-logic\">Tidying Example 1: Active Record <code>respond_to?</code> logic</a>\n</h2>\n\n<p>Using the target application <a href=\"https://www.codetriage.com\">CodeTriage.com</a> and derailed benchmarks, we get a \"pile\" of memory allocations:</p>\n\n<pre><code class=\"lang-term\">$ bundle exec derailed exec perf:objects\n\nallocated memory by gem\n-----------------------------------\n    227058  activesupport/lib\n    134366  codetriage/app\n    # ...\n\n\nallocated memory by file\n-----------------------------------\n    126489  …/code/rails/activesupport/lib/active_support/core_ext/string/output_safety.rb\n     49448  …/code/codetriage/app/views/layouts/_app.html.slim\n     49328  …/code/codetriage/app/views/layouts/application.html.slim\n     36097  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb\n     25096  …/code/codetriage/app/views/pages/_repos_with_pagination.html.slim\n     24432  …/code/rails/activesupport/lib/active_support/core_ext/object/to_query.rb\n     23526  …/code/codetriage/.gem/ruby/2.5.3/gems/rack-mini-profiler-1.0.0/lib/patches/db/pg.rb\n     21912  …/code/rails/activerecord/lib/active_record/connection_adapters/postgresql_adapter.rb\n     18000  …/code/rails/activemodel/lib/active_model/attribute_set/builder.rb\n     15888  …/code/rails/activerecord/lib/active_record/result.rb\n     14610  …/code/rails/activesupport/lib/active_support/cache.rb\n     11109  …/code/codetriage/.gem/ruby/2.5.3/gems/rack-mini-profiler-1.0.0/lib/mini_profiler/storage/file_store.rb\n      9824  …/code/rails/actionpack/lib/abstract_controller/caching/fragments.rb\n      9360  …/.rubies/ruby-2.5.3/lib/ruby/2.5.0/logger.rb\n      8440  …/code/rails/activerecord/lib/active_record/attribute_methods.rb\n      8304  …/code/rails/activemodel/lib/active_model/attribute.rb\n      8160  …/code/rails/actionview/lib/action_view/renderer/partial_renderer.rb\n      8000  …/code/rails/activerecord/lib/active_record/integration.rb\n      7880  …/code/rails/actionview/lib/action_view/log_subscriber.rb\n      7478  …/code/rails/actionview/lib/action_view/helpers/tag_helper.rb\n      7096  …/code/rails/actionview/lib/action_view/renderer/partial_renderer/collection_caching.rb\n      # ...\n</code></pre>\n\n<p>The <a href=\"https://gist.github.com/schneems/5ed597c85a0a49659413456652a1befc\">full output is massive</a>, so I've truncated it here.</p>\n\n<p>Once you've got your memory in a pile. I like to look at the \"allocated memory\" by file. I start at the top and look at each in turn. In this case, we'll look at this file:</p>\n\n<pre><code class=\"lang-term\">      8440  …/code/rails/activerecord/lib/active_record/attribute_methods.rb\n</code></pre>\n\n<p>Once you have a file you want to look at, you can focus on it in derailed like this:</p>\n\n<pre><code class=\"lang-term\">$ ALLOW_FILES=active_record/attribute_methods.rb \\\n  bundle exec derailed exec perf:objects\n\nallocated memory by file\n-----------------------------------\n      8440  …/code/rails/activerecord/lib/active_record/attribute_methods.rb\n\nallocated memory by location\n-----------------------------------\n      8000  …/code/rails/activerecord/lib/active_record/attribute_methods.rb:270\n       320  …/code/rails/activerecord/lib/active_record/attribute_methods.rb:221\n        80  …/code/rails/activerecord/lib/active_record/attribute_methods.rb:189\n        40  …/code/rails/activerecord/lib/active_record/attribute_methods.rb:187\n</code></pre>\n\n<p>Now we can see exactly where the memory is being allocated in this file. Starting at the top of the locations, I'll work my way down to understand how memory is allocated and used. Looking first at this line:</p>\n\n<pre><code class=\"lang-term\">      8000  …/code/rails/activerecord/lib/active_record/attribute_methods.rb:270\n</code></pre>\n\n<p>We can open this in an editor and navigate to that location:</p>\n\n<pre><code class=\"lang-term\">$ bundle open activerecord\n</code></pre>\n\n<p>In that file, here's the line allocating the most memory:</p>\n\n<pre><code class=\"lang-ruby\">def respond_to?(name, include_private = false)\n  return false unless super\n\n  case name\n  when :to_partial_path\n    name = \"to_partial_path\"\n  when :to_model\n    name = \"to_model\"\n  else\n    name = name.to_s # &lt;=== Line 270 here\n  end\n\n  # If the result is true then check for the select case.\n  # For queries selecting a subset of columns, return false for unselected columns.\n  # We check defined?(@attributes) not to issue warnings if called on objects that\n  # have been allocated but not yet initialized.\n  if defined?(@attributes) &amp;&amp; self.class.column_names.include?(name)\n    return has_attribute?(name)\n  end\n\n  true\nend\n</code></pre>\n\n<p>Here we can see on line 270 that it's allocating a string. But why? To answer that question, we need more context. We need to understand how this code is used. When we call <code>respond_to</code> on an object, we want to know if a method by that name exists. Because Active Record is backed by a database, it needs to see if a column exists with that name.</p>\n\n<p>Typically when you call <code>respond_to</code> you pass in a symbol, for example, <code>user.respond_to?(:email)</code>. But in Active Record, columns are stored as strings. On line 270, we're ensuring that the <code>name</code> value is always a string.</p>\n\n<p>This is the code where name is used:</p>\n\n<pre><code class=\"lang-ruby\">  if defined?(@attributes) &amp;&amp; self.class.column_names.include?(name)\n</code></pre>\n\n<p>Here <code>column_names</code> returns an array of column names, and the <code>include?</code> method will iterate over each until it finds the column with that name, or its nothing (<code>nil</code>).</p>\n\n<p>To determine if we can get rid of this allocation, we have to figure out if there's a way to replace it without allocating memory. We need to refactor this code while maintaining correctness. I decided to add a method that converted the array of column names into a hash with symbol keys and string values:</p>\n\n<pre><code class=\"lang-ruby\"># lib/activerecord/model_schema.rb\ndef symbol_column_to_string(name_symbol) # :nodoc:\n  @symbol_column_to_string_name_hash ||= column_names.index_by(&amp;:to_sym)\n  @symbol_column_to_string_name_hash[name_symbol]\nend\n</code></pre>\n\n<p>This is how you would use it:</p>\n\n<pre><code class=\"lang-ruby\">User.symbol_column_to_string(:email) #=&gt; \"email\"\nUser.symbol_column_to_string(:foo)   #=&gt; nil\n</code></pre>\n\n<p>Since the value that is being returned every time by this method is from the same hash, we can re-use the same string and not have to allocate. The refactored <code>respond_to</code> code ends up looking like this:</p>\n\n<pre><code class=\"lang-ruby\">def respond_to?(name, include_private = false)\n  return false unless super\n\n  # If the result is true then check for the select case.\n  # For queries selecting a subset of columns, return false for unselected columns.\n  # We check defined?(@attributes) not to issue warnings if called on objects that\n  # have been allocated but not yet initialized.\n  if defined?(@attributes)\n    if name = self.class.symbol_column_to_string(name.to_sym)\n      return has_attribute?(name)\n    end\n  end\n\n  true\nend\n</code></pre>\n\n<p>Running our benchmarks, this patch yielded a reduction in memory of 1%. Using code that eventually became <code>derailed exec perf:library</code>, I verified that the patch made end-to-end request/response page speed on CodeTriage 1% faster.</p>\n<h2 class=\"anchored\">\n  <a name=\"performance-and-statistical-significance\" href=\"#performance-and-statistical-significance\">Performance and Statistical Significance</a>\n</h2>\n\n<p>When talking about benchmarks, it's important to talk about statistics and their impact. I talk a bit about this in <a href=\"https://schneems.com/2020/03/17/lies-damned-lies-and-averages-perc50-perc95-ex%0Aplained-for-programmers/\">Lies, Damned Lies, and Averages: Perc50, Perc95 explained for Programmers</a>. Essentially any time you measure a value, there's a chance that it could result from randomness. If you run a benchmark 3 times, it will give you 3 different results. If it shows that it was faster twice and slower once, how can you be certain that the results are because of the change and not random chance?</p>\n\n<p>That's precisely the question that \"statistical significance\" tries to answer. While we can never know, we can make an informed decision. How? Well, if you took a measurement of the same code many times, you would know any variation was the result of randomness. This would give you a distribution of randomness. Then you could use this distribution to understand how likely it is that your change was caused by randomness.</p>\n\n<p>In the talk, I go into detail on the origins of \"Student's T-Test.\" In derailed, I've switched to using Kolmogorov-Smirnov instead. When I ran benchmarks on CodeTriage, I wanted to be sure that my results were valid, so I ran them multiple times and ran Kolmogorov Smirnov on them. This gives me a confidence interval. If my results are in that interval, then I can say with 95% certainty that my results are not the result of random chance i.e., that they're valid and are statistically significant.</p>\n\n<p>If it's not significant, it could mean that the change is too small to detect, that you need more samples, or that there is no difference.</p>\n\n<p>In addition to running a significance check on your change, it's useful to see the distribution. Derailed benchmarks does this for you by default now. Here is a result from <code>derailed exec perf:library</code> used to compare the performance difference of two different commits in a library dependency:</p>\n\n<pre><code class=\"lang-term\">                  Histogram - [winner] \"I am the new commit.\"\n                           ┌                                        ┐\n            [11.2 , 11.28) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 12\n            [11.28, 11.36) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 22\n            [11.35, 11.43) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 30\n            [11.43, 11.51) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 17\n   Time (s) [11.5 , 11.58) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 13\n            [11.58, 11.66) ┤▇▇▇▇▇▇▇ 6\n            [11.65, 11.73) ┤ 0\n            [11.73, 11.81) ┤ 0\n            [11.8 , 11.88) ┤ 0\n                           └                                        ┘\n                                      # of runs in range\n\n\n\n                  Histogram - [loser] \"Old commit\"\n                           ┌                                        ┐\n            [11.2 , 11.28) ┤▇▇▇▇ 3\n            [11.28, 11.36) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 19\n            [11.35, 11.43) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 17\n            [11.43, 11.51) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 25\n   Time (s) [11.5 , 11.58) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 15\n            [11.58, 11.66) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 13\n            [11.65, 11.73) ┤▇▇▇▇ 3\n            [11.73, 11.81) ┤▇▇▇▇ 3\n            [11.8 , 11.88) ┤▇▇▇ 2\n                           └                                        ┘\n                                      # of runs in range\n</code></pre>\n\n<p>The TLDR of this whole section is that in addition to showing my change as being faster, I was also able to show that the improvement was statistically significant.</p>\n<h2 class=\"anchored\">\n  <a name=\"tidying-example-2-converting-strings-to-time-takes-time\" href=\"#tidying-example-2-converting-strings-to-time-takes-time\">Tidying example 2: Converting strings to time takes time</a>\n</h2>\n\n<p>One percent faster is good, but it could be better. Let's do it again. First, get a pile of objects:</p>\n\n<pre><code class=\"lang-term\">$ bundle exec derailed exec perf:objects\n\n# ...\n\nallocated memory by file\n-----------------------------------\n    126489  …/code/rails/activesupport/lib/active_support/core_ext/string/output_safety.rb\n     49448  …/code/codetriage/app/views/layouts/_app.html.slim\n     49328  …/code/codetriage/app/views/layouts/application.html.slim\n     36097  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb\n     25096  …/code/codetriage/app/views/pages/_repos_with_pagination.html.slim\n     24432  …/code/rails/activesupport/lib/active_support/core_ext/object/to_query.rb\n     23526  …/code/codetriage/.gem/ruby/2.5.3/gems/rack-mini-profiler-1.0.0/lib/patches/db/pg.rb\n     21912  …/code/rails/activerecord/lib/active_record/connection_adapters/postgresql_adapter.rb\n     18000  …/code/rails/activemodel/lib/active_model/attribute_set/builder.rb\n     15888  …/code/rails/activerecord/lib/active_record/result.rb\n     14610  …/code/rails/activesupport/lib/active_support/cache.rb\n     11148  …/code/codetriage/.gem/ruby/2.5.3/gems/rack-mini-profiler-1.0.0/lib/mini_profiler/storage/file_store.rb\n      9824  …/code/rails/actionpack/lib/abstract_controller/caching/fragments.rb\n      9360  …/.rubies/ruby-2.5.3/lib/ruby/2.5.0/logger.rb\n      8304  …/code/rails/activemodel/lib/active_model/attribute.rb\n</code></pre>\n\n<p>Zoom in on a file:</p>\n\n<pre><code class=\"lang-term\">     36097  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb\n</code></pre>\n\n<p>Isolate the file:</p>\n\n<pre><code class=\"lang-term\">$ ALLOW_FILE=active_model/type/helpers/time_value.rb \\\n  bundle exec derailed exec perf:objects\n\nTotal allocated: 39617 bytes (600 objects)\nTotal retained:  0 bytes (0 objects)\n\nallocated memory by gem\n-----------------------------------\n     39617  activemodel/lib\n\nallocated memory by file\n-----------------------------------\n     39617  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb\n\nallocated memory by location\n-----------------------------------\n     17317  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb:72\n     12000  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb:74\n      6000  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb:73\n      4300  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb:64\n</code></pre>\n\n<p>We're going to do the same thing by starting to look at the top location:</p>\n\n<pre><code class=\"lang-term\">     17317  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb:72\n</code></pre>\n\n<p>Here's the code:</p>\n\n<pre><code class=\"lang-ruby\">def fast_string_to_time(string)\n if string =~ ISO_DATETIME # &lt;=== line 72 Here\n   microsec = ($7.to_r * 1_000_000).to_i\n   new_time $1.to_i, $2.to_i, $3.to_i, $4.to_i, $5.to_i, $6.to_i, microsec\n end\nend\n</code></pre>\n\n<p>On line 72, we are matching the input string with a regular expression constant. This allocates a lot of memory because each grouped match of the regular expression allocates a new string. To understand if we can make this faster, we have to understand how it's used.</p>\n\n<p>This method takes in a string, then uses a regex to split it into parts, and then sends those parts to the <code>new_time</code> method.</p>\n\n<p>There's not much going on that can be sped up there, but what's happening on this line:</p>\n\n<pre><code class=\"lang-ruby\">   microsec = ($7.to_r * 1_000_000).to_i\n</code></pre>\n\n<p>Here's the regex:</p>\n\n<pre><code class=\"lang-ruby\">ISO_DATETIME = /\\A(\\d{4})-(\\d\\d)-(\\d\\d) (\\d\\d):(\\d\\d):(\\d\\d)(\\.\\d+)?\\z/\n</code></pre>\n\n<p>When I ran the code and output $7 from the regex match, I found that it would contain a string that starts with a dot and then has numbers, for example:</p>\n\n<pre><code class=\"lang-ruby\">puts $7 # =&gt; \".1234567\"\n</code></pre>\n\n<p>This code wants microseconds as an integer, so it turns it into a \"rational\" and then multiplies it by a million and turns it into an integer.</p>\n\n<pre><code class=\"lang-ruby\">($7.to_r * 1_000_000).to_i # =&gt; 1234567\n</code></pre>\n\n<p>You might notice that it looks like we're basically dropping the period and then turning it into an integer. So why not do that directly?</p>\n\n<p>Here's what it looks like:</p>\n\n<pre><code class=\"lang-ruby\">def fast_string_to_time(string)\n  if string =~ ISO_DATETIME\n    microsec_part = $7\n    if microsec_part &amp;&amp; microsec_part.start_with?(\".\") &amp;&amp; microsec_part.length == 7\n      microsec_part[0] = \"\"         # &lt;=== HERE\n      microsec = microsec_part.to_i # &lt;=== HERE\n    else\n      microsec = (microsec_part.to_r * 1_000_000).to_i\n    end\n    new_time $1.to_i, $2.to_i, $3.to_i, $4.to_i, $5.to_i, $6.to_i, microsec\n  end\n</code></pre>\n\n<p>We've got to guard this case by checking for the conditions of our optimization. Now the question is: is this faster?</p>\n\n<p>Here's a microbenchmark:</p>\n\n<pre><code class=\"lang-ruby\">original_string = \".443959\"\n\nrequire 'benchmark/ips'\n\nBenchmark.ips do |x|\n  x.report(\"multiply\") {\n    string = original_string.dup\n    (string.to_r * 1_000_000).to_i\n  }\n  x.report(\"new     \") {\n    string = original_string.dup\n    if string &amp;&amp; string.start_with?(\".\".freeze) &amp;&amp; string.length == 7\n      string[0] = ''.freeze\n      string.to_i\n    end\n  }\n  x.compare!\nend\n\n# Warming up --------------------------------------\n#             multiply   125.783k i/100ms\n#             new        146.543k i/100ms\n# Calculating -------------------------------------\n#             multiply      1.751M (± 3.3%) i/s -      8.805M in   5.033779s\n#             new           2.225M (± 2.1%) i/s -     11.137M in   5.007110s\n\n# Comparison:\n#             new     :  2225289.7 i/s\n#             multiply:  1751254.2 i/s - 1.27x  slower\n</code></pre>\n\n<p>The original code is 1.27x slower. YAY!</p>\n<h3 class=\"anchored\">\n  <a name=\"tidying-example-3-lightning-fast-cache-keys\" href=\"#tidying-example-3-lightning-fast-cache-keys\">Tidying Example 3: Lightning fast cache keys</a>\n</h3>\n\n<p>The last speedup is kind of underwhelming, so you might wonder why I added it. If you remember our first example of optimizing <code>respond_to</code>, it helped to understand the broader context of how it's used. Since this is such an expensive object allocation location, is there an opportunity to call it less or not call it at all?</p>\n\n<p>To find out, I added a <code>puts caller</code> in the code and re-ran it. Here's part of a backtrace:</p>\n\n<pre><code class=\"lang-term\">====================================================================================================\n…/code/rails/activemodel/lib/active_model/type/date_time.rb:25:in `cast_value'\n…/code/rails/activerecord/lib/active_record/connection_adapters/postgresql/oid/date_time.rb:16:in `cast_value'\n…/code/rails/activemodel/lib/active_model/type/value.rb:38:in `cast'\n…/code/rails/activemodel/lib/active_model/type/helpers/accepts_multiparameter_time.rb:12:in `block in initialize'\n…/code/rails/activemodel/lib/active_model/type/value.rb:24:in `deserialize'\n…/.rubies/ruby-2.5.3/lib/ruby/2.5.0/delegate.rb:349:in `block in delegating_block'\n…/code/rails/activerecord/lib/active_record/attribute_methods/time_zone_conversion.rb:8:in `deserialize'\n…/code/rails/activemodel/lib/active_model/attribute.rb:164:in `type_cast'\n…/code/rails/activemodel/lib/active_model/attribute.rb:42:in `value'\n…/code/rails/activemodel/lib/active_model/attribute_set.rb:48:in `fetch_value'\n…/code/rails/activerecord/lib/active_record/attribute_methods/read.rb:77:in `_read_attribute'\n…/code/rails/activerecord/lib/active_record/attribute_methods/read.rb:40:in `__temp__57074616475646f51647'\n…/code/rails/activesupport/lib/active_support/core_ext/object/try.rb:16:in `public_send'\n…/code/rails/activesupport/lib/active_support/core_ext/object/try.rb:16:in `try'\n…/code/rails/activerecord/lib/active_record/integration.rb:99:in `cache_version'\n…/code/rails/activerecord/lib/active_record/integration.rb:68:in `cache_key'\n…/code/rails/activesupport/lib/active_support/cache.rb:639:in `expanded_key'\n…/code/rails/activesupport/lib/active_support/cache.rb:644:in `block in expanded_key'\n…/code/rails/activesupport/lib/active_support/cache.rb:644:in `collect'\n…/code/rails/activesupport/lib/active_support/cache.rb:644:in `expanded_key'\n…/code/rails/activesupport/lib/active_support/cache.rb:608:in `normalize_key'\n…/code/rails/activesupport/lib/active_support/cache.rb:565:in `block in read_multi_entries'\n…/code/rails/activesupport/lib/active_support/cache.rb:564:in `each'\n…/code/rails/activesupport/lib/active_support/cache.rb:564:in `read_multi_entries'\n…/code/rails/activesupport/lib/active_support/cache.rb:387:in `block in read_multi'\n</code></pre>\n\n<p>I followed it backwards until I hit these two places:</p>\n\n<pre><code class=\"lang-term\">…/code/rails/activerecord/lib/active_record/integration.rb:99:in `cache_version'\n…/code/rails/activerecord/lib/active_record/integration.rb:68:in `cache_key'\n</code></pre>\n\n<p>It looks like this expensive code is being called while generating a cache key.</p>\n\n<pre><code class=\"lang-ruby\">def cache_key(*timestamp_names)\n  if new_record?\n    \"#{model_name.cache_key}/new\"\n  else\n    if cache_version &amp;&amp; timestamp_names.none? # &lt;== line 68 here\n      \"#{model_name.cache_key}/#{id}\"\n    else\n      timestamp = if timestamp_names.any?\n        ActiveSupport::Deprecation.warn(&lt;&lt;-MSG.squish)\n          Specifying a timestamp name for #cache_key has been deprecated in favor of\n          the explicit #cache_version method that can be overwritten.\n        MSG\n\n        max_updated_column_timestamp(timestamp_names)\n      else\n        max_updated_column_timestamp\n      end\n\n      if timestamp\n        timestamp = timestamp.utc.to_s(cache_timestamp_format)\n        \"#{model_name.cache_key}/#{id}-#{timestamp}\"\n      else\n        \"#{model_name.cache_key}/#{id}\"\n      end\n    end\n  end\nend\n</code></pre>\n\n<p>On line 68 in the <code>cache_key</code> code it calls <code>cache_version</code>. Here's the code for <code>cache_version</code>:</p>\n\n<pre><code class=\"lang-ruby\">def cache_version # &lt;== line 99 here\n  if cache_versioning &amp;&amp; timestamp = try(:updated_at)\n    timestamp.utc.to_s(:usec)\n  end\nend\n</code></pre>\n\n<p>Here is our culprit:</p>\n\n<pre><code class=\"lang-ruby\">timestamp = try(:updated_at)\n</code></pre>\n\n<p>What is happening is that some database adapters, such as the one for Postgres, returned their values from the database driver as strings. Then active record will lazily cast them into Ruby objects when they are needed. In this case, our time value method is being called to convert the updated timestamp into a time object so we can use it to generate a cache version string.</p>\n\n<p>Here's the value before it's converted:</p>\n\n<pre><code class=\"lang-ruby\">User.first.updated_at_before_type_cast # =&gt; \"2019-04-24 21:21:09.232249\"\n</code></pre>\n\n<p>And here's the value after it's converted:</p>\n\n<pre><code class=\"lang-ruby\">User.first.updated_at.to_s(:usec)      # =&gt; \"20190424212109232249\"\n</code></pre>\n\n<p>Basically, all the code is doing is trimming out the non-integer characters. Like before, we need a guard that our optimization can be applied:</p>\n\n<pre><code class=\"lang-ruby\"># Detects if the value before type cast\n# can be used to generate a cache_version.\n#\n# The fast cache version only works with a\n# string value directly from the database.\n#\n# We also must check if the timestamp format has been changed\n# or if the timezone is not set to UTC then\n# we cannot apply our transformations correctly.\ndef can_use_fast_cache_version?(timestamp)\n  timestamp.is_a?(String) &amp;&amp;\n    cache_timestamp_format == :usec &amp;&amp;\n    default_timezone == :utc &amp;&amp;\n    !updated_at_came_from_user?\nend\n</code></pre>\n\n<p>Then once we're in that state, we can modify the string directly:</p>\n\n<pre><code class=\"lang-ruby\"># Converts a raw database string to `:usec`\n# format.\n#\n# Example:\n#\n#   timestamp = \"2018-10-15 20:02:15.266505\"\n#   raw_timestamp_to_cache_version(timestamp)\n#   # =&gt; \"20181015200215266505\"\n#\n# PostgreSQL truncates trailing zeros,\n# https://github.com/postgres/postgres/commit/3e1beda2cde3495f41290e1ece5d544525810214\n# to account for this we pad the output with zeros\ndef raw_timestamp_to_cache_version(timestamp)\n  key = timestamp.delete(\"- :.\")\n  if key.length &lt; 20\n    key.ljust(20, \"0\")\n  else\n    key\n  end\nend\n</code></pre>\n\n<p>There's some extra logic due to the Postgres truncation behavior linked above. The resulting code to <code>cache_version</code> becomes:</p>\n\n<pre><code class=\"lang-ruby\">def cache_version\n  return unless cache_versioning\n\n  if has_attribute?(\"updated_at\")\n    timestamp = updated_at_before_type_cast\n    if can_use_fast_cache_version?(timestamp)\n      raw_timestamp_to_cache_version(timestamp)\n    elsif timestamp = updated_at\n      timestamp.utc.to_s(cache_timestamp_format)\n    end\n  end\nend\n</code></pre>\n\n<p>That's the opportunity. What's the impact?</p>\n\n<pre><code class=\"lang-term\">Before: Total allocated: 743842 bytes (6626 objects)\nAfter:  Total allocated: 702955 bytes (6063 objects)\n</code></pre>\n\n<p>The bytes reduced is 5% fewer allocations. Which is pretty good. How does it translate to speed?</p>\n\n<p>It turns out that time conversion is very CPU intensive and changing this code makes the target application up to 1.12x faster. This means that if your app previously required 100 servers to run, it can now run with about 88 servers.</p>\n<h2 class=\"anchored\">\n  <a name=\"wrap-up\" href=\"#wrap-up\">Wrap up</a>\n</h2>\n\n<p>Adding together these optimizations and others brings the overall performance improvement to 1.23x or a net reduction of 19 servers. Basically, it's like buying 4 servers and getting 1 for free.</p>\n\n<p>These examples were picked from my changes to the Rails codebase, but you can use them to optimize your applications. The general framework looks like this:</p>\n\n<ul>\n<li>Get a list of all your memory</li>\n<li>Zoom in on a hotspot</li>\n<li>Figure out what is causing that memory to be allocated inside of your code</li>\n<li>Ask if you can refactor your code to not depend on those allocations</li>\n</ul>\n\n<p>If you want to learn more about memory, here are my recommendations:</p>\n\n<ul>\n<li>\n<a href=\"https://www.schneems.com/2019/11/07/why-does-my-apps-memory-usage-grow-asymptotically-over-time/\">Why does my App's Memory Use Grow Over Time?</a>  - Start here, an excellent high-level overview of what causes a system's memory to grow that will help you develop an understanding of how Ruby allocates and uses memory at the application level.</li>\n<li>\n<a href=\"https://www.railsspeed.com\">Complete Guide to Rails Performance (Book)</a> - This book is by Nate Berkopec and is excellent. I recommend it to someone at least once a week.</li>\n<li>\n<a href=\"https://www.sitepoint.com/ruby-uses-memory/\">How Ruby uses memory</a> - This is a lower level look at precisely what \"retained\" and \"allocated\" memory means. It uses small scripts to demonstrate Ruby memory behavior. It also explains why the \"total max\" memory of our system rarely goes down.</li>\n<li>\n<a href=\"https://www.schneems.com/2015/05/11/how-ruby-uses-memory.html\">How Ruby uses memory (Video)</a> - If you're new to the concepts of object allocation, this might be an excellent place to start (you can skip the first story in the video, the rest are about memory). Memory stuff starts at 13 minutes</li>\n<li>\n<a href=\"https://www.schneems.com/2017/04/12/jumping-off-the-memory-cliff/\">Jumping off the Ruby Memory Cliff</a> - Sometimes you might see a 'cliff' in your memory metrics or a saw-tooth pattern. This article explores why that behavior exists and what it means.</li>\n<li>\n<a href=\"https://devcenter.heroku.com/articles/ruby-memory-use\">Ruby Memory Use (Heroku Devcenter article I maintain)</a> - This article focuses on alleviating the symptoms of high memory use.</li>\n<li>\n<a href=\"https://blog.codeship.com/debugging-a-memory-leak-on-heroku/\">Debugging a memory leak on Heroku</a> - TLDR; It's probably not a leak. Still worth reading to see how you can come to the same conclusions yourself. Content is valid for other environments than Heroku. Lots of examples of using the tool <code>derailed_benchmarks</code> (that I wrote).</li>\n<li>\n<a href=\"https://www.youtube.com/watch?v=CS11WIalmPM&amp;feature=emb_title\">The Life-Changing Magic of Tidying Active Record Allocations (Video)</a> - This talk shows how I used tools to track down and eliminate memory allocations in real life. All of the examples are from patches I submitted to Rails, but the process works the same for finding allocations caused by your application logic.</li>\n</ul>\n\n<p><em>Get ahold of Richard and stay up-to-date with Ruby, Rails, and other programming related content through a <a href=\"https://www.schneems.com/mailinglist\">subscription to his mailing list</a>.</em></p>","PublishedAt":"2020-09-16 14:58:00+00:00","OriginURL":"https://blog.heroku.com/tidying-ruby-object-allocations","SourceName":"Heroku"}},{"node":{"ID":492,"Title":"What are the different levels of automation testing? : API Testing","Description":"<p>This is Part 4 of our automation series. Read Part 1 to </p>","PublishedAt":"2020-09-14 05:37:41+00:00","OriginURL":"https://blog.paypay.ne.jp/en/different-levels-of-automation/","SourceName":"Paypay"}},{"node":{"ID":626,"Title":"Beyond trivago Tech Pt.1: Side-Projects from Our Developers","Description":"","PublishedAt":"2020-09-08 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2020-09-08-developersideprojects/","SourceName":"Trivago"}},{"node":{"ID":261,"Title":"Spinnaker @ GIPHY","Description":"Like many companies, GIPHY Engineering has been using Kubernetes for the past several years to help our teams quickly build, compile into containers, and distribute applications to our AWS servers. One of the problems with any Kubernetes distribution is: well, the distribution. There is an amalgamation of tools out there vying for your attention (and, [&#8230;]","PublishedAt":"2020-08-28 16:22:24+00:00","OriginURL":"https://engineering.giphy.com/spinnaker-giphy/","SourceName":"GIPHY"}},{"node":{"ID":720,"Title":"New Webinar features for WebSDK","Description":"","PublishedAt":"2020-08-25 17:16:44+00:00","OriginURL":"https://medium.com/zoom-developer-blog/new-webinar-features-for-websdk-84be8dd3b685?source=rss----4a85731adaff---4","SourceName":"Zoom"}},{"node":{"ID":627,"Title":"How Working as a Product Owner Helped Me Be a Better Engineer (and vice versa)","Description":"","PublishedAt":"2020-08-19 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2020-08-19-howworkingasaproductownerhelpedmebeabett/","SourceName":"Trivago"}},{"node":{"ID":262,"Title":"How GIPHY’s Public API Integrates with gRPC Services","Description":"We always work hard at GIPHY to help people find the right GIF at the right time. Adoption of gRPC helps us continue to keep our services fast, stable, and fault-tolerant as we scale to over 10 billion pieces of content a day. When the GIPHY API, which serves content to our third party integrations, [&#8230;]","PublishedAt":"2020-08-13 21:37:16+00:00","OriginURL":"https://engineering.giphy.com/how-giphys-public-api-integrates-with-grpc-services/","SourceName":"GIPHY"}},{"node":{"ID":493,"Title":"CI for Automation Testing Framework","Description":"<p>This is Part 3 of our automation series. Read Part 1 to </p>","PublishedAt":"2020-08-13 01:32:15+00:00","OriginURL":"https://blog.paypay.ne.jp/en/ci-for-automation-testing-framework/","SourceName":"Paypay"}},{"node":{"ID":801,"Title":"Breaking Loose from Third-Party Lock-In with Custom Refactoring Tools","Description":"Code refactoring is an essential part of the job of software developers. As time goes on, technology evolves, product requirements change…","PublishedAt":"2020-08-11 00:00:00+00:00","OriginURL":"https://developers.soundcloud.com/blog/breaking-loose-from-third-party-lock-in-with-custom-refactoring-tools","SourceName":"Soundcloud"}},{"node":{"ID":494,"Title":"Join us for the First PayPay for Developers Webinar","Description":"<p>Join us for a live webinar, as we present our first integration </p>","PublishedAt":"2020-08-05 03:38:31+00:00","OriginURL":"https://blog.paypay.ne.jp/en/webinar-how-to-integrate-paypay-web-payments/","SourceName":"Paypay"}},{"node":{"ID":356,"Title":"Let's Debug a Node.js Application","Description":"<p>There are always challenges when it comes to debugging applications. Node.js' asynchronous workflows add an extra layer of complexity to this arduous process. Although there have been some updates made to the V8 engine in order to easily access asynchronous stack traces, most of the time, we just get errors on the main thread of our applications, which makes debugging a little bit difficult. As well, when our Node.js applications crash, we usually need to <a href=\"https://www.ibm.com/developerworks/library/wa-ibm-node-enterprise-dump-debug-sdk-nodejs-trs/index.html\">rely on some complicated CLI tooling to analyze the core dumps</a>.</p>\n\n<!-- more -->\n\n<p>In this article, we'll take a look at some easier ways to debug your Node.js applications.</p>\n<h2 class=\"anchored\">\n  <a name=\"logging\" href=\"#logging\">Logging</a>\n</h2>\n\n<p>Of course, no developer toolkit is complete without logging. We tend to place <code>console.log</code> statements all over our code in local development, but this is not a really scalable strategy in production. You would likely need to do some filtering and cleanup, or implement a consistent logging strategy, in order to identify important information from genuine errors.</p>\n\n<p>Instead, to implement a proper log-oriented debugging strategy, use a logging tool like <a href=\"https://github.com/pinojs/pino\">Pino</a> or <a href=\"https://github.com/winstonjs/winston\">Winston</a>. These will allow you to set log levels (<code>INFO</code>, <code>WARN</code>, <code>ERROR</code>), allowing you to print verbose log messages locally and only severe ones for production. You can also stream these logs to aggregators, or other endpoints, like LogStash, Papertrail, or even Slack.</p>\n<h2 class=\"anchored\">\n  <a name=\"working-with-node-inspect-and-chrome-devtools\" href=\"#working-with-node-inspect-and-chrome-devtools\">Working with Node Inspect and Chrome DevTools</a>\n</h2>\n\n<p>Logging can only take us so far in understanding why an application is not working the way we would expect. For sophisticated debugging sessions, we will want to use breakpoints to inspect how our code behaves at the moment it is being executed.</p>\n\n<p>To do this, we can use Node Inspect. Node Inspect is a debugging tool which comes with Node.js. It's actually just an implementation of <a href=\"https://developers.google.com/web/tools/chrome-devtools/\">Chrome DevTools</a> for your program, letting you add breakpoints, control step-by-step execution, view variables, and follow the call stack.</p>\n\n<p>There are a couple of ways to launch Node Inspect, but the easiest is perhaps to just call your Node.js application with the <code>--inspect-brk</code> flag:</p>\n\n<pre><code class=\"bash\">$ node --inspect-brk $your_script_name\n</code></pre>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1595873126-image1.png\" alt=\"Node inspector\"></p>\n\n<p>After launching your program, head to the <code>chrome://inspect</code> URL in your Chrome browser to get to the Chrome DevTools. With Chrome DevTools, you have all of the capabilities you'd normally expect when debugging JavaScript in the browser. One of the nicer tools is <a href=\"https://developers.google.com/web/tools/chrome-devtools/memory-problems\">the ability to inspect memory</a>. You can <a href=\"https://developers.google.com/web/tools/chrome-devtools/memory-problems/heap-snapshots\">take heap snapshots</a> and profile memory usage to understand how memory is being allocated, and potentially, plug any memory leaks.</p>\n<h3 class=\"anchored\">\n  <a name=\"using-a-supported-ide\" href=\"#using-a-supported-ide\">Using a supported IDE</a>\n</h3>\n\n<p>Rather than launching your program in a certain way, many modern IDEs also support debugging Node applications. In addition to having many of the features found in Chrome DevTools, they bring their own features, such as <a href=\"https://code.visualstudio.com/blogs/2018/07/12/introducing-logpoints-and-auto-attach\">creating logpoints</a> and allowing you to create multiple debugging profiles. Check out <a href=\"https://nodejs.org/en/docs/guides/debugging-getting-started/#inspector-clients\">the Node.js' guide on inspector clients</a> for more information on these IDEs.</p>\n<h3 class=\"anchored\">\n  <a name=\"using-ndb\" href=\"#using-ndb\">Using NDB</a>\n</h3>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1595873374-image4.png\" alt=\"NDB\"></p>\n\n<p>Another option is to install <a href=\"https://github.com/GoogleChromeLabs/ndb\">ndb</a>, a standalone debugger for Node.js. It makes use of the same DevTools that are available in the browser, just as an isolated, local debugger. It also has some extra features that aren't available in DevTools. It supports edit-in-place, which means you can make changes to your code and have the updated logic supported directly by the debugger platform. This is very useful for doing quick iterations.</p>\n<h2 class=\"anchored\">\n  <a name=\"post-mortem-debugging\" href=\"#post-mortem-debugging\">Post-Mortem Debugging</a>\n</h2>\n\n<p>Suppose your application crashes due to a catastrophic error, like a memory access error. These may be rare, but they do happen, particularly if your app relies on native code.</p>\n\n<p>To investigate these sorts of issues, you can use <a href=\"https://github.com/nodejs/llnode\">llnode</a>. When your program crashes, <code>llnode</code> can be used to inspect JavaScript stack frames and objects by mapping them to objects on the C/C++ side. In order to use it, you first need a core dump of your program. To do this, you will need to use <code>process.abort</code> instead of <code>process.exit</code> to shut down processes in your code. When you use <code>process.abort</code>, the Node process generates a core dump file on exit.</p>\n\n<p>To better understand what <code>llnode</code> can provide, <a href=\"https://asciinema.org/a/29589\">here is a video</a> which demonstrates some of its capabilities.</p>\n<h2 class=\"anchored\">\n  <a name=\"useful-node-modules\" href=\"#useful-node-modules\">Useful Node Modules</a>\n</h2>\n\n<p>Aside from all of the above, there are also a few third-party packages that we can recommend for further debugging.</p>\n<h3 class=\"anchored\">\n  <a name=\"debug\" href=\"#debug\">debug</a>\n</h3>\n\n<p>The first of these is called, simply enough, <a href=\"https://www.npmjs.com/package/debug\">debug</a>. With debug, you can assign a specific namespace to your log messages, based on a function name or an entire module. You can then selectively choose which messages are printed to the console via a specific environment variable.</p>\n\n<p>For example, here's a Node.js server which is logging several messages from the entire application and middleware stack, like <code>sequelize</code>, <code>express:application</code>, and <code>express:router</code>:</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1595873233-image2.png\" alt=\"Debug module full output\"></p>\n\n<p>If we set the <code>DEBUG</code> environment variable to <code>express:router</code> and start the same program, only the messages tagged as <code>express:router</code> are shown:</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1595873290-image3.png\" alt=\"Debug module filtered output\"></p>\n\n<p>By filtering messages in this way, we can hone in on how a single segment of the application is behaving, without needing to drastically change the logging of the code.</p>\n<h3 class=\"anchored\">\n  <a name=\"trace-and-clarify\" href=\"#trace-and-clarify\">trace and clarify</a>\n</h3>\n\n<p>Two more modules that go together are <a href=\"https://github.com/AndreasMadsen/trace\">trace</a> and <a href=\"https://github.com/AndreasMadsen/clarify\">clarify</a>.</p>\n\n<p><code>trace</code> augments your asynchronous stack traces by providing much more detailed information on the async methods that are being called, a roadmap which Node.js does not provide by default. <code>clarify</code> helps by removing all of the information from stack traces which are specific to Node.js internals. This allows you to concentrate on the function calls that are just specific to your application.</p>\n\n<p>Neither of these modules are recommended for running in production! You should only enable them when debugging issues in your local development environment.</p>\n<h2 class=\"anchored\">\n  <a name=\"find-out-more\" href=\"#find-out-more\">Find out more</a>\n</h2>\n\n<p>If you'd like to follow along with how to use these debugging tools in practice, <a href=\"https://vimeo.com/428003519/f132859d08\">here is a video recording</a> which provides more detail. It includes some live demos of how to narrow in on problems in your code. Or, if you have any other questions, you can find me on Twitter <a href=\"https://twitter.com/julian_duque\">@julian_duque</a>!</p>","PublishedAt":"2020-08-03 16:08:55+00:00","OriginURL":"https://blog.heroku.com/debug-node-applications","SourceName":"Heroku"}},{"node":{"ID":628,"Title":"trivago Tech Check-in: Meet Fabian","Description":"","PublishedAt":"2020-08-03 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2020-08-03-trivagotechcheckinmeetfabian/","SourceName":"Trivago"}},{"node":{"ID":263,"Title":"Fixing Bugs in FFMPEG GIF Encoding","Description":"Here at GIPHY Engineering, we frequently use FFmpeg to resize and reformat GIFs. We generate around 40 different renditions for each GIF uploaded to our platform, so it’s important we do so as efficiently as we can. While FFmpeg is powerful, it was designed for processing MP4 files, and its support for the GIF format [&#8230;]","PublishedAt":"2020-07-23 18:06:00+00:00","OriginURL":"https://engineering.giphy.com/fixing-bugs-in-ffmpeg-gif-encoding/","SourceName":"GIPHY"}},{"node":{"ID":357,"Title":"Ground Control to Major TOML: Why Buildpacks Use a Most Peculiar Format","Description":"<p>YAML files dominate configuration in the cloud native ecosystem. They’re used by Kuberentes, Helm, Tekton, and many other projects to define custom configuration and workflows. But YAML has its oddities, which is why the Cloud Native Buildpacks project chose TOML as its primary configuration format.</p>\n\n<p>TOML is a minimal configuration file format that's easy to read because of its simple semantics. You can learn more about TOML from the <a href=\"https://toml.io/en/\">official documentation</a>, but a simple buildpack TOML file looks like this:</p>\n\n<!-- more -->\n\n<pre><code class=\"lang-toml\">api = \"0.2\"\n\n[buildpack]\nid = \"heroku/maven\"\nversion = \"1.0\"\nname = \"Maven\"\n</code></pre>\n\n<p>Unlike YAML, TOML doesn’t rely on significant whitespace with difficult to read indentation. TOML is designed to be human readable, which is why it favors simple structures. It’s also easy for machines to read and write; you can even append to a TOML file without reading it first, which makes it a great data interchange format. But data interchange and machine readability aren’t the main driver for using TOML in the Buildpacks project; it’s humans.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1595374389-major-toml.png\" alt=\"Blog post illustration\"></p>\n<h2 class=\"anchored\">\n  <a name=\"put-your-helmet-on\" href=\"#put-your-helmet-on\">Put Your Helmet On</a>\n</h2>\n\n<p>The first time you use Buildpacks, you probably won’t need to write a TOML file. Buildpacks are designed to get out of your way, and disappear into the details. That’s why there’s no need for large configuration files like a <a href=\"https://helm.sh/docs/chart_template_guide/values_files/\">Helm values.yaml</a> or a <a href=\"https://kubernetes.io/docs/concepts/configuration/\">Kubernetes pod configuration</a>.</p>\n\n<p>Buildpacks favor convention over configuration, and therefore don’t require complex customizations to tweak the inner workings of its tooling. Instead, Buildpacks detect what to do based on the contents of an application, which means configuration is usually limited to simple properties that are defined by a human.</p>\n\n<p>Buildpacks also favor infrastructure as <em>imperative</em> code (rather than <em>declarative</em>). Buildpacks themselves are functions that run against an application, and are best implemented in higher level languages, which can use libraries and testing.</p>\n\n<p>All of these properties lend to a simple configuration format and schema that doesn’t define complex structures. But that doesn’t mean the decision to use TOML was simple.</p>\n<h2 class=\"anchored\">\n  <a name=\"can-you-hear-me-major-toml\" href=\"#can-you-hear-me-major-toml\">Can You Hear Me, Major TOML?</a>\n</h2>\n\n<p>There are many other formats the Buildpacks project could have used besides YAML or TOML, and the Buildpacks core team considered all of these in the early days of the project.</p>\n\n<p>JSON has simple syntax and semantics that are great for data interchange, but it doesn’t make a great human-readable format; in part because it doesn’t allow for comments. Buildpacks use JSON for machine readable config, like the OCI image metadata. But it shouldn’t be used for anything a human writes. </p>\n\n<p>XML has incredibly powerful properties including schema validation, transformation tools, and rich semantics. It’s great for markup (like HTML) but it's much too heavy of a format for what Buildpacks require.</p>\n\n<p>In the end, the Buildpacks project was comfortable choosing TOML because there was solid prior art (even though the format is somewhat obscure). In the cloud native ecosystem, the <a href=\"https://containerd.io/\">containerd</a> project uses TOML. Additionally, many language ecosystem tools like <a href=\"http://doc.crates.io/\">Cargo</a> (for Rust) and <a href=\"https://python-poetry.org/\">Poetry</a> (for Python) use TOML to configure application dependencies. </p>\n<h2 class=\"anchored\">\n  <a name=\"commencing-countdown-engines-on\" href=\"#commencing-countdown-engines-on\">Commencing Countdown, Engines On</a>\n</h2>\n\n<p>The main disadvantage of TOML is its ubiquity. Tools that parse and query TOML files (something comparable to <code>jq</code>) aren’t readily available, and the format can still be jarring to new users even though it’s fairly simple.</p>\n\n<p>Every trend has to start somewhere, and the Cloud Native Buildpacks project is happy to be one of the projects stepping through the door.</p>\n\n<p>If you want to learn more or have any questions around Cloud Native Buildpacks, we will be hosting a <a href=\"https://community.hackernoon.com/t/cloud-native-buildpacks-ama-with-terence-lee-and-joe-kutner-of-heroku/52494\">Live AMA at Hackernoon</a> on July 28th at 2pm PDT. See you there!</p>","PublishedAt":"2020-07-22 15:08:00+00:00","OriginURL":"https://blog.heroku.com/why-buildpacks-use-toml","SourceName":"Heroku"}},{"node":{"ID":629,"Title":"Google Cloud Workload-Placement-Guide","Description":"","PublishedAt":"2020-07-17 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2020-07-17-workloadplacementguidelines/","SourceName":"Trivago"}},{"node":{"ID":802,"Title":"DeveloperBridge: SoundCloud’s Program for Training People from Diverse Backgrounds to Become Engineers","Description":"DeveloperBridge is a year-long, full-time, paid traineeship program where participants learn from and work with engineering teams at…","PublishedAt":"2020-07-17 00:00:00+00:00","OriginURL":"https://developers.soundcloud.com/blog/developerbridge-training-program","SourceName":"Soundcloud"}},{"node":{"ID":358,"Title":"Making Time to Save You Time: How We Sped Up Time-Related Syscalls on Dynos","Description":"<p>I work on Heroku’s Runtime Infrastructure team, which focuses on most of the underlying compute and containerization here at Heroku. Over the years, we’ve tuned our infrastructure in a number of ways to improve performance of customer dynos and harden security.</p>\n\n<p>We recently received a support ticket from a customer inquiring about poor performance in two <a href=\"https://en.wikipedia.org/wiki/System_call\">system calls</a> (more commonly referred to as syscalls) their application was making frequently: <a href=\"https://manpages.ubuntu.com/manpages/bionic/man2/clock_getres.2.html\"><code>clock_gettime(3)</code></a> and <a href=\"https://manpages.ubuntu.com/manpages/bionic/man2/gettimeofday.2.html\"><code>gettimeofday(2)</code></a>.</p>\n\n<p>In this customer’s case, they were using a tool to do transaction tracing to monitor the performance of their application. This tool made many such system calls to measure how long different parts of their application took to execute. Unfortunately, these two system calls were very slow for them. Every request was impacted waiting for the time to return, slowing down the app for their users.</p>\n\n<p>To help diagnose the problem we first examined our existing clocksource configuration. The clocksource determines how the Linux kernel gets the current time. The kernel attempts to choose the \"best\" clocksource from the sources available. In our case, the kernel was defaulting to the <code>xen</code> clocksource, which seems reasonable at a glance since the EC2 infrastructure that powers Heroku’s Common Runtime and Private Spaces products uses the Xen hypervisor under the hood.</p>\n\n<p>Unfortunately, the version of Xen in use does not support a particular optimization—virtual dynamic shared object (or \"<a href=\"https://lwn.net/Articles/615809/\">vDSO</a>\")—for the two system calls in question. In short, vDSO allows certain operations to be performed entirely in userspace rather than having to context switch into kernelspace by mapping some kernel functionality into the current process. Context switching between userspace and kernelspace is a somewhat expensive operation—it takes a lot of CPU time. Most applications won’t see a large impact from occasional context switches, but when context switches are happening hundreds or thousands of times per web request, they can add up very quickly!</p>\n\n<p>Thankfully, there are often several available clocksources to choose from. The available clocksources depends on a combination of the CPU, the Linux kernel version, and the hardware virtualization software being used. Our research revealed <code>tsc</code> seemed to be the most promising clocksource and would support vDSO. <code>tsc</code> utilizes the <a href=\"https://en.wikipedia.org/wiki/Time_Stamp_Counter\">Time Stamp Counter</a> to determine the System Time.</p>\n\n<p>During our research, we also encountered a few other <a href=\"https://blog.packagecloud.io/eng/2017/03/08/system-calls-are-much-slower-on-ec2/\">blog</a> <a href=\"https://heap.io/blog/engineering/clocksource-aws-ec2-vdso\">posts</a> about TSC. Every source we referenced agreed that non-vDSO accelerated system calls were significantly slower, but there was some disagreement on how safe use of TSC would be. The Wikipedia article linked in the previous paragraph also lists some of these safety concerns. The two primary concerns centered around backwards clock drift that could occur due to: (1) TSC inconsistency that plagued older processors in hyper-threaded or multi-CPU configurations, and (2) when freezing/unfreezing Xen virtual machines. To the first concern, Heroku uses newer Intel CPUs for all dynos that have significantly safer TSC implementations. To the second concern, EC2 instances, which Heroku dynos use, do not utilize freezing/unfreezing today. We decided that <code>tsc</code> would be the best clocksource choice to support vDSO for these system calls without introducing negative side effects.</p>\n\n<p>We were able to confirm using the <code>tsc</code> clocksource enabled vDSO acceleration with the excellent <a href=\"https://github.com/nlynch-mentor/vdsotest\">vdsotest</a> tool (although you can verify your own results using <code>strace</code>). After our internal testing, we deployed the <code>tsc</code> clocksource configuration change to the Heroku Common Runtime and Private Spaces dyno fleet.</p>\n\n<p>While the customer who filed the initial support ticket that led to this change noticed the improvement, the biggest surprise for us was when other customers started inquiring about unexpected performance improvements (which we knew to be a result of this change). It’s always nice for us when our work to solve a problem for a specific customer has a significant positive impact for all customers.</p>\n\n<p>We're glad to be able to make changes like this that benefit all Heroku users. Detailed diagnostic and tuning work like this may not be worth the time investment for an individual engineering team managing their own infrastructure outside of Heroku. Heroku’s scale allows us to identify unique optimization opportunities and invest time into validating and implementing tweaks like this that make apps on Heroku run faster and more reliably.</p>","PublishedAt":"2020-07-16 16:50:00+00:00","OriginURL":"https://blog.heroku.com/clocksource-tuning","SourceName":"Heroku"}},{"node":{"ID":264,"Title":"Modifying FFMPEG to Support Transparent GIFs","Description":"A sticker (left) is just a GIF (right) with transparent pixels. Here at GIPHY, we differentiate between GIFs and Stickers in our business language, as the two products are served to different searches and customers. However, we still use the GIF format to store stickers &#8211; all they really are is GIFs with transparent pixels. [&#8230;]","PublishedAt":"2020-07-09 21:26:26+00:00","OriginURL":"https://engineering.giphy.com/modifying-ffmpeg-to-support-transparent-gifs/","SourceName":"GIPHY"}},{"node":{"ID":359,"Title":"A Fast Car Needs Good Brakes: How We Added Client Rate Throttling to the Platform API Gem","Description":"<p>When API requests are made one-after-the-other they'll quickly hit rate limits and when that happens:</p>\n\n<p></p><blockquote class=\"twitter-tweet tw-align-center\">\n<p lang=\"en\" dir=\"ltr\">If you provide an API client that doesn't include rate limiting, you don't really have an API client. You've got an exception generator with a remote timer.</p>— Richard Schneeman 🤠 Stay Inside (@schneems) <a href=\"https://twitter.com/schneems/status/1138899094137651200?ref_src=twsrc%5Etfw\">June 12, 2019</a>\n</blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n<p>That tweet spawned a discussion that generated a quest to add rate throttling logic to the <a href=\"https://rubygems.org/gems/platform-api\"><code>platform-api</code></a> gem that Heroku maintains for talking to its API in Ruby.</p>\n\n<blockquote>\n<p>If the term \"rate throttling\" is new to you, read <a href=\"https://schneems.com/2020/06/25/rate-limiting-rate-throttling-and-how-they-work-together/\">Rate limiting, rate throttling, and how they work together</a></p>\n</blockquote>\n\n<p>The Heroku API uses <a href=\"https://brandur.org/rate-limiting\">Genetic Cell Rate Algorithm (GCRA) as described by Brandur in this post</a> on the server-side. Heroku's <a href=\"https://devcenter.heroku.com/articles/platform-api-reference#rate-limits\">API docs</a> state:</p>\n\n<blockquote>\n<p>The API limits the number of requests each user can make per hour to protect against abuse and buggy code. Each account has a pool of request tokens that can hold at most 4500 tokens. Each API call removes one token from the pool. Tokens are added to the account pool at a rate of roughly 75 per minute (or 4500 per hour), up to a maximum of 4500. If no tokens remain, further calls will return 429 Too Many Requests until more tokens become available.</p>\n</blockquote>\n\n<p>I needed to write an algorithm that never errored as a result of a 429 response. A \"simple\" solution would be to add a retry to all requests when they see a 429, but that would effectively DDoS the API. I made it a goal for the rate throttling client also to minimize its retry rate. That is, if the client makes 100 requests, and 10 of them are a 429 response that its retry rate is 10%. Since the code needed to be contained entirely in the client library, it needed to be able to function without distributed coordination between multiple clients on multiple machines except for whatever information the Heroku API returned.</p>\n<h2 class=\"anchored\">\n  <a name=\"making-client-throttling-maintainable\" href=\"#making-client-throttling-maintainable\">Making client throttling maintainable</a>\n</h2>\n\n<p>Before we can get into what logic goes into a quality rate throttling algorithm, I want to talk about the process that I used as I think the journey is just as fascinating as the destination.</p>\n\n<p>I initially started wanting to write tests for my rate throttling strategy. I quickly realized that while testing the behavior \"retries a request after a 429 response,\" it is easy to check. I also found that checking for quality \"this rate throttle strategy is better than others\" could not be checked quite as easily. The solution that I came up with was to write a simulator in addition to tests. I would simulate the server's behavior, and then boot up several processes and threads and hit the simulated server with requests to observe the system's behavior.</p>\n\n<p>I initially just output values to the CLI as the simulation ran, but found it challenging to make sense of them all, so I added charting. I found my simulation took too long to run and so I added a mechanism to speed up the simulated time. I used those two outputs to write what I thought was a pretty good rate throttling algorithm. The next task was wiring it up to the <code>platform-api</code> gem.</p>\n\n<p>To help out I paired with <a href=\"https://twitter.com/lolaodelola\">a Heroku Engineer, Lola</a>, we ended up making several PRs to a bunch of related projects, and that's its own story to tell. Finally, the day came where we were ready to get rate throttling into the <code>platform-api</code> gem; all we needed was a review.</p>\n\n<p>Unfortunately, the algorithm I developed from \"watching some charts for a few hours\" didn't make a whole lot of sense, and it was painfully apparent that it wasn't maintainable. While I had developed a good gut feel for what a \"good\" algorithm did and how it behaved, I had no way of solidifying that knowledge into something that others could run with. Imagine someone in the future wants to make a change to the algorithm, and I'm no longer here. The tests I had could prevent them from breaking some expectations, but there was nothing to help them make a better algorithm.</p>\n<h2 class=\"anchored\">\n  <a name=\"the-making-of-an-algorithm\" href=\"#the-making-of-an-algorithm\">The making of an algorithm</a>\n</h2>\n\n<p>At this point, I could explain the approach I had taken to build an algorithm, but I had no way to quantify the \"goodness\" of my algorithm. That's when I decided to throw it all away and start from first principles. Instead of asking \"what would make my algorithm better,\" I asked, \"how would I know a change to my algorithm is better\" and then worked to develop some ways to quantify what \"better\" meant. Here are the goals I ended up coming up with:</p>\n\n<ul>\n<li>Minimize average retry rate: The fewer failed API requests, the better</li>\n<li>Minimize maximum sleep time: Rate throttling involves waiting, and no one wants to wait for too long</li>\n<li>Minimize variance of request count between clients: No one likes working with a greedy co-worker, API clients are no different. No client in the distributed system should be an extended outlier</li>\n<li>Minimize time to clear a large request capacity: As the system changes, clients should respond quickly to changes.</li>\n</ul>\n\n<p>I figured that if I could generate metrics on my rate-throttle algorithm and compare it to simpler algorithms, then I could show why individual decisions were made.</p>\n\n<p>I moved my hacky scripts for my simulation into a separate repo and, rather than relying on watching charts and logs, moved to have my simulation <a href=\"https://github.com/zombocom/rate_throttle_client/blob/master/lib/rate_throttle_client/demo.rb\">produce numbers that could be used to quantify and compare algorithms</a>.</p>\n\n<p>With that work under my belt, I threw away everything I knew about rate-throttling and decided to use science and measurement to guide my way.</p>\n<h2 class=\"anchored\">\n  <a name=\"writing-a-better-rate-throttling-algorithm-with-science-exponential-backoff\" href=\"#writing-a-better-rate-throttling-algorithm-with-science-exponential-backoff\">Writing a better rate-throttling algorithm with science: exponential backoff</a>\n</h2>\n\n<p>Earlier I mentioned that a \"simple\" algorithm would be to retry requests. A step up in complexity and functionality would be to retry requests after an exponential backoff. I coded it up and got some numbers for a simulated 30-minute run (which takes 3 minutes of real-time):</p>\n\n<pre><code>Avg retry rate:      60.08 %\nMax sleep time:      854.89 seconds\nStdev Request Count: 387.82\n\nTime to clear workload (4500 requests, starting_sleep: 1s):\n74.23 seconds\n</code></pre>\n\n<p>Now that we've got baseline numbers, how could we work to minimize any of these values? In my initial exponential backoff model, I multiplied sleep by a factor of 2.0, what would happen if I increased it to 3.0 or decreased it to 1.2?</p>\n\n<p>To find out, I plugged in those values and re-ran my simulations. I found that there was a correlation between retry rate and max sleep value with the backoff factor, but they were inverse. I could lower the retry rate by increasing the factor (to 3.0), but this increased my maximum sleep time. I could reduce the maximum sleep time by decreasing the factor (to 1.2), but it increased my retry rate.</p>\n\n<p>That experiment told me that if I wanted to optimize both retry rate and sleep time, I could not do it via only changing the exponential factor since an improvement in one meant a degradation in the other value.</p>\n\n<p>At this point, we could theoretically do anything, but our metrics judge our success. We could put a cap on the maximum sleep time, for example, we could write code that says \"don't sleep longer than 300 seconds\", but it too would hurt the retry rate. The biggest concern for me in this example is the maximum sleep time, 854 seconds is over 14 minutes which is WAAAYY too long for a single client to be sleeping.</p>\n\n<p>I ended up picking the 1.2 factor to decrease that value at the cost of a worse retry-rate:</p>\n\n<pre><code>Avg retry rate:      80.41 %\nMax sleep time:      46.72 seconds\nStdev Request Count: 147.84\n\nTime to clear workload (4500 requests, starting_sleep: 1s):\n74.33 seconds\n</code></pre>\n\n<p>Forty-six seconds is better than 14 minutes of sleep by a long shot. How could we get the retry rate down?</p>\n<h2 class=\"anchored\">\n  <a name=\"incremental-improvement-exponential-sleep-with-a-gradual-decrease\" href=\"#incremental-improvement-exponential-sleep-with-a-gradual-decrease\">Incremental improvement: exponential sleep with a gradual decrease</a>\n</h2>\n\n<p>In the exponential backoff model, it backs-off once it sees a 429, but as soon as it hits a success response, it doesn't sleep at all. One way to reduce the retry-rate would be to assume that once a request had been rate-throttled, that future requests would need to wait as well. Essentially we would make the sleep value \"sticky\" and sleep before all requests. If we only remembered the sleep value, our rate throttle strategy wouldn't be responsive to any changes in the system, and it would have a poor \"time to clear workload.\" Instead of only remembering the sleep value, we can gradually reduce it after every successful request. This logic is very similar to <a href=\"https://en.wikipedia.org/wiki/TCP_congestion_control#Slow_start\">TCP slow start</a>.</p>\n\n<p>How does it play out in the numbers?</p>\n\n<pre><code>Avg retry rate:      40.56 %\nMax sleep time:      139.91 seconds\nStdev Request Count: 867.73\n\nTime to clear workload (4500 requests, starting_sleep: 1s):\n115.54 seconds\n</code></pre>\n\n<p>Retry rate did go down by about half. Sleep time went up, but it's still well under the 14-minute mark we saw earlier. But there's a problem with a metric I've not talked about before, the \"stdev request count.\" It's easier to understand if you look at a chart to see what's going on:</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1594232091-ExponentialBackoff.png\" alt=\"Exponential sleep with gradual decrease chart\"></p>\n\n<p>Here you can see one client is sleeping a lot (the red client) while other clients are not sleeping at all and chewing through all the available requests at the bottom. Not all the clients are behaving equitably. This behavior makes it harder to tune the system.</p>\n\n<p>One reason for this inequity is that all clients are decreasing by the same constant value for every successful request. For example, let's say we have a client A that is sleeping for 44 seconds, and client B that is sleeping for 11 seconds and both decrease their sleep value by 1 second after every request. If both clients ran for 45 seconds, it would look like this:</p>\n\n<pre><code>Client A) Sleep 44 (Decrease value: 1)\nClient B) Sleep 11 (Decrease value: 1)\nClient B) Sleep 10 (Decrease value: 1)\nClient B) Sleep  9 (Decrease value: 1)\nClient B) Sleep  8 (Decrease value: 1)\nClient B) Sleep  7 (Decrease value: 1)\nClient A) Sleep 43 (Decrease value: 1)\n</code></pre>\n\n<p>So while client A has decreased by 1 second total, client B has reduced by 4 seconds total, since it is firing 4x as fast (i.e., it's sleep time is 4x lower). So while the decrease rate is equal, it is not equitable. Ideally, we would want all clients to decrease at the same rate.</p>\n<h2 class=\"anchored\">\n  <a name=\"all-clients-created-equal-exponential-increase-proportional-decrease\" href=\"#all-clients-created-equal-exponential-increase-proportional-decrease\">All clients created equal: exponential increase proportional decrease</a>\n</h2>\n\n<p>Since clients cannot communicate with each other in our distributed system, one way to guaranteed proportional decreases is to use the sleep value in the decrease amount:</p>\n\n<pre><code>decrease_value = (sleep_time) / some_value\n</code></pre>\n\n<p>Where <code>some_value</code> is a magic number. In this scenario the same clients A and B running for 45 seconds would look like this with a value of 100:</p>\n\n<pre><code>Client A) Sleep 44\nClient B) Sleep 11\nClient B) Sleep 10.89 (Decrease value: 11.00/100 = 0.1100)\nClient B) Sleep 10.78 (Decrease value: 10.89/100 = 0.1089)\nClient B) Sleep 10.67 (Decrease value: 10.78/100 = 0.1078)\nClient B) Sleep 10.56 (Decrease value: 10.67/100 = 0.1067)\nClient A) Sleep 43.56 (Decrease value: 44.00/100 = 0.4400)\n</code></pre>\n\n<p>Now client A has had a decrease of 0.44, and client B has had a reduction of 0.4334 (11 seconds - 10.56 seconds), which is a lot more equitable than before. Since <code>some_value</code> is tunable, I wanted to use a larger number so that the retry rate would be lower than 40%. I chose 4500 since that's the maximum number of requests in the GCRA bucket for Heroku's API.</p>\n\n<p>Here's what the results looked like:</p>\n\n<pre><code>Avg retry rate:      3.66 %\nMax sleep time:      17.31 seconds\nStdev Request Count: 101.94\n\nTime to clear workload (4500 requests, starting_sleep: 1s):\n551.10 seconds\n</code></pre>\n\n<p>The retry rate went WAAAY down, which makes sense since we're decreasing slower than before (the constant decrease value previously was 0.8). Stdev went way down as well. It's about 8x lower. Surprisingly the max sleep time went down as well. I believe this to be a factor of a decrease in the number of required exponential backoff events. Here's what this algorithm looks like:</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1594232161-ExponentialIncreaseProportionalDecrease.png\" alt=\"Exponential increase proportional decrease chart\"></p>\n\n<p>The only problem here is that the \"time to clear workload\" is 5x higher than before. What exactly is being measured here? In this scenario, we're simulating a cyclical workflow where clients are running under high load, then go through a light load, and then back to a high load. The simulation starts all clients with a sleep value, but the server's rate-limit is reset to 4500. The time is how long it takes the client to clear all 4500 requests.</p>\n\n<p>What this metric of 551 seconds is telling me is that this strategy is not very responsive to a change in the system. To illustrate this problem, I ran the same algorithm starting each client at 8 seconds of sleep instead of 1 second to see how long it would take to trigger a rate limit:</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1594143623-CleanShot%202020-07-07%20at%2010.39.35%402x.png\" alt=\"Exponential increase proportional decrease chart 7-hour torture test\"></p>\n\n<p>The graph shows that it takes about 7 hours to clear all these requests, which is not good. What we need is a way to clear requests faster when there are more requests.</p>\n<h2 class=\"anchored\">\n  <a name=\"the-only-remaining-option-exponential-increase-proportional-remaining-decrease\" href=\"#the-only-remaining-option-exponential-increase-proportional-remaining-decrease\">The only remaining option: exponential increase proportional remaining decrease</a>\n</h2>\n\n<p>When you make a request to the Heroku API, it tells you how many requests you have left remaining in your bucket in a header. Our problem with the \"proportional decrease\" is mostly that when there are lots of requests remaining in the bucket, it takes a long time to clear them (if the prior sleep rate was high, such as in a varying workload). To account for this, we can decrease the sleep value quicker when the remaining bucket is full and slower when the remaining bucket is almost empty. To express that in an expression, it might look like this:</p>\n\n<pre><code>decrease_value = (sleep_time * request_count_remaining) / some_value\n</code></pre>\n\n<p>In my case, I chose <code>some_value</code> to be the maximum number of requests possible in a bucket, which is 4500. You can imagine a scenario where workers were very busy for a period and being rate limited. Then no jobs came in for over an hour - perhaps the workday was over, and the number of requests remaining in the bucket re-filled to 4500. On the next request, this algorithm would reduce the sleep value by itself since 4500/4500 is one:</p>\n\n<pre><code>decrease_value = sleep_time * 4500 / 4500\n</code></pre>\n\n<p>That means it doesn't matter how immense the sleep value is, it will adjust fairly quickly to a change in workload. Good in theory, how does it perform in the simulation?</p>\n\n<pre><code>Avg retry rate:      3.07 %\nMax sleep time:      17.32 seconds\nStdev Request Count: 78.44\n\nTime to clear workload (4500 requests, starting_sleep: 1s):\n84.23 seconds\n</code></pre>\n\n<p>This rate throttle strategy performs very well on all metrics. It is the best (or very close) to several metrics. Here's a chart:</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1594232264-ExponentialIncreaseProportionalRemainingDecrease.png\" alt=\"Exponential increase proportional remaining decrease chart\"></p>\n\n<p>This strategy is the \"winner\" of my experiments and the algorithm that I  chose to go into the <code>platform-api</code> gem.</p>\n<h2 class=\"anchored\">\n  <a name=\"my-original-solution\" href=\"#my-original-solution\">My original solution</a>\n</h2>\n\n<p>While I originally built this whole elaborate scheme to prove how my solution was optimal, I did something by accident. By following a scientific and measurement-based approach, I accidentally found a simpler solution that performed better than my original answer. Which I'm happier about, it shows that the extra effort was worth it. To \"prove\" what I found by observation and tinkering could be not only quantified by numbers but improved upon is fantastic.</p>\n\n<p>While my original solution had some scripts and charts, this new solution has tests covering the behavior of the simulation and charting code. My initial solution was very brittle. I didn't feel very comfortable coming back and making changes to it; this new solution and the accompanying support code is a joy to work with. My favorite part though is that now if anyone asks me, \"what about trying <x>\" or \"have you considered <y>\" is that I can point them at <a href=\"https://github.com/zombocom/rate_throttle_client\">my rate client throttling library</a>, they have all the tools to implement their idea, test it, and report back with a swift feedback loop.</y></x></p>\n<h2 class=\"anchored\">\n  <a name=\"code-gem-39-platform-api-39-39-gt-3-0-39-code\" href=\"#code-gem-39-platform-api-39-39-gt-3-0-39-code\"><code>gem 'platform-api', '~&gt; 3.0'</code></a>\n</h2>\n\n<p>While I mostly wanted to talk about the process of writing rate-throttling code, this whole thing started from a desire to get client rate-throttling into the <code>platform-api</code> gem. Once I did the work to prove my solution was reasonable, we worked on a rollout strategy. We released a version of the gem in a minor bump with rate-throttling available, but with a \"null\" strategy that would preserve existing behavior. This release strategy allowed us to issue a warning to anyone depending on the original behavior. Then we released a major version with the rate-throttling strategy enabled by default. We did this first with \"pre\" release versions and then actual versions to be extra safe.</p>\n\n<p>So far, the feedback has been overwhelming that no one has noticed. We didn't cause any significant breaks or introduce any severe disfunction to any applications. If you've not already, I invite you to upgrade to 3.0.0+ of the <code>platform-api</code> gem and give it a spin. I would love to hear your feedback.</p>\n\n<p><em>Get ahold of Richard and stay up-to-date with Ruby, Rails, and other programming related content through a <a href=\"https://www.schneems.com/mailinglist\">subscription to his mailing list</a>.</em></p>","PublishedAt":"2020-07-07 20:30:00+00:00","OriginURL":"https://blog.heroku.com/rate-throttle-api-client","SourceName":"Heroku"}},{"node":{"ID":630,"Title":"Interview for BrowserStack's Breakpoint 2020 Conference","Description":"","PublishedAt":"2020-07-03 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2020-07-03-browsertack/","SourceName":"Trivago"}},{"node":{"ID":803,"Title":"Changing the Interview Process during Remote Working","Description":"Please also see Part 1: Rethinking the Backend Engineering Interview Take-Home Challenge and Part 2: The Recruiting Perspective and Results…","PublishedAt":"2020-06-30 00:00:00+00:00","OriginURL":"https://developers.soundcloud.com/blog/changing-interview-process-during-remote-working","SourceName":"Soundcloud"}},{"node":{"ID":804,"Title":"Technical Interview Reform, Part 2: The Recruiting Perspective and Results","Description":"Please also see Part 1: Rethinking the Backend Engineering Interview Take-Home Challenge Among the engineering groups at SoundCloud, backend…","PublishedAt":"2020-06-26 00:00:00+00:00","OriginURL":"https://developers.soundcloud.com/blog/backend-code-challenge-recruiting-perspective-and-results","SourceName":"Soundcloud"}},{"node":{"ID":805,"Title":"Technical Interview Reform, Part 1: Rethinking the Backend Engineering Interview Take-Home Challenge","Description":"Most SoundCloud backend engineers have good feelings about the old backend engineering take-home challenge. It’s commonly been characterized…","PublishedAt":"2020-06-25 00:00:00+00:00","OriginURL":"https://developers.soundcloud.com/blog/rethinking-the-backend-code-challenge","SourceName":"Soundcloud"}},{"node":{"ID":360,"Title":"Building a GraphQL API in JavaScript","Description":"<p>Over the last few years, <a href=\"https://graphql.org/\">GraphQL</a> has emerged as a very popular API specification that focuses on making data fetching easier for clients, whether the clients are a front-end or a third-party.</p>\n\n<p>In a traditional REST-based API approach, the client makes a request, and the server dictates the response:</p>\n\n<pre><code class=\"lang-term\">$ curl https://api.heroku.space/users/1\n\n{\n  \"id\": 1,\n  \"name\": \"Luke\",\n  \"email\": \"luke@heroku.space\",\n  \"addresses\": [\n    {\n    \"street\": \"1234 Rodeo Drive\",\n    \"city\": \"Los Angeles\",\n    \"country\": \"USA\"\n    }\n  ]\n}\n</code></pre>\n\n<p>But, in GraphQL, the client determines precisely the data it wants from the server. For example, the client may want only the user's name and email, and none of the address information:</p>\n\n<pre><code class=\"lang-term\">$ curl -X POST https://api.heroku.space/graphql -d '\nquery {\n  user(id: 1) {\n    name\n    email\n  }\n}\n'\n\n{\n  \"data\":\n    {\n    \"name\": \"Luke\",\n    \"email\": \"luke@heroku.space\"\n    }\n}\n</code></pre>\n\n<p>With this new paradigm, clients can make more efficient queries to a server by trimming down the response to meet their needs. For single-page apps (SPAs) or other front-end heavy client-side applications, this speeds up rendering time by reducing the payload size. However, as with any framework or language, GraphQL has its trade-offs. In this post, we'll take a look at some of the pros and cons of using GraphQL as a query language for APIs, as well as how to get started building an implementation.</p>\n<h2 class=\"anchored\">\n  <a name=\"why-would-you-choose-graphql\" href=\"#why-would-you-choose-graphql\">Why would you choose GraphQL?</a>\n</h2>\n\n<p>As with any technical decision, it's important to understand what advantages GraphQL offers to your project, rather than simply choosing it because it's a buzzword.</p>\n\n<p>Consider a SaaS application that uses an API to connect to a remote database; you'd like to render a user's profile page. You might need to make one API <code>GET</code> call to fetch information about the user, like their name or email. You might then need to make another API call to fetch information about the address, which is stored in a different table. As the application evolves, because of the way it's architected, you might need to continue to make more API calls to different locations. While each of these API calls can be done asynchronously, you must also handle their responses, whether there's an error, a network timeout, or even pausing the page render until all the data is received. As noted above, the payloads from these responses might be more than necessary to render your current pages. And each API call has network latency and the total latencies added up can be substantial. </p>\n\n<p>With GraphQL, instead of making several API calls, like <code>GET /user/:id</code> and <code>GET /user/:id/addresses</code>, you make one API call and submit your query to a single endpoint:</p>\n\n<pre><code class=\"lang-graphql\">query {\n  user(id: 1) {\n    name\n    email\n    addresses {\n    street\n    city\n    country\n    }\n  }\n}\n</code></pre>\n\n<p>GraphQL, then, gives you just one endpoint to query for all the domain logic that you need. If your application grows, and you find yourself adding more data stores to your architecture—PostgreSQL might be a good place to store user information, while Redis might be good for other kinds—a single call to a GraphQL endpoint will resolve all of these disparate locations and respond to a client with the data they requested.</p>\n\n<p>If you're unsure of the needs of your application and how data will be stored in the future, GraphQL can prove useful here, too. To modify a query, you'd only need to add the name of the field you want:</p>\n\n<pre><code class=\"lang-diff\">    addresses {\n      street\n+     apartmentNumber   # new information\n      city\n      country\n    }\n</code></pre>\n\n<p>This vastly simplifies the process of evolving your application over time.</p>\n<h2 class=\"anchored\">\n  <a name=\"defining-a-graphql-schema\" href=\"#defining-a-graphql-schema\">Defining a GraphQL schema</a>\n</h2>\n\n<p>There are GraphQL server implementations in a variety of programming languages, but before you get started, you'll need to identify the objects in your business domain, as with any API. Just as a REST API might use something like <a href=\"https://json-schema.org/\">JSON schema</a>, GraphQL defines its schema using SDL, or <a href=\"https://graphql.org/learn/schema/\">Schema Definition Language</a>, an idempotent way to describe all the objects and fields available by your GraphQL API. The general format for an SDL entry looks like this:</p>\n\n<pre><code class=\"lang-graphql\">type $OBJECT_TYPE {\n  $FIELD_NAME($ARGUMENTS): $FIELD_TYPE\n}\n</code></pre>\n\n<p>Let's build on our earlier example by defining what entries for the user and address might look like:</p>\n\n<pre><code class=\"lang-graphql\">type User {\n  name:     String\n  email:    String\n  addresses:   [Address]\n}\n\ntype Address {\n  street:   String\n  city:     String\n  country:  String\n}\n</code></pre>\n\n<p><code>User</code> defines two <code>String</code> fields called <code>name</code> and <code>email</code>. It also includes a field called <code>addresses</code>, which is an array of <code>Address</code> objects. <code>Address</code> also defines a few fields of its own. (By the way, there's more to a GraphQL <a href=\"https://graphql.org/learn/schema/\">schema</a> than just objects, fields, and scalar types. You can also incorporate interfaces, unions, and arguments, to build more complex models, but we won’t cover those for this post.)</p>\n\n<p>There's one more type we need to define, which is the entry point to our GraphQL API. You'll remember that earlier, we said a GraphQL query looked like this:</p>\n\n<pre><code class=\"lang-graphql\">query {\n  user(id: 1) {\n    name\n    email\n  }\n}\n</code></pre>\n\n<p>That <code>query</code> field belongs to a special reserved type called <code>Query</code>. This specifies the main entry point to fetching objects. (There’s also a <code>Mutation</code> type for modifying objects.) Here, we define a <code>user</code> field, which returns a <code>User</code> object, so our schema needs to define this too:</p>\n\n<pre><code class=\"lang-graphql\">type Query {\n  user(id: Int!): User\n}\n\ntype User { ... }\ntype Address { ... }\n</code></pre>\n\n<p>Arguments on a field are a comma-separated list, which takes the form of <code>$NAME: $TYPE</code>. The <code>!</code> is GraphQL's way of denoting that the argument is required—omitting means it's optional.</p>\n\n<p>Depending on your language of choice, the process of incorporating this schema into your server varies, but in general, consuming this information as a string is enough. Node.js has <a href=\"https://www.npmjs.com/package/graphql\">the <code>graphql</code> package</a> to prepare a GraphQL schema, but we're going to use <a href=\"https://www.npmjs.com/package/graphql-tools\">the <code>graphql-tools</code> package</a> instead, because it provides a few more niceties. Let's import the package and read our type definitions in preparation for future development:</p>\n\n<pre><code class=\"lang-javascript\">const fs = require('fs')\nconst { makeExecutableSchema } = require(\"graphql-tools\");\n\nlet typeDefs = fs.readFileSync(\"schema.graphql\", {\n  encoding: \"utf8\",\n  flag: \"r\",\n});\n</code></pre>\n<h2 class=\"anchored\">\n  <a name=\"setting-up-resolvers\" href=\"#setting-up-resolvers\">Setting up resolvers</a>\n</h2>\n\n<p>A schema sets up the ways in which queries can be constructed but establishing a schema to define your data model is just one part of the GraphQL specification. The other portion deals with actually fetching the data. This is done through the use of <a href=\"https://graphql.org/learn/execution/#root-fields-resolvers\"><em>resolvers</em></a>. A resolver is a function that returns a field's underlying value.</p>\n\n<p>Let's take a look at how you might implement resolvers in Node.js. The intent is to solidify concepts around how resolvers operate in conjunction with schemas, so we won't go into too much detail around how the data stores are set up. In the \"real world\", we might establish a database connection with something like <a href=\"https://knexjs.org/\">knex</a>. For now, let's just set up some dummy data:</p>\n\n<pre><code class=\"lang-javascript\">const users = {\n  1: {\n    name: \"Luke\",\n    email: \"luke@heroku.space\",\n    addresses: [\n    {\n          street: \"1234 Rodeo Drive\",\n          city: \"Los Angeles\",\n          country: \"USA\",\n    },\n    ],\n  },\n  2: {\n    name: \"Jane\",\n    email: \"jane@heroku.space\",\n    addresses: [\n    {\n          street: \"1234 Lincoln Place\",\n          city: \"Brooklyn\",\n          country: \"USA\",\n    },\n    ],\n  },\n};\n</code></pre>\n\n<p>GraphQL resolvers in Node.js amount to an Object with the key as the name of the field to be retrieved, and the value being a function that returns the data. Let's start with a barebones example of the initial <code>user</code> lookup by id:</p>\n\n<pre><code class=\"lang-javascript\">const resolvers = {\n  Query: {\n    user: function (parent, { id }) {\n      // user lookup logic\n    },\n  },\n}\n</code></pre>\n\n<p>This resolver takes two arguments: an object representing the parent (which in the initial root query is often unused), and a JSON object containing the arguments passed to your field. Not every field will have arguments, but in this case, we will, because we need to retrieve our user by their ID. The rest of the function is straightforward:</p>\n\n<pre><code class=\"lang-javascript\">const resolvers = {\n  Query: {\n    user: function (_, { id }) {\n      return users[id];\n    },\n  }\n}\n</code></pre>\n\n<p>You'll notice that we didn't explicitly define a resolver for <code>User</code> or <code>Addresses</code>. The <code>graphql-tools</code> package is intelligent enough to automatically map these for us. We can override these if we choose, but with our type definitions and resolvers now defined, we can build our complete schema:</p>\n\n<pre><code class=\"lang-javascript\">const schema = makeExecutableSchema({ typeDefs, resolvers });\n</code></pre>\n<h2 class=\"anchored\">\n  <a name=\"running-the-server\" href=\"#running-the-server\">Running the server</a>\n</h2>\n\n<p>Finally, let's get this demo running! Since we're using Express, we can use <a href=\"https://www.npmjs.com/package/express-graphql\">the <code>express-graphql</code> package</a> to expose our schema as an endpoint. The package requires two arguments: your schema, and your root value. It takes one optional argument, <code>graphiql</code>, which we'll talk about in a bit.</p>\n\n<p>Set up your Express server on your favorite port with the GraphQL middleware like this:</p>\n\n<pre><code class=\"lang-javascript\">const express = require(\"express\");\nconst express_graphql = require(\"express-graphql\");\n\nconst app = express();\napp.use(\n  \"/graphql\",\n  express_graphql({\n    schema: schema,\n    graphiql: true,\n  })\n);\napp.listen(5000, () =&gt; console.log(\"Express is now live at localhost:5000\"));\n</code></pre>\n\n<p>Navigate your browser to <code>http://localhost:5000/graphql</code>, and you should see a sort of IDE interface. On the left pane, you can enter any valid GraphQL query you like, and on your right you'll get the results. This is what <code>graphiql: true</code> provides: a convenient way of testing out your queries. You probably wouldn't want to expose this in a production environment, but it makes testing much easier.</p>\n\n<p>Try entering the query we demonstrated above:</p>\n\n<pre><code class=\"lang-graphql\">query {\n  user(id: 1) {\n    name\n    email\n  }\n}\n</code></pre>\n\n<p>To explore GraphQL's typing capabilities, try passing in a string instead of an integer for the ID argument:</p>\n\n<pre><code class=\"lang-graphql\"># this doesn't work\nquery {\n  user(id: \"1\") {\n    name\n    email\n  }\n}\n</code></pre>\n\n<p>You can even try requesting fields that don't exist:</p>\n\n<pre><code class=\"lang-graphql\"># this doesn't work\nquery {\n  user(id: 1) {\n    name\n    zodiac\n  }\n}\n</code></pre>\n\n<p>With just a few clear lines of code expressed by the schema, a strongly-typed contract between the client and server is established. This protects your services from receiving bogus data and expresses errors clearly to the requester.</p>\n<h2 class=\"anchored\">\n  <a name=\"performance-considerations\" href=\"#performance-considerations\">Performance considerations</a>\n</h2>\n\n<p>For as much as GraphQL takes care of for you, it doesn't solve every problem inherent in building APIs. In particular, caching and authorization are just two areas that require some forethought to prevent performance issues. The GraphQL spec does not provide any guidance for implementing either of these, which means that the responsibility for building them falls onto you.</p>\n<h3 class=\"anchored\">\n  <a name=\"caching\" href=\"#caching\">Caching</a>\n</h3>\n\n<p>REST-based APIs don't need to be overly concerned when it comes to caching, because they can build on <a href=\"https://restfulapi.net/caching/\">existing HTTP header strategies</a> that the rest of the web uses. GraphQL doesn't come with these caching mechanisms, which can place undue processing burden on your servers for repeated requests. Consider the following two queries:</p>\n\n<pre><code class=\"lang-graphql\">query {\n  user(id: 1) {\n    name\n  }\n}\n\nquery {\n  user(id: 1) {\n    email\n  }\n}\n</code></pre>\n\n<p>Without some sort of caching in place, this would result in two database queries to fetch the <code>User</code> with an ID of <code>1</code>, just to retrieve two different columns. In fact, since GraphQL also allows for <a href=\"https://graphql.org/learn/queries/#aliases\">aliases</a>, the following query is valid and also performs two lookups:</p>\n\n<pre><code class=\"lang-graphql\">query {\n  one: user(id: 1) {\n    name\n  }\n  two: user(id: 2) {\n    name\n  }\n}\n</code></pre>\n\n<p>This second example exposes the problem of how to batch queries. In order to be fast and efficient, we want GraphQL to access the same database rows with as few roundtrips as possible.</p>\n\n<p><a href=\"https://github.com/graphql/dataloader\">The <code>dataloader</code> package</a> was designed to handle both of these issues. Given an array of IDs, we will fetch all of those at once from the database; as well, subsequent calls to the same ID will fetch the item from the cache. To build this out using <code>dataloader</code>, we need two things. First, we need a function to load all of the requested objects. In our sample, that looks something like this:</p>\n\n<pre><code class=\"lang-javascript\">const DataLoader = require('dataloader');\nconst batchGetUserById = async (ids) =&gt; {\n   // in real life, this would be a DB call\n  return ids.map(id =&gt; users[id]);\n};\n// userLoader is now our \"batch loading function\"\nconst userLoader = new DataLoader(batchGetUserById);\n</code></pre>\n\n<p>This takes care of the issue with batching. To load the data, and work with the cache, we'll replace our previous data lookup with a call to the <code>load</code> method and pass in our user ID:</p>\n\n<pre><code class=\"lang-javascript\">const resolvers = {\n  Query: {\n    user: function (_, { id }) {\n      return userLoader.load(id);\n    },\n  },\n}\n</code></pre>\n<h3 class=\"anchored\">\n  <a name=\"authorization\" href=\"#authorization\">Authorization</a>\n</h3>\n\n<p>Authorization is an entirely different problem with GraphQL. In a nutshell, it's the process of identifying whether a given user has permission to see some data. We can imagine scenarios where an authenticated user can execute queries to get their own address information, but they should not be able to get the addresses of other users.</p>\n\n<p>To handle this, we need to modify our resolver functions. In addition to a field's arguments, a resolver also has access to its parent, as well as a special  <em>context</em> value passed in, which can provide information about the currently authenticated user. Since we know that <code>addresses</code> is a sensitive field, we need to change our code such that a call to users doesn't just return a list of addresses, but actually, calls out to some business logic to validate the request:</p>\n\n<pre><code class=\"lang-javascript\">const getAddresses = function(currUser, user) {\n  if (currUser.id == user.id) {\n    return user.addresses\n  }\n\n  return [];\n}\n\nconst resolvers = {\n  Query: {\n    user: function (_, { id }) {\n      return users[id];\n    },\n  },\n  User: {\n    addresses: function (parentObj, {}, context) {\n          return getAddresses(context.currUser, parentObj);\n    },\n  },\n};\n</code></pre>\n\n<p>Again, we don't need to explicitly define a resolver for each <code>User</code> field—only the one which we want to modify.</p>\n\n<p>By default, <code>express-graphql</code> passes the current HTTP <code>request</code> as a value for <code>context</code>, but this can be changed when setting up your server:</p>\n\n<pre><code class=\"lang-javascript\">app.use(\n  \"/graphql\",\n  express_graphql({\n    schema: schema,\n    graphiql: true,\n    context: {\n      currUser: user // currently authenticated user\n    }\n  })\n);\n</code></pre>\n<h2 class=\"anchored\">\n  <a name=\"schema-best-practices\" href=\"#schema-best-practices\">Schema best practices</a>\n</h2>\n\n<p>One aspect missing from the GraphQL spec is the lack of guidance on versioning schemas. As applications grow and change over time, so too will their APIs, and it's likely that GraphQL fields and objects will need to be removed or modified. But this downside can also be positive: by designing your GraphQL schema carefully, you can avoid pitfalls apparent in easier to implement (and easier to break) REST endpoints, such as inconsistencies in naming and confusing relationships. Marc-Andre has <a href=\"https://www.apollographql.com/blog/graphql-schema-design-building-evolvable-schemas-1501f3c59ed5\">listed several strategies</a> for building evolvable schemas which we highly recommend reading through.</p>\n\n<p>In addition, you should try to keep as much of <a href=\"https://graphql.org/learn/thinking-in-graphs/#business-logic-layer\">your business logic separate from your resolver logic</a>. Your business logic should be a single source of truth for your entire application. It can be tempting to perform validation checks within a resolver, but as your schema grows, it will become an untenable strategy.</p>\n<h2 class=\"anchored\">\n  <a name=\"when-is-graphql-not-a-good-fit\" href=\"#when-is-graphql-not-a-good-fit\">When is GraphQL not a good fit?</a>\n</h2>\n\n<p>GraphQL doesn't mold precisely to the needs of HTTP communication the same way that REST does. For example, GraphQL specifies only a single status code—<code>200 OK</code>—regardless of the query’s success. A special <code>errors</code> key is returned in this response for clients to parse and identify what went wrong. Because of this, error handling can be a bit trickier.</p>\n\n<p>As well, GraphQL is just a specification, and it won't automatically solve every problem your application faces. Performance issues won't disappear, database queries won't become faster, and in general, you'll need to rethink everything about your API: authorization, logging, monitoring, caching. Versioning your GraphQL API can also be a challenge, as the official spec currently has no support for handling breaking changes, an inevitable part of building any software. If you're interested in exploring GraphQL, you will need to dedicate some time to learning how to best integrate it with your needs.</p>\n<h2 class=\"anchored\">\n  <a name=\"learning-more\" href=\"#learning-more\">Learning more</a>\n</h2>\n\n<p>The community has rallied around this new paradigm and come up with <a href=\"https://github.com/chentsulin/awesome-graphql\">a list of awesome GraphQL resources</a>, for both frontend and backend engineers. You can also see what queries and types look like by <a href=\"https://graphql.org/swapi-graphql/\">making real requests on the official playground</a>.</p>\n\n<p>We also have a <a href=\"https://www.heroku.com/podcasts/codeish/44-graphqls-benefits-and-costs\">Code[ish] podcast episode</a> dedicated entirely to the benefits and costs of GraphQL.</p>","PublishedAt":"2020-06-24 15:30:00+00:00","OriginURL":"https://blog.heroku.com/building-graphql-api-javascript","SourceName":"Heroku"}},{"node":{"ID":361,"Title":"From Project to Productionized with Python","Description":"<p>We hope that you and your loved ones are staying safe from the COVID-19 pandemic. As a result of its effect on large gatherings, <a href=\"https://pycon.blogspot.com/2020/03/pycon-us-2020-in-pittsburgh.html\">PyCon 2020 was <del>cancelled</del> changed to an online event</a>. Although not being able to gather in person was disheartening for organizers, speakers, and attendees, the Python community shared virtual high-fives and hugs with <a href=\"https://us.pycon.org/2020/online/\">PyCon US 2020 Online.</a> We <a href=\"https://www.youtube.com/watch?v=1923eduj0Gg\">recorded our planned Heroku workshop for the event</a>, on which this blog post is based.</p>\n\n<div class=\"embedded-video-wrapper\">\n<iframe title=\"From Project to Productionized on Heroku\" src=\"https://www.youtube-nocookie.com/embed/1923eduj0Gg\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n\n\n<p>Imagine that you've just spent the last two weeks pouring all your energy into an application. It's magnificent, and you're finally ready to share it on the Internet. How do you do it? In this post, we're going to walk through the hands-on process aimed at Python developers deploying their local application to Heroku.</p>\n\n<p>An application running on Heroku works best as <a href=\"https://12factor.net/\">a 12-factor application</a>. This is actually a concept that Heroku championed over 10 years ago. It's the idea that you build an application with robust redeployments in mind. Most of this workshop is actually not specific to Heroku, but rather, about taking a regular Django application and making it meet the 12 factor app methodology, which has become a standard that most cloud deployment providers not only support but recommend.</p>\n<h2 class=\"anchored\">\n  <a name=\"prerequisites\" href=\"#prerequisites\">Prerequisites</a>\n</h2>\n\n<p>Before completing this workshop, we're going to make a few assumptions about you, dear reader. First, this is not going to be a Django tutorial. If you're looking for an introduction to Django, <a href=\"https://www.djangoproject.com/start/\">their documentation has some excellent tutorials to follow</a>. You will also need a little bit of  <a href=\"https://git-scm.com\">Git</a> familiarity, and have it installed on your machine.</p>\n\n<p>In order to complete this workshop, you'll need a few things:</p>\n\n<ol>\n<li>\n<a href=\"https://signup.heroku.com/\">An account on Heroku</a>. This is completely free and doesn't require any payment information.</li>\n<li>\n<a href=\"https://devcenter.heroku.com/articles/heroku-cli#download-and-install\">The Heroku CLI</a>. Once your application is on Heroku, this will make managing it much easier.</li>\n<li>You'll need to <a href=\"https://github.com/heroku-python/PyCon2020\">clone the repository for this workship</a>, and be able to open it in a text editor.</li>\n</ol>\n\n<p>With all that sorted, it's time to begin!</p>\n<h2 class=\"anchored\">\n  <a name=\"look-around-you\" href=\"#look-around-you\">Look around you</a>\n</h2>\n\n<p>With the project cloned and available on your computer, take a moment to explore its structure. We'll be modifying the <code>manage.py</code> and <code>requirements.txt</code> files, as well as <code>settings.py</code> and <code>wsgi.py</code> in the <code>gettingstarted</code> folder.</p>\n<h3 class=\"anchored\">\n  <a name=\"updating-code-gitignore-code\" href=\"#updating-code-gitignore-code\">Updating <code>.gitignore</code></a>\n</h3>\n\n<p>To begin with, we'll be updating the gitignore file. <a href=\"https://git-scm.com/docs/gitignore\">A gitignore file excludes files</a> which you don't want to check into your repository. In order to deploy to Heroku, you don't technically need a gitignore file. You can deploy successfully without one, but it's highly recommended to always have one (and not just for Heroku). A gitignore can be essential for keeping out passwords and credentials keys, large binary files, local configurations, or anything else that you don't want to expose to the public.</p>\n\n<p>Copy the following block of code and paste it into the gitignore file in the root of your project:</p>\n\n<pre><code>/venv\n__pycache__\ndb.sqlite3          # not needed if you're using Postgres locally\ngettingstarted/static/\n</code></pre>\n\n<p>The <code>venv</code> directory contains <a href=\"https://docs.python.org/3/tutorial/venv.html\">a virtual environment</a> with the packages necessary for your local Python version. Similarly, the <code>__pycache__</code> directory contains <a href=\"https://docs.python.org/3/tutorial/modules.html#compiled-python-files\">precompiled modules unique to your system</a>. We don't want to check in our database (<code>db.sqlite3</code>), as we don't want to expose any local data. Last, the static files will be automatically generated for us during the build and deploy process to Heroku, so we'll exclude the <code>gettingstarted/static/</code> directory.</p>\n\n<p>Go ahead and run <code>git status</code> on your terminal to make sure that gitignore is the only file that's been modified. After that, call <code>git add</code>, then <code>git commit -m \"step 1 add git ignore\"</code>.</p>\n<h3 class=\"anchored\">\n  <a name=\"modularize-your-settings\" href=\"#modularize-your-settings\">Modularize your settings</a>\n</h3>\n\n<p>Next up, we want to modularize our Django settings. To do that, add a new folder within <code>gettingstarted</code> called <code>settings</code>. Then, move the <code>settings.py</code> file into that directory. Since this naming scheme is a bit confusing, let's go ahead and rename that file to <code>base.py</code>. We'll call it that because it will serve as the base (or default) configuration that all the other configurations are going to pull from. If something like <code>dev.py</code> or <code>local.py</code> makes more sense to you, feel free to use that instead!</p>\n\n<p>Local projects only have one environment to keep track of: your local machine. But once you want to deploy to different places, it's important to keep track of what settings go where. Nesting our settings files this way makes it easy for us to keep track of where those settings are, as well as take advantage of Heroku's continuous delivery tool pipelines.</p>\n\n<p>By moving and renaming the settings file, our Django application now has two broken references. Let's fix them before we move on.</p>\n\n<p>The first is in the <code>wsgi.py</code> in your <code>gettingstarted</code> folder. Open it up, and on <a href=\"https://github.com/heroku-python/PyCon2020/blob/1a4cf7eabfcc994f60f4b8efeed1f0d9a245e768/gettingstarted/wsgi.py#L12\">line 12</a> you'll see that a default Django settings module is being set to <code>gettingstarted.settings</code>, a file which no longer exists:</p>\n\n<pre><code class=\"lang-python\">os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"gettingstarted.settings\")\n</code></pre>\n\n<p>To fix this, append the name of the file you just created in the settings subfolder. For example, since we called ours <code>base.py</code>, the line should now look like this:</p>\n\n<pre><code class=\"lang-python\">os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"gettingstarted.settings.base\")\n</code></pre>\n\n<p>After saving that, navigate up one directory to <code>manage.py</code>. On <a href=\"https://github.com/heroku-python/PyCon2020/blob/1a4cf7eabfcc994f60f4b8efeed1f0d9a245e768/manage.py#L6\">line 6</a>, you'll see the same default being set for the Django settings module. Once again, append <code>.base</code> to the end of this line, then commit both of them to Git.</p>\n<h2 class=\"anchored\">\n  <a name=\"continuous-delivery-pipelines\" href=\"#continuous-delivery-pipelines\">Continuous delivery pipelines</a>\n</h2>\n\n<p>In an application's deployment lifecycle, there are typically four stages:</p>\n\n<ol>\n<li>You build your app in the development stage on your local machine to make sure it works.</li>\n<li>Next comes the review stage, where you check to see if your changes pass with the full test suite of your code base.</li>\n<li>If that goes well, you merge your changes to staging. This is where you have conditions as close to public as possible, perhaps with some dummy data available, in order to more accurately predict how the change will impact your users.</li>\n<li>Lastly, if all that goes well, you push to production, where the change is now live for your customers.</li>\n</ol>\n\n<p>Continuous delivery (CD) workflows are designed to test your change in conditions progressively closer and closer to production and with more and more detail. Continuous delivery is a powerful workflow that can make all of the difference in your experience as a developer once you've productionized your application. Heroku can save you a lot of time here, as we've already built the tools for you to have a continuous delivery workflow. From your dashboard on Heroku, you can—with the mere click of a button!–<a href=\"https://devcenter.heroku.com/articles/pipelines\">set up a pipeline</a>, add applications to staging and production, and deploy them.</p>\n\n<p>If you <a href=\"https://devcenter.heroku.com/articles/github-integration\">connect your GitHub repository</a>, pipelines can also automatically deploy and test new PRs opened on your repo. By providing the tooling and automating these processes, Heroku's continuous delivery workflow is powerful enough to help you keep up with your development cycle.</p>\n<h2 class=\"anchored\">\n  <a name=\"adding-new-middleware-to-code-base-py-code\" href=\"#adding-new-middleware-to-code-base-py-code\">Adding new middleware to <code>base.py</code></a>\n</h2>\n\n<p>Modularizing your Django settings is a great way to take advantage of this continuous delivery workflow by splitting up your settings, whether you're deploying to Heroku or elsewhere, but there's one more change we have to make to <code>base.py</code>.</p>\n\n<p>Django static assets work best when you also use the <a href=\"http://whitenoise.evans.io/en/stable/django.html\">whitenoise</a> package to manage your static assets. It's really easy to add to your project.</p>\n\n<p>In your <code>base.py</code> file, scroll down to about line 43, and you should see an array of package names like this:</p>\n\n<pre><code class=\"lang-python\">MIDDLEWARE = [\n    \"django.middleware.security.SecurityMiddleware\",\n    # Whitenoise goes here\n    \"django.contrib.sessions.middleware.SessionMiddleware\",\n    \"django.middleware.common.CommonMiddleware\",\n    \"django.middleware.csrf.CsrfViewMiddleware\",\n    \"django.contrib.auth.middleware.AuthenticationMiddleware\",\n    \"django.contrib.messages.middleware.MessageMiddleware\",\n    \"django.middleware.clickjacking.XFrameOptionsMiddleware\",\n]\n</code></pre>\n\n<p>This is your list of <a href=\"https://docs.djangoproject.com/en/3.0/topics/http/middleware/\">Django middleware</a>, which are sort of like plugins for your server. Django loads your middleware in the order that it's listed, so you always want your security middleware first, but it's important to add whitenoise as the second step in this base file.</p>\n\n<p>Copy the following line of code and replace the line that says <code>Whitenoise goes here</code> with this:</p>\n\n<pre><code class=\"lang-python\">\"whitenoise.middleware.WhiteNoiseMiddleware\",\n</code></pre>\n\n<p>We've loaded whitenoise as middleware, but to actually <em>use</em> the whitenoise compression, we need to set one more variable. Copy the following code and paste it right at the end of your <code>base.py</code> file:</p>\n\n<pre><code class=\"lang-python\">STATICFILES_STORAGE = \"whitenoise.storage.CompressedManifestStaticFilesStorage\"\n</code></pre>\n\n<p>With that, we're done with <code>base.py</code>. Congratulations! Save your work and commit it to Git.</p>\n<h2 class=\"anchored\">\n  <a name=\"setting-up-code-heroku-py-code\" href=\"#setting-up-code-heroku-py-code\">Setting up <code>heroku.py</code></a>\n</h2>\n\n<p>Our base settings are complete, but now we need our Heroku-specific settings. Create a new file under <code>gettingstarted/settings</code> called <code>heroku.py</code> and paste the following block of code:</p>\n\n<pre><code class=\"lang-python\">\"\"\"\nProduction Settings for Heroku\n\"\"\"\n\nimport environ\n\n# If using in your own project, update the project namespace below\nfrom gettingstarted.settings.base import *\n\nenv = environ.Env(\n    # set casting, default value\n    DEBUG=(bool, False)\n)\n\n# False if not in os.environ\nDEBUG = env('DEBUG')\n\n# Raises django's ImproperlyConfigured exception if SECRET_KEY not in os.environ\nSECRET_KEY = env('SECRET_KEY')\n\nALLOWED_HOSTS = env.list('ALLOWED_HOSTS')\n\n# Parse database connection url strings like psql://user:pass@127.0.0.1:8458/db\nDATABASES = {\n    # read os.environ['DATABASE_URL'] and raises ImproperlyConfigured exception if not found\n    'default': env.db(),\n}\n</code></pre>\n\n<p>You can see in this file the values that we're listing here are the ones that we're overriding from our base settings, so these are the settings that will be different and unique for Heroku.</p>\n\n<p>To do this, we're using one of my favorite packages, <a href=\"https://github.com/joke2k/django-environ\">Django-environ</a>. This allows us to quickly and easily interface with the operating system environment without knowing much about it. It has built-in type conversions, and in particular it has automatic database parsing. This is all we need in order to parse our Heroku Postgres database URL that we will be given. It's just really convenient.</p>\n<h2 class=\"anchored\">\n  <a name=\"heroku-specific-files\" href=\"#heroku-specific-files\">Heroku-specific files</a>\n</h2>\n\n<p>That's all the work we need to do to get our application into 12 factored shape, but there are three more files we need in order to deploy to Heroku.</p>\n<h3 class=\"anchored\">\n  <a name=\"code-requirements-txt-code\" href=\"#code-requirements-txt-code\"><code>requirements.txt</code></a>\n</h3>\n\n<p>In addition to the packages your project already uses, there are a few more you need to deploy to Heroku. If we take a look at the provided <code>requirements.txt</code> file, you can see these required packages here. We've already talked about Django, Django-environ, and whitenoise, and we've already configured those for use. But the other two are also important and needed for deployment.</p>\n\n<p>The first one is called <a href=\"https://gunicorn.org/\">Gunicorn</a>. This is the recommended WSGI server for Heroku. We'll take a look at configuring this in just a bit. The next one is <a href=\"https://www.psycopg.org/\">psychopg2</a>. This is a Postgres database adapter. You need it in your <code>requirements.txt</code> file to deploy, but you don't need any code changes in order to activate it.</p>\n\n<p>A quick side note: we're keeping our discussion on packages simple for the purpose of this demo, but when you're ready to deploy a real project to Heroku, consider freezing your dependencies. You can do this with the <a href=\"https://pip.pypa.io/en/stable/reference/pip_freeze/\"><code>pip freeze</code></a> command. This will make your build a little bit more predictable by locking your exact dependency versions into your Git repo. If your dependencies aren't locked, you might find yourself deploying one version of Django one day and a new one the next.</p>\n<h3 class=\"anchored\">\n  <a name=\"code-runtime-txt-code\" href=\"#code-runtime-txt-code\"><code>runtime.txt</code></a>\n</h3>\n\n<p>Heroku will install a default Python version if you don't specify one, but if you want to pick your Python version, you'll need a <code>runtime.txt</code> file. Create one in the root directory, next to your <code>requirements.txt</code>, <code>manage.py</code>, <code>.gitignore</code> and the rest. Specify your Python version with the prefix <code>python-</code>, followed by the major, minor, and patch version that you want your application to run on:</p>\n\n<pre><code class=\"lang-txt\">python-3.8.2\n</code></pre>\n<h3 class=\"anchored\">\n  <a name=\"code-procfile-code\" href=\"#code-procfile-code\"><code>Procfile</code></a>\n</h3>\n\n<p>The last file we need to add is a file specific to Heroku: <a href=\"https://devcenter.heroku.com/articles/procfile\">the <code>Procfile</code></a>. This is what we use to specify the processes our application should run. The processes specified in this file will automatically boot on deploy to Heroku. Create a file named <code>Procfile</code> in the root level directory, right next to your <code>requirements.txt</code> and <code>runtime.txt</code> files. (Make sure to capitalize the P of Procfile otherwise Heroku might not recognize it!) Copy-paste the following lines into it:</p>\n\n<pre><code class=\"lang-txt\">release: python3 manage.py migrate\nweb: gunicorn gettingstarted.wsgi --preload --log-file -\n</code></pre>\n\n<p><a href=\"https://devcenter.heroku.com/articles/release-phase\">The <code>release</code> phase</a> of a Heroku deployment is the best place to run tasks, like migrations or updates. The command we will run during this phase is to simply run the <code>migrate</code> task defined in <code>manage.py</code>.</p>\n\n<p>The other process is the <code>web</code> process, which is very important, if not outright essential, for any web application. This is where we pass our Gunicorn config, the same things we need when running the server locally. We pass it our WSGI file, which is located in the <code>gettingstarted</code> directory, and then we pass a few more flags to add it a bit more configuration. The <code>--preload</code> flag ensures that the app can receive requests just a little bit faster; the <code>--logfile</code> just specifies that the log file should get routed to Heroku.</p>\n<h2 class=\"anchored\">\n  <a name=\"readying-for-deployment\" href=\"#readying-for-deployment\">Readying for deployment</a>\n</h2>\n\n<p>Take a second before moving on and just double check that you've saved and committed all of your changes to Git. Remember, we need those changes in the Git repo in order for them to successfully deploy. After that, let's get ready to make an app!</p>\n<h3 class=\"anchored\">\n  <a name=\"creating-an-app-with-code-heroku-create-code\" href=\"#creating-an-app-with-code-heroku-create-code\">Creating an app with <code>heroku create</code></a>\n</h3>\n\n<p>Since we have the Heroku CLI installed, we can <a href=\"https://devcenter.heroku.com/articles/creating-apps\">call <code>heroku create</code> on the command line to have an app generated</a> for us:</p>\n\n<pre><code class=\"lang-term\">$ heroku create\nCreating app... done, ⬢ mystic-wind-83\nCreated http://mystic-wind-83.herokuapp.com/ | git@heroku.com:mystic-wind-83.git\n</code></pre>\n\n<p>Your app will be assigned a random name—in this example, it's <code>mystic-wind-83</code>—as well as a publicly accessible URL.</p>\n<h3 class=\"anchored\">\n  <a name=\"setting-environment-variables-on-heroku\" href=\"#setting-environment-variables-on-heroku\">Setting environment variables on Heroku</a>\n</h3>\n\n<p>When we created our <code>heroku.py</code> settings file, we used Django-environ to load environment variables into our settings config. <a href=\"https://devcenter.heroku.com/articles/config-vars\">Those environment variables also need to be present in our Heroku environment</a>, so let's set those now.</p>\n\n<p>The Heroku CLI command we'll be using for this is <code>heroku config:set</code>. This will take in key-value pairs as arguments and set them in your Heroku runtime environment. First, let's configure our allowed hosts. Type the following line, and replace <code>YOUR_UNIQUE_URL</code> with the URL generated by <code>heroku create</code>:</p>\n\n<pre><code class=\"lang-term\">$ heroku config:set ALLOWED_HOSTS=&lt;YOUR_UNIQUE_URL&gt;\n</code></pre>\n\n<p>Next, let's set our Django settings module. This is what determines what settings configuration we use on this platform. Instead of using the default of <code>base</code>, we want the Heroku-specific settings:</p>\n\n<pre><code class=\"lang-term\">$ heroku config:set DJANGO_SETTINGS_MODULE=gettingstarted.settings.heroku\n</code></pre>\n\n<p>Lastly, we'll need to create a <code>SECRET_KEY</code>. For this demo, it doesn't matter what its value is. You can use a secure hash generator like <code>md5</code>, or a password manager's generator. Just be sure to keep this value secure, don't reuse it, and NEVER check it into source code! You can set it using the same CLI command:</p>\n\n<pre><code class=\"lang-term\">$ heroku config:set SECRET_KEY=&lt;gobbledygook&gt;\n</code></pre>\n<h2 class=\"anchored\">\n  <a name=\"provisioning-our-database\" href=\"#provisioning-our-database\">Provisioning our database</a>\n</h2>\n\n<p>Locally, Django is configured to use a SQLite database but we're productionizing. We need something a little bit more robust. Let's provision a Postgres database for production.</p>\n\n<p>First, let's check if we have a database already. The <a href=\"https://devcenter.heroku.com/articles/heroku-cli-commands#heroku-addons-all-app-app\"><code>heroku addons</code></a> command will tell us if one exists:</p>\n\n<pre><code class=\"lang-term\">$ heroku addons\nNo add-ons for app mystic-wind-83.\n</code></pre>\n\n<p>No add-ons exist for our app, which makes sense—we just created it! To add a Postgres database, we can use the <code>addons:create</code> command like this:</p>\n\n<pre><code class=\"lang-term\">$ heroku addons:create heroku-postgresql:hobby-dev\n</code></pre>\n\n<p>Heroku offers several tiers of Postgres databases. <code>hobby-dev</code> is the free tier, so you can play around with this without paying a dime.</p>\n<h2 class=\"anchored\">\n  <a name=\"going-live\" href=\"#going-live\">Going live</a>\n</h2>\n\n<p>It is time. Your code is ready, your Heroku app is configured, you are ready to deploy. This is the easy part!</p>\n\n<p>Just type out</p>\n\n<pre><code class=\"lang-term\">$ git push heroku master\n</code></pre>\n\n<p>And we'll take care of the rest! You'll see your build logs scrolling through your terminal. This will show you what we're installing on your behalf and where you are in the build process. You'll also see the <code>release</code> phase as well that we specified earlier.</p>\n<h2 class=\"anchored\">\n  <a name=\"scaling-up\" href=\"#scaling-up\">Scaling up</a>\n</h2>\n\n<p>The last step is to scale up our web process. This creates new dynos, or, in other words, copies of your code on Heroku servers to handle more web traffic. You can do this using the following command:</p>\n\n<pre><code class=\"lang-term\">$ heroku ps:scale web=1\n</code></pre>\n\n<p>To see your app online, enter <code>heroku open</code> on the terminal. This should pop open a web browser with the site you just built.</p>\n<h2 class=\"anchored\">\n  <a name=\"debugging\" href=\"#debugging\">Debugging</a>\n</h2>\n\n<p>If you hit some snags, don't worry, we have some tips that might help:</p>\n\n<ul>\n<li>Are all of your changes saved and checked into Git?</li>\n<li>Are your changes on the <code>master</code> branch or are they on a different branch? Make sure that whatever you're deploying, all of your changes are in that Git branch.</li>\n<li>Did you deploy from the root directory of your project? Did you also call <code>heroku create</code> from the root directory of your project? If not, this could absolutely cause a trip up.</li>\n<li>Did you remove anything from the code in the provided demo that we didn't discuss?</li>\n</ul>\n<h3 class=\"anchored\">\n  <a name=\"logging\" href=\"#logging\">Logging</a>\n</h3>\n\n<p>If you've run through this list and still have issues, take a look at your log files. In addition to your build logs—which will tell you whether your application successfully deployed or not—you have access to all logs produced by Heroku and by your application. You can get to these through a couple of different ways, but the quickest way is just to run the following command:</p>\n\n<pre><code class=\"lang-term\">$ heroku logs --tail\n</code></pre>\n<h3 class=\"anchored\">\n  <a name=\"remote-console\" href=\"#remote-console\">Remote console</a>\n</h3>\n\n<p>Another tool you have is the <code>heroku run bash</code> command. This provides you with direct access from your terminal to a Heroku dyno with your code deployed to it. If you type <code>ls</code>, you can see that this is your deployed application. It can be useful to check that what is up here matches what is locally on your machine. If not, you might see some issues.</p>\n<h2 class=\"anchored\">\n  <a name=\"wrapping-up\" href=\"#wrapping-up\">Wrapping up</a>\n</h2>\n\n<p>Congratulations on successfully deploying your productionized app onto Heroku!</p>\n\n<p>To help you learn about Heroku, we also have a wealth of technical documentation. Our <a href=\"https://devcenter.heroku.com\">Dev Center</a> is where you'll find most of our technical how-to and supported technologies information. If you're having a technical issue, chances are someone else has asked the same question and it's been answered on our help docs. Use these resources to solve your problems as well as to learn about best practices when deploying to Heroku.</p>","PublishedAt":"2020-06-22 16:00:00+00:00","OriginURL":"https://blog.heroku.com/from-project-to-productionized-python","SourceName":"Heroku"}},{"node":{"ID":806,"Title":"Announcing Twinagle: Twirp and Protobuf for Finagle","Description":"A previous post on this blog ended with the following paragraph: “We might also replace JSON with a more efficient serialization protocol…","PublishedAt":"2020-06-12 00:00:00+00:00","OriginURL":"https://developers.soundcloud.com/blog/announcing-twinagle","SourceName":"Soundcloud"}},{"node":{"ID":631,"Title":"Cross-Cluster Traffic Mirroring with Istio","Description":"","PublishedAt":"2020-06-10 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2020-06-10-crossclustertrafficmirroringwithistio/","SourceName":"Trivago"}}]}},"pageContext":{"limit":30,"skip":5280,"numPages":193,"currentPage":177}},"staticQueryHashes":["3649515864"]}