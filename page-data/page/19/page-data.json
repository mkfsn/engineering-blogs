{"componentChunkName":"component---src-templates-blog-list-tsx","path":"/page/19","result":{"data":{"allPost":{"edges":[{"node":{"ID":5169,"Title":"Introducing Griffin 2.0: Instacart’s Next-Gen ML Platform","Description":"","PublishedAt":"2023-11-17 22:44:52+00:00","OriginURL":"https://tech.instacart.com/introducing-griffin-2-0-instacarts-next-gen-ml-platform-b7331e73b8d7?source=rss----587883b5d2ee---4","SourceName":"Instacart"}},{"node":{"ID":5066,"Title":" How to execute an object file: Part 4, AArch64 edition ","Description":" The initial posts are dedicated to the x86 architecture. Since then, the fleet of our working machines has expanded to include a large and growing number of ARM CPUs. This time we’ll repeat this exercise for the aarch64 architecture. ","PublishedAt":"2023-11-17 14:00:35+00:00","OriginURL":"http://blog.cloudflare.com/how-to-execute-an-object-file-part-4/","SourceName":"Cloudflare"}},{"node":{"ID":5266,"Title":"How to execute an object file: Part 4, AArch64 edition","Description":" The initial posts are dedicated to the x86 architecture. Since then, the fleet of our working machines has expanded to include a large and growing number of ARM CPUs. This time we’ll repeat this exercise for the aarch64 architecture. ","PublishedAt":"2023-11-17 14:00:35+00:00","OriginURL":"https://blog.cloudflare.com/how-to-execute-an-object-file-part-4","SourceName":"Cloudflare"}},{"node":{"ID":5366,"Title":"How to execute an object file: Part 4, AArch64 edition","Description":" The initial posts are dedicated to the x86 architecture. Since then, the fleet of our working machines has expanded to include a large and growing number of ARM CPUs. This time we’ll repeat this exercise for the aarch64 architecture. ","PublishedAt":"2023-11-17 14:00:35+00:00","OriginURL":"https://staging.blog.mrk.cfdata.org/how-to-execute-an-object-file-part-4","SourceName":"Cloudflare"}},{"node":{"ID":5212,"Title":"The next evolution of marketing analytics","Description":"<figure><img src=\"https://mixpanel.com/wp-content/uploads/2023/11/Mixpanel-Marketing-Analytics-at-6-months-1-1024x576.png\" class=\"type:primaryImage\" /></figure>\n<p>Most companies are built on one fundamental insight. In the case of Mixpanel, that insight is that the event-based data structure is the most powerful, flexible, and intuitive model for all forms of analytics. While this idea is mainstream for product teams, it’s only just crossed the chasm for marketing teams: With the emergence of</p>\n<p>The post <a href=\"https://mixpanel.com/blog/the-next-evolution-of-marketing-analytics/\">The next evolution of marketing analytics</a> appeared first on <a href=\"https://mixpanel.com\">Mixpanel</a>.</p>\n","PublishedAt":"2023-11-17 01:54:05+00:00","OriginURL":"https://mixpanel.com/blog/the-next-evolution-of-marketing-analytics/","SourceName":"Mixpanel"}},{"node":{"ID":5065,"Title":"Troubleshooting Kibana health","Description":"","PublishedAt":"2023-11-17 00:00:00+00:00","OriginURL":"https://www.elastic.co/blog/troubleshooting-kibana-health","SourceName":"Elastic"}},{"node":{"ID":5062,"Title":"Happy anniversary, Amazon CloudFront: 15 years of evolution and internet advancements","Description":"I can’t believe it’s been 15 years since Amazon CloudFront was launched! When Amazon S3 became available in 2006, developers loved the flexibility and started to build a new kind of globally distributed applications where storage was not a bottleneck. These applications needed to be performant, reliable, and cost-efficient for every user on the planet. […]","PublishedAt":"2023-11-16 21:52:19+00:00","OriginURL":"https://aws.amazon.com/blogs/aws/happy-anniversary-amazon-cloudfront-15-years-of-evolution-and-internet-advancements/","SourceName":"AWS"}},{"node":{"ID":5063,"Title":"New – Long-Form voices for Amazon Polly","Description":"We are launching three new voices for Polly. Powered by a new long-form engine, the voices are natural and expressive, with appropriate pauses, emphasis, and tone. New Voices The new long-form voices are perfect for blog posts, news articles, training videos, and marketing content. The underlying Machine Learning model extracts meaning from the text, learning […]","PublishedAt":"2023-11-16 19:33:57+00:00","OriginURL":"https://aws.amazon.com/blogs/aws/new-long-form-voices-for-amazon-polly/","SourceName":"AWS"}},{"node":{"ID":5060,"Title":" Introducing advanced session audit capabilities in Cloudflare One ","Description":" Administrators can now easily audit all active user sessions and associated data used by their Cloudflare One policies. This enables the best of both worlds: extremely granular controls, while maintaining an improved ability to troubleshoot and diagnose ","PublishedAt":"2023-11-16 18:49:23+00:00","OriginURL":"http://blog.cloudflare.com/introducing-advanced-session-audit-capabilities-in-cloudflare-one/","SourceName":"Cloudflare"}},{"node":{"ID":5267,"Title":"Introducing advanced session audit capabilities in Cloudflare One","Description":" Administrators can now easily audit all active user sessions and associated data used by their Cloudflare One policies. This enables the best of both worlds: extremely granular controls, while maintaining an improved ability to troubleshoot and diagnose ","PublishedAt":"2023-11-16 18:49:23+00:00","OriginURL":"https://blog.cloudflare.com/introducing-advanced-session-audit-capabilities-in-cloudflare-one","SourceName":"Cloudflare"}},{"node":{"ID":5367,"Title":"Introducing advanced session audit capabilities in Cloudflare One","Description":" Administrators can now easily audit all active user sessions and associated data used by their Cloudflare One policies. This enables the best of both worlds: extremely granular controls, while maintaining an improved ability to troubleshoot and diagnose ","PublishedAt":"2023-11-16 18:49:23+00:00","OriginURL":"https://staging.blog.mrk.cfdata.org/introducing-advanced-session-audit-capabilities-in-cloudflare-one","SourceName":"Cloudflare"}},{"node":{"ID":5064,"Title":"Build AI apps with PartyRock and Amazon Bedrock","Description":"If you are ready to learn more about Generative AI while having fun and building cool stuff, check out PartyRock.aws . You can experiment, learn all about prompt engineering, build mini-apps, and share them with your friends — all without writing any code or creating an AWS account. You can also start with an app […]","PublishedAt":"2023-11-16 17:14:43+00:00","OriginURL":"https://aws.amazon.com/blogs/aws/build-ai-apps-with-partyrock-and-amazon-bedrock/","SourceName":"AWS"}},{"node":{"ID":5056,"Title":"Elastic Support Hub moves to semantic search","Description":"","PublishedAt":"2023-11-16 00:00:00+00:00","OriginURL":"https://www.elastic.co/blog/elastic-support-hub-moves-to-semantic-search","SourceName":"Elastic"}},{"node":{"ID":5057,"Title":"Switching majors led Kathleen DeRusso to a career in search","Description":"","PublishedAt":"2023-11-16 00:00:00+00:00","OriginURL":"https://www.elastic.co/blog/culture-kathleen-derusso-search-software-engineer","SourceName":"Elastic"}},{"node":{"ID":5058,"Title":"Elastic and University of Canberra: A partnership for student success","Description":"","PublishedAt":"2023-11-16 00:00:00+00:00","OriginURL":"https://www.elastic.co/blog/elastic-university-of-canberra-partnership-student-success","SourceName":"Elastic"}},{"node":{"ID":5059,"Title":"Elastic Agent’s new output to Kafka: Endless possibilities for data collection and streaming","Description":"","PublishedAt":"2023-11-16 00:00:00+00:00","OriginURL":"https://www.elastic.co/blog/elastic-agent-output-kafka-data-collection-streaming","SourceName":"Elastic"}},{"node":{"ID":5061,"Title":"Key learnings from the State of Cloud Security study","Description":"<img class=\"webfeedsFeaturedVisual rss\" src=\"https://imgix.datadoghq.com/img/blog/cloud-security-study-learnings/cloudsec-learnings-hero.png\" width=\"100%\"/>We recently released the State of Cloud Security study, where we analyzed the security posture of thousands of organizations using AWS, Azure, and Google Cloud. In particular, we found that:Long-lived cloud credentials continue to be problematic and expose cloud identities Multi-factor authentication (MFA) is not always consistently enforced for cloud users Adoption of IMDSv2 in AWS is rising, though still insufficient Use of public access blocks on storage buckets varies across cloud platforms and is more prevalent in AWS than Azure A number of cloud workloads have non-administrator permissions that still allow them to access sensitive data or escalate their privileges Many virtual machines are exposed to the internet In this post, we provide key recommendations based on these findings, and we explain how you can leverage Datadog Cloud Security Management (CSM) to improve your security posture.","PublishedAt":"2023-11-16 00:00:00+00:00","OriginURL":"https://www.datadoghq.com/blog/cloud-security-study-learnings/","SourceName":"Datadog"}},{"node":{"ID":5067,"Title":"Blog: Kubernetes Removals, Deprecations, and Major Changes in Kubernetes 1.29","Description":"<p><strong>Authors:</strong> Carol Valencia, Kristin Martin, Abigail McCarthy, James Quigley, Hosam Kamel</p>\n<p>As with every release, Kubernetes v1.29 will introduce feature deprecations and removals. Our continued ability to produce high-quality releases is a testament to our robust development cycle and healthy community. The following are some of the deprecations and removals coming in the Kubernetes 1.29 release.</p>\n<h2 id=\"the-kubernetes-api-removal-and-deprecation-process\">The Kubernetes API removal and deprecation process</h2>\n<p>The Kubernetes project has a well-documented deprecation policy for features. This policy states that stable APIs may only be deprecated when a newer, stable version of that same API is available and that APIs have a minimum lifetime for each stability level. A deprecated API is one that has been marked for removal in a future Kubernetes release; it will continue to function until removal (at least one year from the deprecation), but usage will result in a warning being displayed. Removed APIs are no longer available in the current version, at which point you must migrate to using the replacement.</p>\n<ul>\n<li>Generally available (GA) or stable API versions may be marked as deprecated, but must not be removed within a major version of Kubernetes.</li>\n<li>Beta or pre-release API versions must be supported for 3 releases after deprecation.</li>\n<li>Alpha or experimental API versions may be removed in any release without prior deprecation notice.</li>\n</ul>\n<p>Whether an API is removed as a result of a feature graduating from beta to stable or because that API simply did not succeed, all removals comply with this deprecation policy. Whenever an API is removed, migration options are communicated in the documentation.</p>\n<h2 id=\"a-note-about-the-k8s-gcr-io-redirect-to-registry-k8s-io\">A note about the k8s.gcr.io redirect to registry.k8s.io</h2>\n<p>To host its container images, the Kubernetes project uses a community-owned image registry called registry.k8s.io. Starting last March traffic to the old k8s.gcr.io registry began being redirected to registry.k8s.io. The deprecated k8s.gcr.io registry will eventually be phased out. For more details on this change or to see if you are impacted, please read <a href=\"https://kubernetes.io/blog/2023/03/10/image-registry-redirect/\">k8s.gcr.io Redirect to registry.k8s.io - What You Need to Know</a>.</p>\n<h2 id=\"a-note-about-the-kubernetes-community-owned-package-repositories\">A note about the Kubernetes community-owned package repositories</h2>\n<p>Earlier in 2023, the Kubernetes project <a href=\"https://kubernetes.io/blog/2023/08/15/pkgs-k8s-io-introduction/\">introduced</a> <code>pkgs.k8s.io</code>, community-owned software repositories for Debian and RPM packages. The community-owned repositories replaced the legacy Google-owned repositories (<code>apt.kubernetes.io</code> and <code>yum.kubernetes.io</code>).\nOn September 13, 2023, those legacy repositories were formally deprecated and their contents frozen.</p>\n<p>For more information on this change or to see if you are impacted, please read the <a href=\"https://kubernetes.io/blog/2023/08/31/legacy-package-repository-deprecation/\">deprecation announcement</a>.</p>\n<h2 id=\"deprecations-and-removals-for-kubernetes-v1-29\">Deprecations and removals for Kubernetes v1.29</h2>\n<p>See the official list of <a href=\"https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-29\">API removals</a> for a full list of planned deprecations for Kubernetes v1.29.</p>\n<h3 id=\"removal-of-in-tree-integrations-with-cloud-providers-kep-2395-https-kep-k8s-io-2395\">Removal of in-tree integrations with cloud providers (<a href=\"https://kep.k8s.io/2395\">KEP-2395</a>)</h3>\n<p>The <a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/\">feature gates</a> <code>DisableCloudProviders</code> and <code>DisableKubeletCloudCredentialProviders</code> will both be set to <code>true</code> by default for Kubernetes v1.29. This change will require that users who are currently using in-tree cloud provider integrations (Azure, GCE, or vSphere) enable external cloud controller managers, or opt in to the legacy integration by setting the associated feature gates to <code>false</code>.</p>\n<p>Enabling external cloud controller managers means you must run a suitable cloud controller manager within your cluster's control plane; it also requires setting the command line argument <code>--cloud-provider=external</code> for the kubelet (on every relevant node), and across the control plane (kube-apiserver and kube-controller-manager).</p>\n<p>For more information about how to enable and run external cloud controller managers, read <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/\">Cloud Controller Manager Administration</a> and <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/controller-manager-leader-migration/\">Migrate Replicated Control Plane To Use Cloud Controller Manager</a>.</p>\n<p>For general information about cloud controller managers, please see\n<a href=\"https://kubernetes.io/docs/concepts/architecture/cloud-controller/\">Cloud Controller Manager</a> in the Kubernetes documentation.</p>\n<h3 id=\"removal-of-the-v1beta2-flow-control-api-group\">Removal of the <code>v1beta2</code> flow control API group</h3>\n<p>The <em>flowcontrol.apiserver.k8s.io/v1beta2</em> API version of FlowSchema and PriorityLevelConfiguration will <a href=\"https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-29\">no longer be served</a> in Kubernetes v1.29.</p>\n<p>To prepare for this, you can edit your existing manifests and rewrite client software to use the <code>flowcontrol.apiserver.k8s.io/v1beta3</code> API version, available since v1.26. All existing persisted objects are accessible via the new API. Notable changes in <code>flowcontrol.apiserver.k8s.io/v1beta3</code> include\nthat the PriorityLevelConfiguration <code>spec.limited.assuredConcurrencyShares</code> field was renamed to <code>spec.limited.nominalConcurrencyShares</code>.</p>\n<h3 id=\"deprecation-of-the-status-nodeinfo-kubeproxyversion-field-for-node\">Deprecation of the <code>status.nodeInfo.kubeProxyVersion</code> field for Node</h3>\n<p>The <code>.status.kubeProxyVersion</code> field for Node objects will be <a href=\"https://github.com/kubernetes/enhancements/issues/4004\">marked as deprecated</a> in v1.29 in preparation for its removal in a future release. This field is not accurate and is set by kubelet, which does not actually know the kube-proxy version, or even if kube-proxy is running.</p>\n<h2 id=\"want-to-know-more\">Want to know more?</h2>\n<p>Deprecations are announced in the Kubernetes release notes. You can see the announcements of pending deprecations in the release notes for:</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.25.md#deprecation\">Kubernetes v1.25</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.26.md#deprecation\">Kubernetes v1.26</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.27.md#deprecation\">Kubernetes v1.27</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.28.md#deprecation\">Kubernetes v1.28</a></li>\n</ul>\n<p>We will formally announce the deprecations that come with <a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.29.md#deprecation\">Kubernetes v1.29</a> as part of the CHANGELOG for that release.</p>\n<p>For information on the deprecation and removal process, refer to the official Kubernetes <a href=\"https://kubernetes.io/docs/reference/using-api/deprecation-policy/#deprecating-parts-of-the-api\">deprecation policy</a> document.</p>","PublishedAt":"2023-11-16 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/11/16/kubernetes-1-29-upcoming-changes/","SourceName":"Kubernetes"}},{"node":{"ID":5068,"Title":"Blog: The Case for Kubernetes Resource Limits: Predictability vs. Efficiency","Description":"<p><strong>Author:</strong> Milan Plžík (Grafana Labs)</p>\n<p>There’s been quite a lot of posts suggesting that not using Kubernetes resource limits might be a fairly useful thing (for example, <a href=\"https://home.robusta.dev/blog/stop-using-cpu-limits/\">For the Love of God, Stop Using CPU Limits on Kubernetes</a> or <a href=\"https://erickhun.com/posts/kubernetes-faster-services-no-cpu-limits/\">Kubernetes: Make your services faster by removing CPU limits</a> ). The points made there are totally valid – it doesn’t make much sense to pay for compute power that will not be used due to limits, nor to artificially increase latency. This post strives to argue that limits have their legitimate use as well.</p>\n<p>As a Site Reliability Engineer on the <a href=\"https://grafana.com/\">Grafana Labs</a> platform team, which maintains and improves internal infrastructure and tooling used by the product teams, I primarily try to make Kubernetes upgrades as smooth as possible. But I also spend a lot of time going down the rabbit hole of various interesting Kubernetes issues. This article reflects my personal opinion, and others in the community may disagree.</p>\n<p>Let’s flip the problem upside down. Every pod in a Kubernetes cluster has inherent resource limits – the actual CPU, memory, and other resources of the machine it’s running on. If those physical limits are reached by a pod, it will experience throttling similar to what is caused by reaching Kubernetes limits.</p>\n<h2 id=\"the-problem\">The problem</h2>\n<p>Pods without (or with generous) limits can easily consume the extra resources on the node. This, however, has a hidden cost – the amount of extra resources available often heavily depends on pods scheduled on the particular node and their actual load. These extra resources make each pod a special snowflake when it comes to real resource allocation. Even worse, it’s fairly hard to figure out the resources that the pod had at its disposal at any given moment – certainly not without unwieldy data mining of pods running on a particular node, their resource consumption, and similar. And finally, even if we pass this obstacle, we can only have data sampled up to a certain rate and get profiles only for a certain fraction of our calls. This can be scaled up, but the amount of observability data generated might easily reach diminishing returns. Thus, there’s no easy way to tell if a pod had a quick spike and for a short period of time used twice as much memory as usual to handle a request burst.</p>\n<p>Now, with Black Friday and Cyber Monday approaching, businesses expect a surge in traffic. Good performance data/benchmarks of the past performance allow businesses to plan for some extra capacity. But is data about pods without limits reliable? With memory or CPU instant spikes handled by the extra resources, everything might look good according to past data. But once the pod bin-packing changes and the extra resources get more scarce, everything might start looking different – ranging from request latencies rising negligibly to requests slowly snowballing and causing pod OOM kills. While almost no one actually cares about the former, the latter is a serious issue that requires instant capacity increase.</p>\n<h2 id=\"configuring-the-limits\">Configuring the limits</h2>\n<p>Not using limits takes a tradeoff – it opportunistically improves the performance if there are extra resources available, but lowers predictability of the performance, which might strike back in the future. There are a few approaches that can be used to increase the predictability again. Let’s pick two of them to analyze:</p>\n<ul>\n<li><strong>Configure workload limits to be a fixed (and small) percentage more than the requests</strong> – I'll call it <em>fixed-fraction headroom</em>. This allows the use of some extra shared resources, but keeps the per-node overcommit bound and can be taken to guide worst-case estimates for the workload. Note that the bigger the limits percentage is, the bigger the variance in the performance that might happen across the workloads.</li>\n<li><strong>Configure workloads with <code>requests</code> = <code>limits</code></strong>. From some point of view, this is equivalent to giving each pod its own tiny machine with constrained resources; the performance is fairly predictable. This also puts the pod into the <em>Guaranteed</em> QoS class, which makes it get evicted only after <em>BestEffort</em> and <em>Burstable</em> pods have been evicted by a node under resource pressure (see <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/\">Quality of Service for Pods</a>).</li>\n</ul>\n<p>Some other cases might also be considered, but these are probably the two simplest ones to discuss.</p>\n<h2 id=\"cluster-resource-economy\">Cluster resource economy</h2>\n<p>Note that in both cases discussed above, we’re effectively preventing the workloads from using some cluster resources it has at the cost of getting more predictability – which might sound like a steep price to pay for a bit more stable performance. Let’s try to quantify the impact there.</p>\n<h3 id=\"bin-packing-and-cluster-resource-allocation\">Bin-packing and cluster resource allocation</h3>\n<p>Firstly, let’s discuss bin-packing and cluster resource allocation. There’s some inherent cluster inefficiency that comes to play – it’s hard to achieve 100% resource allocation in a Kubernetes cluster. Thus, some percentage will be left unallocated.</p>\n<p>When configuring fixed-fraction headroom limits, a proportional amount of this will be available to the pods. If the percentage of unallocated resources in the cluster is lower than the constant we use for setting fixed-fraction headroom limits (see the figure, line 2), all the pods together are able to theoretically use up all the node’s resources; otherwise there are some resources that will inevitably be wasted (see the figure, line 1). In order to eliminate the inevitable resource waste, the percentage for fixed-fraction headroom limits should be configured so that it’s at least equal to the expected percentage of unallocated resources.</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2023/11/16/the-case-for-kubernetes-resource-limits/requests-limits-configurations.svg\"\nalt=\"Chart displaying various requests/limits configurations\" width=\"40%\"/>\n</figure>\n<p>For requests = limits (see the figure, line 3), this does not hold: Unless we’re able to allocate all node’s resources, there’s going to be some inevitably wasted resources. Without any knobs to turn on the requests/limits side, the only suitable approach here is to ensure efficient bin-packing on the nodes by configuring correct machine profiles. This can be done either manually or by using a variety of cloud service provider tooling – for example <a href=\"https://karpenter.sh/\">Karpenter</a> for EKS or <a href=\"https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning\">GKE Node auto provisioning</a>.</p>\n<h3 id=\"optimizing-actual-resource-utilization\">Optimizing actual resource utilization</h3>\n<p>Free resources also come in the form of unused resources of other pods (reserved vs. actual CPU utilization, etc.), and their availability can’t be predicted in any reasonable way. Configuring limits makes it next to impossible to utilize these. Looking at this from a different perspective, if a workload wastes a significant amount of resources it has requested, re-visiting its own resource requests might be a fair thing to do. Looking at past data and picking more fitting resource requests might help to make the packing more tight (although at the price of worsening its performance – for example increasing long tail latencies).</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Optimizing resource requests and limits is hard. Although it’s much easier to break things when setting limits, those breakages might help prevent a catastrophe later by giving more insights into how the workload behaves in bordering conditions. There are cases where setting limits makes less sense: batch workloads (which are not latency-sensitive – for example non-live video encoding), best-effort services (don’t need that level of availability and can be preempted), clusters that have a lot of spare resources by design (various cases of specialty workloads – for example services that handle spikes by design).</p>\n<p>On the other hand, setting limits shouldn’t be avoided at all costs – even though figuring out the &quot;right” value for limits is harder and configuring a wrong value yields less forgiving situations. Configuring limits helps you learn about a workload’s behavior in corner cases, and there are simple strategies that can help when reasoning about the right value. It’s a tradeoff between efficient resource usage and performance predictability and should be considered as such.</p>\n<p>There’s also an economic aspect of workloads with spiky resource usage. Having “freebie” resources always at hand does not serve as an incentive to improve performance for the product team. Big enough spikes might easily trigger efficiency issues or even problems when trying to defend a product’s SLA – and thus, might be a good candidate to mention when assessing any risks.</p>","PublishedAt":"2023-11-16 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/11/16/the-case-for-kubernetes-resource-limits/","SourceName":"Kubernetes"}},{"node":{"ID":5053,"Title":"New – Amazon EBS Snapshot Lock","Description":"You can now lock individual Amazon Elastic Block Store (Amazon EBS) snapshots in order to enforce better compliance with your data retention policies. Locked snapshots cannot be deleted until the lock is expired or released, giving you the power to keep critical backups safe from accidental or malicious deletion, including ransomware attacks. The Need for […]","PublishedAt":"2023-11-15 22:59:02+00:00","OriginURL":"https://aws.amazon.com/blogs/aws/new-amazon-ebs-snapshot-lock/","SourceName":"AWS"}},{"node":{"ID":5054,"Title":"Solutions to secret sprawl","Description":"Secret sprawl makes companies more vulnerable to data breaches. Learn how to locate and centralize secrets in a single location for greater security and simplified management and compliance.","PublishedAt":"2023-11-15 19:00:00+00:00","OriginURL":"https://www.hashicorp.com/blog/solutions-to-secret-sprawl","SourceName":"HashiCorp"}},{"node":{"ID":5055,"Title":"How to Use pgvector for Similarity Search on Heroku Postgres","Description":"<h2 class=\"anchored\">\n  <a name=\"introducing-pgvector-for-heroku-postgres\" href=\"#introducing-pgvector-for-heroku-postgres\">Introducing pgvector for Heroku Postgres</a>\n</h2>\n\n<p>Over the past few weeks, we worked on adding <a href=\"https://github.com/pgvector/pgvector\">pgvector</a> as an extension on Heroku Postgres. We're excited to release this feature, and based on the feedback on <a href=\"https://github.com/heroku/roadmap/issues/156\">our public roadmap</a>, many of you are too. We want to share a bit more about how you can use it and how it may be helpful to you. </p>\n\n<p>All <a href=\"https://devcenter.heroku.com/articles/heroku-postgres-plans#plan-tiers\">Standard-tier or higher</a> databases running Postgres 15 now support the <a href=\"https://devcenter.heroku.com/articles/heroku-postgres-extensions-postgis-full-text-search#pgvector\"><code>pgvector</code> extension</a>. You can get started by running <code>CREATE EXTENSION vector;</code> in a client session. Postgres 15 has been the default version on Heroku Postgres since March 2023.  If you're on an older version and want to use pgvector, <a href=\"https://devcenter.heroku.com/articles/upgrading-heroku-postgres-databases\">upgrade</a> to Postgres 15.</p>\n\n<p>The extension adds the vector data type to Heroku Postgres along with additional functions to work with it. Vectors are important for working with large language models and other machine learning applications, as the <a href=\"https://huggingface.co/blog/getting-started-with-embeddings#understanding-embeddings\">embeddings</a> generated by these models are often output in vector format. Working with vectors lets you implement things like similarity search across these embeddings. See our <a href=\"https://blog.heroku.com/pgvector-launch#understanding-pgvector-and-its-significance\">launch blog</a> for more background into what pgvector is, its significance, and ideas for how to use this new data type.</p>\n<h2 class=\"anchored\">\n  <a name=\"an-example-word-vector-similarity-search\" href=\"#an-example-word-vector-similarity-search\">An Example: Word Vector Similarity Search</a>\n</h2>\n\n<p>To show a simple example of how to generate and save vector data to your Heroku database, I'm using the <a href=\"https://wikipedia2vec.github.io/wikipedia2vec/\">Wikipedia2Vec</a> pretrained embeddings. However, you can train your own embeddings or use other models providing embeddings via API, like <a href=\"https://huggingface.co/blog/getting-started-with-embeddings\">HuggingFace</a> or <a href=\"https://openai.com/\">OpenAI</a>. The model you want to use depends on the type of data you're working with. There are models for tasks like computing sentence similarities, searching large texts, or performing image classification. Wikipedia2Vec uses a <a href=\"https://en.wikipedia.org/wiki/Word2vec\">Word2vec</a> algorithm to generate vectors for individual words, which maps similar words close to each other in a continuous vector space. </p>\n\n<p>I like animals, so I want to use Wikipedia2Vec to group similar animals. I’m using the vector embeddings of each animal and the distance between them to find animals that are alike.</p>\n\n<p>If I want to get the embedding for a word from Wikipedia2Vec, I need to use a model. I downloaded one from the <a href=\"https://wikipedia2vec.github.io/wikipedia2vec/pretrained/\">pretrained embeddings</a> on their website. Then I can use their Python module and the function <code>get_word_vector</code> as follows:</p>\n\n<pre><code>from wikipedia2vec import Wikipedia2Vec\nwiki2vec = Wikipedia2Vec.load('enwiki_20180420_100d.pkl')\nwiki2vec.get_word_vector('llama')\n</code></pre>\n\n<p>The output of the vector looks like this:</p>\n\n<pre><code>memmap([-0.15647224,  0.04055957,  0.48439676, -0.22689971, -0.04544162,\n        -0.06538601,  0.22609918, -0.26075622, -0.7195759 , -0.24022003,\n         0.1050799 , -0.5550985 ,  0.4054564 ,  0.14180332,  0.19856507,\n         0.09962048,  0.38372937, -1.1912689 , -0.93939453, -0.28067762,\n         0.04410955,  0.43394643, -0.3429818 ,  0.22209083, -0.46317756,\n        -0.18109794,  0.2775289 , -0.21939017, -0.27015808,  0.72002393,\n        -0.01586861, -0.23480305,  0.365697  ,  0.61743397, -0.07460125,\n        -0.10441436, -0.6537417 ,  0.01339269,  0.06189647, -0.17747395,\n         0.2669941 , -0.03428648, -0.8533792 , -0.09588563, -0.7616592 ,\n        -0.11528812, -0.07127796,  0.28456485, -0.12986512, -0.8063386 ,\n        -0.04875885, -0.27353695, -0.32921   , -0.03807172,  0.10544889,\n         0.49989182, -0.03783042, -0.37752548, -0.19257008,  0.06255971,\n         0.25994852, -0.81092316, -0.15077794,  0.00658835,  0.02033841,\n        -0.32411653, -0.03033727, -0.64633304, -0.43443972, -0.30764043,\n        -0.11036412,  0.04134453, -0.26934972, -0.0289086 , -0.50319433,\n        -0.0204528 , -0.00278326,  0.36589545,  0.5446438 , -0.10852882,\n         0.09699931, -0.01168614,  0.08618425, -0.28925297, -0.25445923,\n         0.63120073,  0.52186656,  0.3439454 ,  0.6686451 ,  0.1076297 ,\n        -0.34688494,  0.05976971, -0.3720558 ,  0.20328045, -0.485623  ,\n        -0.2222396 , -0.22480975,  0.4386788 , -0.7506131 ,  0.14270408],\n       dtype=float32)\n</code></pre>\n\n<p>To get your vector data into your database:</p>\n\n<ol>\n<li>Generate the embeddings.</li>\n<li>Add a column to your database to store your embeddings.</li>\n<li>Save the embeddings to the database.</li>\n</ol>\n\n<p>I already have the embeddings from Wikipedia2Vec, so let’s walk through preparing my database and saving them. When creating a vector column, it's necessary to declare a length for it, so check and see the length of the embedding the model outputs. In my case, the embeddings are 100 numbers long, so I add that column to my table.</p>\n\n<pre><code>CREATE TABLE animals(id serial PRIMARY KEY, name VARCHAR(100), embedding VECTOR(100));\n</code></pre>\n\n<p>From there, save the items you're interested in to your database. You can do it directly in SQL:</p>\n\n<pre><code>INSERT INTO animals(name, embedding) VALUES ('llama', '[-0.15647223591804504, \n…\n-0.7506130933761597, 0.1427040845155716]');\n</code></pre>\n\n<p>But you can also use your <a href=\"https://devcenter.heroku.com/articles/connecting-heroku-postgres\">favorite programming language</a> along with a Postgres client and a <a href=\"https://github.com/pgvector/pgvector#languages\">pgvector library</a>. For this example, I used Python, <a href=\"https://github.com/psycopg/psycopg\">psycopg</a>, and <a href=\"https://github.com/pgvector/pgvector-python\">pgvector-python</a>. Here I'm using the pretrained embedding file to generate embeddings for a list of animals I made, <code>valeries-animals.txt</code>,  and save them to my database.</p>\n\n<pre><code>import psycopg\nfrom pathlib import Path\nfrom pgvector.psycopg import register_vector\nfrom wikipedia2vec import Wikipedia2Vec\n\nwiki2vec = Wikipedia2Vec.load('enwiki_20180420_100d.pkl')\nanimals = Path('valeries-animals.txt').read_text().split('\\n')\n\nwith psycopg.connect(DATABASE_URL, sslmode='require', autocommit=True) as conn:\n    register_vector(conn)\n    cur = conn.cursor()\n    for animal in animals:\n        cur.execute(\"INSERT INTO animals(name, embedding) VALUES (%s, %s)\", (animal, wiki2vec.get_word_vector(animal)))\n</code></pre>\n\n<p>Now that I have the embeddings in my database, I can use pgvector's functions to query them. The extension includes functions to calculate Euclidean distance (<code>&lt;-&gt;</code>), cosine distance (<code>&lt;=&gt;</code>), and inner product (<code>&lt;#&gt;</code>). You can use all three for <a href=\"https://developers.google.com/machine-learning/clustering/similarity/measuring-similarity\">calculating similarity</a> between vectors. Which one you use depends on <a href=\"https://cmry.github.io/notes/euclidean-v-cosine\">your data as well as your use case</a>.</p>\n\n<p>Here I'm using Euclidean distance to find the five animals closest to a shark:</p>\n\n<pre><code>=&gt; SELECT name FROM animals WHERE name != 'shark' ORDER BY embedding &lt;-&gt; (SELECT embedding FROM animals WHERE name = 'shark') LIMIT 5;\n name \n-----------\n crocodile\n dolphin\n whale\n turtle\n alligator\n(5 rows)\n</code></pre>\n\n<p>It works! It's worth noting that the model that we used is based on words appearing together in Wikipedia articles, and different models or source corpuses likely yield different results. The results here are also limited to the hundred or so animals that I added to my database.</p>\n<h2 class=\"anchored\">\n  <a name=\"pgvector-optimization-and-performance-considerations\" href=\"#pgvector-optimization-and-performance-considerations\">pgvector Optimization and Performance Considerations</a>\n</h2>\n\n<p>As you add more vector data to your database, you may notice performance issues or slowness in performing queries. You can index vector data like other columns in Postgres, and pgvector provides a few ways to do so, but there are some important considerations to keep in mind:</p>\n\n<ul>\n<li>Adding an index causes pgvector to switch to using approximate nearest neighbor search instead of exact nearest neighbor search, possibly causing a difference in query results.</li>\n<li>Indexing functions are based on distance calculations, so create one based on the calculation you plan to rely on the most in your application.</li>\n<li>There are two index types supported, IVFFlat and HNSW. Before you add an IVFFlat index, make sure you have some data in your table for better recall.</li>\n</ul>\n\n<p>Check out the <a href=\"https://github.com/pgvector/pgvector#indexing\">pgvector documentation</a> for more information on indexing and other performance considerations.</p>\n<h2 class=\"anchored\">\n  <a name=\"collaborate-and-share-your-pgvector-projects\" href=\"#collaborate-and-share-your-pgvector-projects\">Collaborate and Share Your pgvector Projects</a>\n</h2>\n\n<p>Now that pgvector for Heroku Postgres is out in the world, we're really excited to hear what you do with it! One of pgvector's great advantages is that it lets vector data live alongside all the other data you might already have in Postgres. You can add an embedding column to your existing tables and start experimenting. Our <a href=\"https://blog.heroku.com/pgvector-launch\">launch blog</a> for this feature includes a lot of ideas and possible use cases for how to use this new tool, and I'm sure you can come up with many more. If you have questions, our <a href=\"https://help.heroku.com/\">Support team</a> is available to assist. Don't forget you can share your solutions using the <a href=\"https://devcenter.heroku.com/articles/heroku-button\">Heroku Button</a> on your repo. If you feel like blogging on your success, tag us on social media and we would love to read about it!</p>","PublishedAt":"2023-11-15 18:42:51+00:00","OriginURL":"https://blog.heroku.com/pgvector-for-similarity-search-on-heroku-postgres","SourceName":"Heroku"}},{"node":{"ID":5050,"Title":"Wisdom of Unstructured Data: Building Airbnb’s Listing Knowledge from Big Text Data","Description":"","PublishedAt":"2023-11-15 17:02:20+00:00","OriginURL":"https://medium.com/airbnb-engineering/wisdom-of-unstructured-data-building-airbnbs-listing-knowledge-from-big-text-data-7c533466a63c?source=rss----53c7c27702d5---4","SourceName":"Airbnb"}},{"node":{"ID":5051,"Title":"Watch: Meta’s engineers on building network infrastructure for AI","Description":"<p>Meta is building for the future of AI at every level &#8211; from hardware like MTIA v1, Meta’s first-generation AI inference accelerator to publicly released models like Llama 2, Meta’s next-generation large language model, as well as new generative AI (GenAI) tools like Code Llama. Delivering next-generation AI products and services at Meta’s scale also [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2023/11/15/networking-traffic/watch-metas-engineers-on-building-network-infrastructure-for-ai/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2023/11/15/networking-traffic/watch-metas-engineers-on-building-network-infrastructure-for-ai/\">Watch: Meta’s engineers on building network infrastructure for AI</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n","PublishedAt":"2023-11-15 17:00:16+00:00","OriginURL":"https://engineering.fb.com/2023/11/15/networking-traffic/watch-metas-engineers-on-building-network-infrastructure-for-ai/","SourceName":"Facebook"}},{"node":{"ID":5048,"Title":" Introducing hostname and ASN lists to simplify WAF rules creation ","Description":" Today we are expanding Custom Lists by enabling you to create lists of hostnames and ASNs ","PublishedAt":"2023-11-15 14:00:53+00:00","OriginURL":"http://blog.cloudflare.com/hostname-asn-lists/","SourceName":"Cloudflare"}},{"node":{"ID":5268,"Title":"Introducing hostname and ASN lists to simplify WAF rules creation","Description":" Today we are expanding Custom Lists by enabling you to create lists of hostnames and ASNs ","PublishedAt":"2023-11-15 14:00:53+00:00","OriginURL":"https://blog.cloudflare.com/hostname-asn-lists","SourceName":"Cloudflare"}},{"node":{"ID":5368,"Title":"Introducing hostname and ASN lists to simplify WAF rules creation","Description":" Today we are expanding Custom Lists by enabling you to create lists of hostnames and ASNs ","PublishedAt":"2023-11-15 14:00:53+00:00","OriginURL":"https://staging.blog.mrk.cfdata.org/hostname-asn-lists","SourceName":"Cloudflare"}},{"node":{"ID":5217,"Title":"Psyberg: Automated end to end catch up","Description":"","PublishedAt":"2023-11-15 03:25:23+00:00","OriginURL":"https://netflixtechblog.com/3-psyberg-automated-end-to-end-catch-up-260fbe366fe2?source=rss----2615bd06b42e---4","SourceName":"Netflix"}},{"node":{"ID":5218,"Title":"Diving Deeper into Psyberg: Stateless vs Stateful Data Processing","Description":"","PublishedAt":"2023-11-15 03:25:13+00:00","OriginURL":"https://netflixtechblog.com/2-diving-deeper-into-psyberg-stateless-vs-stateful-data-processing-1d273b3aaefb?source=rss----2615bd06b42e---4","SourceName":"Netflix"}},{"node":{"ID":5219,"Title":"1. Streamlining Membership Data Engineering at Netflix with Psyberg","Description":"","PublishedAt":"2023-11-15 03:24:49+00:00","OriginURL":"https://netflixtechblog.com/1-streamlining-membership-data-engineering-at-netflix-with-psyberg-f68830617dd1?source=rss----2615bd06b42e---4","SourceName":"Netflix"}}]}},"pageContext":{"limit":30,"skip":540,"numPages":193,"currentPage":19}},"staticQueryHashes":["3649515864"]}