{"componentChunkName":"component---src-templates-blog-list-tsx","path":"/page/170","result":{"data":{"allPost":{"edges":[{"node":{"ID":1103,"Title":"Migrating a monolithic service under the bed (part 1 of 3)","Description":"<p>This is the 23rd entry in Mercari Advent Calendar 2021, by @greg.weng from Metadata Ecosystem team. Brief Overview In 2019, Mercari decided to close a four-year-old monolithic service (“Kauru”) and migrate its data and features to microservices. Before it was closed, the Kauru monolithic service cost ¥1,500,000 per month on the Google App Engine 1st [&hellip;]</p>\n","PublishedAt":"2021-12-23 16:36:11+00:00","OriginURL":"https://engineering.mercari.com/en/blog/entry/20211223-migrating-a-monolithic-service-under-the-bed-part-1-of-3/","SourceName":"Mercari"}},{"node":{"ID":1104,"Title":"Exporting our Customers’ Personal Information with care","Description":"<p>This post is for Day 23 of Merpay Advent Calendar 2021, brought to you by @chris from the Merpay KYC team. What’s this export thing about? The KYC team is in charge of saving and retaining the personal information of our customers. In order to comply with laws and regulations, we sometimes have to get [&hellip;]</p>\n","PublishedAt":"2021-12-23 12:00:47+00:00","OriginURL":"https://engineering.mercari.com/en/blog/entry/20211222-exporting-our-customer-personal-information-with-care/","SourceName":"Mercari"}},{"node":{"ID":762,"Title":"How We Saved 70K Cores Across 30 Mission-Critical Services (Large-Scale, Semi-Automated Go GC Tuning @Uber)","Description":"<h1><span style=\"font-weight: 400;\">Introduction</span></h1>\n<p><span style=\"font-weight: 400;\">As part of Uber engineering’s wide efforts to reach profitability, recently our team was focused on reducing cost of compute capacity by improving efficiency. Some of the most impactful work was around GOGC optimization. In this blog we want </span>&#8230;</p>\n<p>The post <a rel=\"nofollow\" href=\"https://eng.uber.com/how-we-saved-70k-cores-across-30-mission-critical-services/\">How We Saved 70K Cores Across 30 Mission-Critical Services (Large-Scale, Semi-Automated Go GC Tuning @Uber)</a> appeared first on <a rel=\"nofollow\" href=\"https://eng.uber.com\">Uber Engineering Blog</a>.</p>\n","PublishedAt":"2021-12-22 17:30:17+00:00","OriginURL":"https://eng.uber.com/how-we-saved-70k-cores-across-30-mission-critical-services/","SourceName":"Uber"}},{"node":{"ID":81,"Title":"CX90: Rethinking, Redesigning and Reimplementing the Groupon User Experience","Description":"","PublishedAt":"2021-12-22 16:57:17+00:00","OriginURL":"https://medium.com/groupon-eng/cx90-rethinking-redesigning-and-reimplementing-the-groupon-user-experience-59a03b6c306c?source=rss----5c13a88f9872---4","SourceName":"Groupon"}},{"node":{"ID":1105,"Title":"The Mercari Client CI/CD Team: Improving the Productivity of a Development Organization That Continues to Expand  #TeamInterview","Description":"<p>CI/CD (continuous integration, continuous delivery) is crucial for developing software and delivering value to users quickly. CI/CD is required at each technology layer at both the backend and frontend. In this article, we spotlight the Client CI/CD team, which provides CI/CD for the mobile app and web versions of Mercari. We interviewed @y-kazama, @kaito, @thi, [&hellip;]</p>\n","PublishedAt":"2021-12-22 12:38:33+00:00","OriginURL":"https://engineering.mercari.com/en/blog/entry/20211125-mercari-client-cicd-interview/","SourceName":"Mercari"}},{"node":{"ID":1106,"Title":"The Merpay QA Team: Maintaining Quality for Safe and Secure Products #TeamInterview","Description":"<p>Chiefly because Merpay provides its users with financial services, maintaining the quality of our services and products is paramount. It’s thanks to the Merpay QA (Quality Assurance) team and the support that they provide, that this is possible. In this entry, we interviewed @gaku and @miisan from the Merpay QA team to hear about their [&hellip;]</p>\n","PublishedAt":"2021-12-22 11:48:58+00:00","OriginURL":"https://engineering.mercari.com/en/blog/entry/20211111-504f3da724/","SourceName":"Mercari"}},{"node":{"ID":1107,"Title":"Interpretability/explainability in machine learning","Description":"<p>* This article is a translation of the Japanese article written on Dec. 24th, 2019. Hello! On day 24 of the Merpay Advent Calendar 2019, I (@yuhi) from the Merpay Machine Learning team will share interpretability in machine learning. Table of Contents What is interpretability in machine learning? Why do we need interpretability? 1. Accountability [&hellip;]</p>\n","PublishedAt":"2021-12-22 10:15:16+00:00","OriginURL":"https://engineering.mercari.com/en/blog/entry/20211222-2019-12-24-070000/","SourceName":"Mercari"}},{"node":{"ID":1108,"Title":"A Deep Dive into Table-Driven Testing in Golang","Description":"<p>This article is the 22nd entry in Merpay Advent Calendar 2021 by @adlerhsieh from the Payment Platform Team. We write a significant amount of Go code in the Merpay backend. While it&#8217;s fun, maintaining the production code quality is a challenge. One essential component to make that happen is to write effective unit tests. Great [&hellip;]</p>\n","PublishedAt":"2021-12-22 10:00:19+00:00","OriginURL":"https://engineering.mercari.com/en/blog/entry/20211221-a-deep-dive-into-table-driven-testing-in-golang/","SourceName":"Mercari"}},{"node":{"ID":1236,"Title":"Blog: Kubernetes-in-Kubernetes and the WEDOS PXE bootable server farm","Description":"<p><strong>Author</strong>: Andrei Kvapil (WEDOS)</p>\n<p>When you own two data centers, thousands of physical servers, virtual machines and hosting for hundreds of thousands sites, Kubernetes can actually simplify the management of all these things. As practice has shown, by using Kubernetes, you can declaratively describe and manage not only applications, but also the infrastructure itself. I work for the largest Czech hosting provider <strong>WEDOS Internet a.s</strong> and today I'll show you two of my projects — <a href=\"https://github.com/kvaps/kubernetes-in-kubernetes\">Kubernetes-in-Kubernetes</a> and <a href=\"https://github.com/kvaps/kubefarm\">Kubefarm</a>.</p>\n<p>With their help you can deploy a fully working Kubernetes cluster inside another Kubernetes using Helm in just a couple of commands. How and why?</p>\n<p>Let me introduce you to how our infrastructure works. All our physical servers can be divided into two groups: <strong>control-plane</strong> and <strong>compute</strong> nodes. Control plane nodes are usually set up manually, have a stable OS installed, and designed to run all cluster services including Kubernetes control-plane. The main task of these nodes is to ensure the smooth operation of the cluster itself. Compute nodes do not have any operating system installed by default, instead they are booting the OS image over the network directly from the control plane nodes. Their work is to carry out the workload.</p>\n<figure>\n<img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/scheme01.svg\"\nalt=\"Kubernetes cluster layout\"/>\n</figure>\n<p>Once nodes have downloaded their image, they can continue to work without keeping connection to the PXE server. That is, a PXE server is just keeping rootfs image and does not hold any other complex logic. After our nodes have booted, we can safely restart the PXE server, nothing critical will happen to them.</p>\n<figure>\n<img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/scheme02.svg\"\nalt=\"Kubernetes cluster after bootstrapping\"/>\n</figure>\n<p>After booting, the first thing our nodes do is join to the existing Kubernetes cluster, namely, execute the <strong>kubeadm join</strong> command so that kube-scheduler could schedule some pods on them and launch various workloads afterwards. From the beginning we used the scheme when nodes were joined into the same cluster used for the control-plane nodes.</p>\n<figure>\n<img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/scheme03.svg\"\nalt=\"Kubernetes scheduling containers to the compute nodes\"/>\n</figure>\n<p>This scheme worked stably for over two years. However later we decided to add containerized Kubernetes to it. And now we can spawn new Kubernetes-clusters very easily right on our control-plane nodes which are now member special admin-clusters. Now, compute nodes can be joined directly to their own clusters - depending on the configuration.</p>\n<figure>\n<img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/scheme04.svg\"\nalt=\"Multiple clusters are running in single Kubernetes, compute nodes joined to them\"/>\n</figure>\n<h2 id=\"kubefarm\">Kubefarm</h2>\n<p>This project came with the goal of enabling anyone to deploy such an infrastructure in just a couple of commands using Helm and get about the same in the end.</p>\n<p>At this time, we moved away from the idea of a monocluster. Because it turned out to be not very convenient for managing work of several development teams in the same cluster. The fact is that Kubernetes was never designed as a multi-tenant solution and at the moment it does not provide sufficient means of isolation between projects. Therefore, running separate clusters for each team turned out to be a good idea. However, there should not be too many clusters, to let them be convenient to manage. Nor is it too small to have sufficient independence between development teams.</p>\n<p>The scalability of our clusters became noticeably better after that change. The more clusters you have per number of nodes, the smaller the failure domain and the more stable they work. And as a bonus, we got a fully declaratively described infrastructure. Thus, now you can deploy a new Kubernetes cluster in the same way as deploying any other application in Kubernetes.</p>\n<p>It uses <a href=\"http://github.com/kvaps/kubernetes-in-kubernetes\">Kubernetes-in-Kubernetes</a> as a basis, <a href=\"https://github.com/ltsp/ltsp/\">LTSP</a> as PXE-server from which the nodes are booted, and automates the DHCP server configuration using <a href=\"https://github.com/kvaps/dnsmasq-controller\">dnsmasq-controller</a>:</p>\n<figure>\n<img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/kubefarm.png\"\nalt=\"Kubefarm\"/>\n</figure>\n<h2 id=\"how-it-works\">How it works</h2>\n<p>Now let's see how it works. In general, if you look at Kubernetes as from an application perspective, you can note that it follows all the principles of <a href=\"https://12factor.net/\">The Twelve-Factor App</a>, and is actually written very well. Thus, it means running Kubernetes as an app in a different Kubernetes shouldn't be a big deal.</p>\n<h3 id=\"running-kubernetes-in-kubernetes\">Running Kubernetes in Kubernetes</h3>\n<p>Now let's take a look at the <a href=\"https://github.com/kvaps/kubernetes-in-kubernetes\">Kubernetes-in-Kubernetes</a> project, which provides a ready-made Helm chart for running Kubernetes in Kubernetes.</p>\n<p>Here is the parameters that you can pass to Helm in the values file:</p>\n<ul>\n<li><a href=\"https://github.com/kvaps/kubernetes-in-kubernetes/tree/v0.13.1/deploy/helm/kubernetes\"><strong>kubernetes/values.yaml</strong></a></li>\n</ul>\n<img alt=\"Kubernetes is just five binaries\" style=\"float: right; max-height: 280px;\" src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/5binaries.png\">\n<p>Beside <strong>persistence</strong> (storage parameters for the cluster), the Kubernetes control-plane components are described here: namely: <strong>etcd cluster</strong>, <strong>apiserver</strong>, <strong>controller-manager</strong> and <strong>scheduler</strong>. These are pretty much standard Kubernetes components. There is a light-hearted saying that “Kubernetes is just five binaries”. So here is where the configuration for these binaries is located.</p>\n<p>If you ever tried to bootstrap a cluster using kubeadm, then this config will remind you it's configuration. But in addition to Kubernetes entities, you also have an admin container. In fact, it is a container which holds two binaries inside: <strong>kubectl</strong> and <strong>kubeadm</strong>. They are used to generate kubeconfig for the above components and to perform the initial configuration for the cluster. Also, in an emergency, you can always exec into it to check and manage your cluster.</p>\n<p>After the release <a href=\"https://asciinema.org/a/407280\">has been deployed</a>, you can see a list of pods: <strong>admin-container</strong>, <strong>apiserver</strong> in two replicas, <strong>controller-manager</strong>, <strong>etcd-cluster</strong>, <strong>scheduller</strong> and the initial job that initializes the cluster. In the end you have a command, which allows you to get shell into the admin container, you can use it to see what is happening inside:</p>\n<p><a href=\"https://asciinema.org/a/407280?autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot01.svg\" alt=\"\"></a></p>\n<p>Also, let's take look at the certificates. If you've ever installed Kubernetes, then you know that it has a <em>scary</em> directory <code>/etc/kubernetes/pki</code> with a bunch of some certificates. In case of Kubernetes-in-Kubernetes, you have fully automated management of them with cert-manager. Thus, it is enough to pass all certificates parameters to Helm during installation, and all the certificates will automatically be generated for your cluster.</p>\n<p><a href=\"https://asciinema.org/a/407280?t=15&amp;autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot02.svg\" alt=\"\"></a></p>\n<p>Looking at one of the certificates, eg. apiserver, you can see that it has a list of DNS names and IP addresses. If you want to make this cluster accessible outside, then just describe the additional DNS names in the values file and update the release. This will update the certificate resource, and cert-manager will regenerate the certificate. You'll no longer need to think about this. If kubeadm certificates need to be renewed at least once a year, here the cert-manager will take care and automatically renew them.</p>\n<p><a href=\"https://asciinema.org/a/407280?t=25&amp;autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot03.svg\" alt=\"\"></a></p>\n<p>Now let's log into the admin container and look at the cluster and nodes. Of course, there are no nodes, yet, because at the moment you have deployed just the blank control-plane for Kubernetes. But in kube-system namespace you can see some coredns pods waiting for scheduling and configmaps already appeared. That is, you can conclude that the cluster is working:</p>\n<p><a href=\"https://asciinema.org/a/407280?t=30&amp;autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot04.svg\" alt=\"\"></a></p>\n<p>Here is the <a href=\"https://kvaps.github.io/images/posts/Kubernetes-in-Kubernetes-and-PXE-bootable-servers-farm/Argo_CD_kink_network.html\">diagram of the deployed cluster</a>. You can see services for all Kubernetes components: <strong>apiserver</strong>, <strong>controller-manager</strong>, <strong>etcd-cluster</strong> and <strong>scheduler</strong>. And the pods on right side to which they forward traffic.</p>\n<p><a href=\"https://kvaps.github.io/images/posts/Kubernetes-in-Kubernetes-and-PXE-bootable-servers-farm/Argo_CD_kink_network.html\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/argocd01.png\" alt=\"\"></a></p>\n<p><em>By the way, the diagram, is drawn in <a href=\"https://argoproj.github.io/argo-cd/\">ArgoCD</a> — the GitOps tool we use to manage our clusters, and cool diagrams are one of its features.</em></p>\n<h3 id=\"orchestrating-physical-servers\">Orchestrating physical servers</h3>\n<p>OK, now you can see the way how is our Kubernetes control-plane deployed, but what about worker nodes, how are we adding them? As I already said, all our servers are bare metal. We do not use virtualization to run Kubernetes, but we orchestrate all physical servers by ourselves.</p>\n<p>Also, we do use Linux network boot feature very actively. Moreover, this is exactly the booting, not some kind of automation of the installation. When the nodes are booting, they just run a ready-made system image for them. That is, to update any node, we just need to reboot it - and it will download a new image. It is very easy, simple and convenient.</p>\n<p>For this, the <a href=\"https://github.com/kvaps/kubefarm\">Kubefarm</a> project was created, which allows you to automate this. The most commonly used examples can be found in the <a href=\"https://github.com/kvaps/kubefarm/tree/v0.13.1/examples\">examples</a> directory. The most standard of them named <a href=\"https://github.com/kvaps/kubefarm/tree/v0.13.1/examples/generic\">generic</a>. Let's take a look at values.yaml:</p>\n<ul>\n<li><a href=\"https://github.com/kvaps/kubefarm/blob/v0.13.1/examples/generic/values.yaml\"><strong>generic/values.yaml</strong></a></li>\n</ul>\n<p>Here you can specify the parameters which are passed into the upstream Kubernetes-in-Kubernetes chart. In order for you control-plane to be accessible from the outside, it is enough to specify the IP address here, but if you wish, you can specify some DNS name here.</p>\n<p>In the PXE server configuration you can specify a timezone. You can also add an SSH key for logging in without a password (but you can also specify a password), as well as kernel modules and parameters that should be applied during booting the system.</p>\n<p>Next comes the <strong>nodePools</strong> configuration, i.e. the nodes themselves. If you've ever used a terraform module for gke, then this logic will remind you of it. Here you statically describe all nodes with a set of parameters:</p>\n<ul>\n<li>\n<p><strong>Name</strong> (hostname);</p>\n</li>\n<li>\n<p><strong>MAC-addresses</strong> — we have nodes with two network cards, and each one can boot from any of the MAC addresses specified here.</p>\n</li>\n<li>\n<p><strong>IP-address</strong>, which the DHCP server should issue to this node.</p>\n</li>\n</ul>\n<p>In this example, you have two pools: the first has five nodes, the second has only one, the second pool has also two tags assigned. Tags are the way to describe configuration for specific nodes. For example, you can add specific DHCP options for some pools, options for the PXE server for booting (e.g. here is debug option enabled) and set of <strong>kubernetesLabels</strong> and <strong>kubernetesTaints</strong> options. What does that mean?</p>\n<p>For example, in this configuration you have a second nodePool with one node. The pool has <strong>debug</strong> and <strong>foo</strong> tags assigned. Now see the options for <strong>foo</strong> tag in <strong>kubernetesLabels</strong>. This means that the m1c43 node will boot with these two labels and taint assigned. Everything seems to be simple. Now <a href=\"https://asciinema.org/a/407282\">let's try</a> this in practice.</p>\n<h3 id=\"demo\">Demo</h3>\n<p>Go to <a href=\"https://github.com/kvaps/kubefarm/tree/v0.13.1/examples\">examples</a> and update previously deployed chart to Kubefarm. Just use the <a href=\"https://github.com/kvaps/kubefarm/tree/v0.13.1/examples/generic\">generic</a> parameters and look at the pods. You can see that a PXE server and one more job were added. This job essentially goes to the deployed Kubernetes cluster and creates a new token. Now it will run repeatedly every 12 hours to generate a new token, so that the nodes can connect to your cluster.</p>\n<p><a href=\"https://asciinema.org/a/407282?autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot05.svg\" alt=\"\"></a></p>\n<p>In a <a href=\"https://kvaps.github.io/images/posts/Kubernetes-in-Kubernetes-and-PXE-bootable-servers-farm/Argo_CD_Applications_kubefarm-network.html\">graphical representation</a>, it looks about the same, but now apiserver started to be exposed outside.</p>\n<p><a href=\"https://kvaps.github.io/images/posts/Kubernetes-in-Kubernetes-and-PXE-bootable-servers-farm/Argo_CD_Applications_kubefarm-network.html\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/argocd02.png\" alt=\"\"></a></p>\n<p>In the diagram, the IP is highlighted in green, the PXE server can be reached through it. At the moment, Kubernetes does not allow creating a single LoadBalancer service for TCP and UDP protocols by default, so you have to create two different services with the same IP address. One is for TFTP, and the second for HTTP, through which the system image is downloaded.</p>\n<p>But this simple example is not always enough, sometimes you might need to modify the logic at boot. For example, here is a directory <a href=\"https://github.com/kvaps/kubefarm/tree/v0.13.1/examples/advanced_network\">advanced_network</a>, inside which there is a <a href=\"https://github.com/kvaps/kubefarm/tree/v0.13.1/examples/advanced_network\">values file</a> with a simple shell script. Let's call it <code>network.sh</code>:</p>\n<ul>\n<li><a href=\"https://github.com/kvaps/kubefarm/blob/v0.13.1/examples/advanced_network/values.yaml#L14-L78\"><strong>network.sh</strong></a></li>\n</ul>\n<p>All this script does is take environment variables at boot time, and generates a network configuration based on them. It creates a directory and puts the netplan config inside. For example, a bonding interface is created here. Basically, this script can contain everything you need. It can hold the network configuration or generate the system services, add some hooks or describe any other logic. Anything that can be described in bash or shell languages will work here, and it will be executed at boot time.</p>\n<p>Let's see how it can be <a href=\"https://asciinema.org/a/407284\">deployed</a>. Let's pass the generic values file as the first parameter, and an additional values file as the second parameter. This is a standard Helm feature. This way you can also pass the secrets, but in this case, the configuration is just expanded by the second file:</p>\n<p><a href=\"https://asciinema.org/a/407284?autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot06.svg\" alt=\"\"></a></p>\n<p>Let's look at the configmap <strong>foo-kubernetes-ltsp</strong> for the netboot server and make sure that <code>network.sh</code> script is really there. These commands used to configure the network at boot time:</p>\n<p><a href=\"https://asciinema.org/a/407284?t=15&amp;autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot07.svg\" alt=\"\"></a></p>\n<p><a href=\"https://asciinema.org/a/407286\">Here</a> you can see how it works in principle. The chassis interface (we use HPE Moonshots 1500) have the nodes, you can enter <code>show node list</code> command to get a list of all the nodes. Now you can see the booting process.</p>\n<p><a href=\"https://asciinema.org/a/407286?autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot08.svg\" alt=\"\"></a></p>\n<p>You can also get their MAC addresses by <code>show node macaddr all</code> command. We have a clever operator that collects MAC-addresses from chassis automatically and passes them to the DHCP server. Actually, it's just creating custom configuration resources for dnsmasq-controller which is running in same admin Kubernetes cluster. Also, trough this interface you can control the nodes themselves, e.g. turn them on and off.</p>\n<p>If you have no such opportunity to enter the chassis through iLO and collect a list of MAC addresses for your nodes, you can consider using <a href=\"https://asciinema.org/a/407287\">catchall cluster</a> pattern. Purely speaking, it is just a cluster with a dynamic DHCP pool. Thus, all nodes that are not described in the configuration to other clusters will automatically join to this cluster.</p>\n<p><a href=\"https://asciinema.org/a/407287?autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot09.svg\" alt=\"\"></a></p>\n<p>For example, you can see a special cluster with some nodes. They are joined to the cluster with an auto-generated name based on their MAC address. Starting from this point you can connect to them and see what happens there. Here you can somehow prepare them, for example, set up the file system and then rejoin them to another cluster.</p>\n<p>Now let's try connecting to the node terminal and see how it is booting. After the BIOS, the network card is configured, here it sends a request to the DHCP server from a specific MAC address, which redirects it to a specific PXE server. Later the kernel and initrd image are downloaded from the server using the standard HTTP protocol:</p>\n<p><a href=\"https://asciinema.org/a/407286?t=28&amp;autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot10.svg\" alt=\"\"></a></p>\n<p>After loading the kernel, the node downloads the rootfs image and transfers control to systemd. Then the booting proceeds as usual, and after that the node joins Kubernetes:</p>\n<p><a href=\"https://asciinema.org/a/407286?t=80&amp;autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot11.svg\" alt=\"\"></a></p>\n<p>If you take a look at <strong>fstab</strong>, you can see only two entries there: <strong>/var/lib/docker</strong> and <strong>/var/lib/kubelet</strong>, they are mounted as <strong>tmpfs</strong> (in fact, from RAM). At the same time, the root partition is mounted as <strong>overlayfs</strong>, so all changes that you make here on the system will be lost on the next reboot.</p>\n<p>Looking into the block devices on the node, you can see some nvme disk, but it has not yet been mounted anywhere. There is also a loop device - this is the exact rootfs image downloaded from the server. At the moment it is located in RAM, occupies 653 MB and mounted with the <strong>loop</strong> option.</p>\n<p>If you look in <strong>/etc/ltsp</strong>, you find the <code>network.sh</code> file that was executed at boot. From containers, you can see running <code>kube-proxy</code> and <code>pause</code> container for it.</p>\n<p><a href=\"https://asciinema.org/a/407286?t=100&amp;autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot12.svg\" alt=\"\"></a></p>\n<h2 id=\"details\">Details</h2>\n<h3 id=\"network-boot-image\">Network Boot Image</h3>\n<p>But where does the main image come from? There is a little trick here. The image for the nodes is built through the <a href=\"https://github.com/kvaps/kubefarm/tree/v0.13.1/build/ltsp\">Dockerfile</a> along with the server. The <a href=\"https://docs.docker.com/develop/develop-images/multistage-build/\">Docker multi-stage build</a> feature allows you to easily add any packages and kernel modules exactly at the stage of the image build. It looks like this:</p>\n<ul>\n<li><a href=\"https://github.com/kvaps/kubefarm/blob/v0.13.1/build/ltsp/Dockerfile\"><strong>Dockerfile</strong></a></li>\n</ul>\n<p>What's going on here? First, we take a regular Ubuntu 20.04 and install all the packages we need. First of all we install the <strong>kernel</strong>, <strong>lvm</strong>, <strong>systemd</strong>, <strong>ssh</strong>. In general, everything that you want to see on the final node should be described here. Here we also install <code>docker</code> with <code>kubelet</code> and <code>kubeadm</code>, which are used to join the node to the cluster.</p>\n<p>And then we perform an additional configuration. In the last stage, we simply install <code>tftp</code> and <code>nginx</code> (which serves our image to clients), <strong>grub</strong> (bootloader). Then root of the previous stages copied into the final image and generate squashed image from it. That is, in fact, we get a docker image, which has both the server and the boot image for our nodes. At the same time, it can be easily updated by changing the Dockerfile.</p>\n<h3 id=\"webhooks-and-api-aggregation-layer\">Webhooks and API aggregation layer</h3>\n<p>I want to pay special attention to the problem of webhooks and aggregation layer. In general, webhooks is a Kubernetes feature that allows you to respond to the creation or modification of any resources. Thus, you can add a handler so that when resources are applied, Kubernetes must send request to some pod and check if configuration of this resource is correct, or make additional changes to it.</p>\n<p>But the point is, in order for the webhooks to work, the apiserver must have direct access to the cluster for which it is running. And if it is started in a separate cluster, like our case, or even separately from any cluster, then Konnectivity service can help us here. Konnectivity is one of the optional but officially supported Kubernetes components.</p>\n<p>Let's take cluster of four nodes for example, each of them is running a <code>kubelet</code> and we have other Kubernetes components running outside: <code>kube-apiserver</code>, <code>kube-scheduler</code> and <code>kube-controller-manager</code>. By default, all these components interact with the apiserver directly - this is the most known part of the Kubernetes logic. But in fact, there is also a reverse connection. For example, when you want to view the logs or run a <code>kubectl exec command</code>, the API server establishes a connection to the specific kubelet independently:</p>\n<figure>\n<img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/konnectivity01.svg\"\nalt=\"Kubernetes apiserver reaching kubelet\"/>\n</figure>\n<p>But the problem is that if we have a webhook, then it usually runs as a standard pod with a service in our cluster. And when apiserver tries to reach it, it will fail because it will try to access an in-cluster service named <strong>webhook.namespace.svc</strong> being outside of the cluster where it is actually running:</p>\n<figure>\n<img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/konnectivity02.svg\"\nalt=\"Kubernetes apiserver can&#39;t reach webhook\"/>\n</figure>\n<p>And here Konnectivity comes to our rescue. Konnectivity is a tricky proxy server developed especially for Kubernetes. It can be deployed as a server next to the apiserver. And Konnectivity-agent is deployed in several replicas directly in the cluster you want to access. The agent establishes a connection to the server and sets up a stable channel to make apiserver able to access all webhooks and all kubelets in the cluster. Thus, now all communication with the cluster will take place through the Konnectivity-server:</p>\n<figure>\n<img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/konnectivity03.svg\"\nalt=\"Kubernetes apiserver reaching webhook via konnectivity\"/>\n</figure>\n<h2 id=\"our-plans\">Our plans</h2>\n<p>Of course, we are not going to stop at this stage. People interested in the project often write to me. And if there will be a sufficient number of interested people, I hope to move Kubernetes-in-Kubernetes project under <a href=\"https://github.com/kubernetes-sigs\">Kubernetes SIGs</a>, by representing it in form of the official Kubernetes Helm chart. Perhaps, by making this project independent we'll gather an even larger community.</p>\n<p>I am also thinking of integrating it with the Machine Controller Manager, which would allow creating worker nodes, not only of physical servers, but also, for example, for creating virtual machines using kubevirt and running them in the same Kubernetes cluster. By the way, it also allows to spawn virtual machines in the clouds, and have a control-plane deployed locally.</p>\n<p>I am also considering the option of integrating with the Cluster-API so that you can create physical Kubefarm clusters directly through the Kubernetes environment. But at the moment I'm not completely sure about this idea. If you have any thoughts on this matter, I'll be happy to listen to them.</p>","PublishedAt":"2021-12-22 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/12/22/kubernetes-in-kubernetes-and-pxe-bootable-server-farm/","SourceName":"Kubernetes"}},{"node":{"ID":239,"Title":"Cloudera Data Engineering 2021 Year End Review","Description":"<p>Since the release of Cloudera Data Engineering (CDE)  more than a year ago, our number one goal was operationalizing Spark pipelines at scale with first class tooling designed to streamline automation and observability.   In working with thousands of customers deploying Spark applications, we saw significant challenges with managing Spark as well as automating, delivering, [&#8230;]</p>\n<p>The post <a rel=\"nofollow\" href=\"https://blog.cloudera.com/cloudera-data-engineering-2021-year-end-review/\">Cloudera Data Engineering 2021 Year End Review</a> appeared first on <a rel=\"nofollow\" href=\"https://blog.cloudera.com\">Cloudera Blog</a>.</p>\n","PublishedAt":"2021-12-21 15:13:32+00:00","OriginURL":"https://blog.cloudera.com/cloudera-data-engineering-2021-year-end-review/","SourceName":"Cloudera"}},{"node":{"ID":1237,"Title":"Blog: Using Admission Controllers to Detect Container Drift at Runtime","Description":"<p><strong>Author:</strong> Saifuding Diliyaer (Box)\n<figure>\n<img src=\"https://kubernetes.io/blog/2021/12/21/admission-controllers-for-container-drift/intro-illustration.png\"\nalt=\"Introductory illustration\"/> <figcaption>\n<p>Illustration by Munire Aireti</p>\n</figcaption>\n</figure>\n</p>\n<p>At Box, we use Kubernetes (K8s) to manage hundreds of micro-services that enable Box to stream data at a petabyte scale. When it comes to the deployment process, we run <a href=\"https://github.com/box/kube-applier\">kube-applier</a> as part of the GitOps workflows with declarative configuration and automated deployment. Developers declare their K8s apps manifest into a Git repository that requires code reviews and automatic checks to pass, before any changes can get merged and applied inside our K8s clusters. With <code>kubectl exec</code> and other similar commands, however, developers are able to directly interact with running containers and alter them from their deployed state. This interaction could then subvert the change control and code review processes that are enforced in our CI/CD pipelines. Further, it allows such impacted containers to continue receiving traffic long-term in production.</p>\n<p>To solve this problem, we developed our own K8s component called <a href=\"https://github.com/box/kube-exec-controller\">kube-exec-controller</a> along with its corresponding <a href=\"https://github.com/box/kube-exec-controller#kubectl-pi\">kubectl plugin</a>. They function together in detecting and terminating potentially mutated containers (caused by interactive kubectl commands), as well as revealing the interaction events directly to the target Pods for better visibility.</p>\n<h2 id=\"admission-control-for-interactive-kubectl-commands\">Admission control for interactive kubectl commands</h2>\n<p>Once a request is sent to K8s, it needs to be authenticated and authorized by the API server to proceed. Additionally, K8s has a separate layer of protection called <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/\">admission controllers</a>, which can intercept the request before an object is persisted in <em>etcd</em>. There are various predefined admission controls compiled into the API server binary (e.g. ResourceQuota to enforce hard resource usage limits per namespace). Besides, there are two dynamic admission controls named <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook\">MutatingAdmissionWebhook</a> and <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook\">ValidatingAdmissionWebhook</a>, used for mutating or validating K8s requests respectively. The latter is what we adopted to detect container drift at runtime caused by interactive kubectl commands. This whole process can be divided into three steps as explained in detail below.</p>\n<h3 id=\"1-admit-interactive-kubectl-command-requests\">1. Admit interactive kubectl command requests</h3>\n<p>First of all, we needed to enable a validating webhook that sends qualified requests to <em>kube-exec-controller</em>. To add the new validation mechanism applying to interactive kubectl commands specifically, we configured the webhook’s rules with resources as <code>[pods/exec, pods/attach]</code>, and operations as <code>CONNECT</code>. These rules tell the cluster's API server that all <code>exec</code> and <code>attach</code> requests should be subject to our admission control webhook. In the ValidatingAdmissionWebhook that we configured, we specified a <code>service</code> reference (could also be replaced with <code>url</code> that gives the location of the webhook) and <code>caBundle</code> to allow validating its X.509 certificate, both under the <code>clientConfig</code> stanza.</p>\n<p>Here is a short example of what our ValidatingWebhookConfiguration object looks like:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>admissionregistration.k8s.io/v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>ValidatingWebhookConfiguration<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>example-validating-webhook-config<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">webhooks</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>validate-pod-interaction.example.com<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">sideEffects</span>:<span style=\"color:#bbb\"> </span>None<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">rules</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">apiGroups</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;*&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">apiVersions</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;*&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">operations</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;CONNECT&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resources</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;pods/exec&#34;</span>,<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;pods/attach&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">failurePolicy</span>:<span style=\"color:#bbb\"> </span>Fail<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">clientConfig</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">service</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#080;font-style:italic\"># reference to kube-exec-controller service deployed inside the K8s cluster</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>example-service<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespace</span>:<span style=\"color:#bbb\"> </span>kube-exec-controller<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">path</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;/admit-pod-interaction&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">caBundle</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;{{VALUE}}&#34;</span><span style=\"color:#bbb\"> </span><span style=\"color:#080;font-style:italic\"># PEM encoded CA bundle to validate kube-exec-controller&#39;s certificate</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">admissionReviewVersions</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;v1&#34;</span>,<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;v1beta1&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><h3 id=\"2-label-the-target-pod-with-potentially-mutated-containers\">2. Label the target Pod with potentially mutated containers</h3>\n<p>Once a request of <code>kubectl exec</code> comes in, <em>kube-exec-controller</em> makes an internal note to label the associated Pod. The added labels mean that we can not only query all the affected Pods, but also enable the security mechanism to retrieve previously identified Pods, in case the controller service itself gets restarted.</p>\n<p>The admission control process cannot directly modify the targeted in its admission response. This is because the <code>pods/exec</code> request is against a subresource of the Pod API, and the API kind for that subresource is <code>PodExecOptions</code>. As a result, there is a separate process in <em>kube-exec-controller</em> that patches the labels asynchronously. The admission control always permits the <code>exec</code> request, then acts as a client of the K8s API to label the target Pod and to log related events. Developers can check whether their Pods are affected or not using <code>kubectl</code> or similar tools. For example:</p>\n<pre tabindex=\"0\"><code>$ kubectl get pod --show-labels\nNAME READY STATUS RESTARTS AGE LABELS\ntest-pod 1/1 Running 0 2s box.com/podInitialInteractionTimestamp=1632524400,box.com/podInteractorUsername=username-1,box.com/podTTLDuration=1h0m0s\n$ kubectl describe pod test-pod\n...\nEvents:\nType Reason Age    From                            Message\n----       ------       ----   ----                            -------\nWarning PodInteraction 5s admission-controller-service Pod was interacted with &#39;kubectl exec&#39; command by user &#39;username-1&#39; initially at time 2021-09-24 16:00:00 -0800 PST\nWarning PodInteraction 5s admission-controller-service Pod will be evicted at time 2021-09-24 17:00:00 -0800 PST (in about 1h0m0s).\n</code></pre><h3 id=\"3-evict-the-target-pod-after-a-predefined-period\">3. Evict the target Pod after a predefined period</h3>\n<p>As you can see in the above event messages, the affected Pod is not evicted immediately. At times, developers might have to get into their running containers necessarily for debugging some live issues. Therefore, we define a time to live (TTL) of affected Pods based on the environment of clusters they are running. In particular, we allow a longer time in our dev clusters as it is more common to run <code>kubectl exec</code> or other interactive commands for active development.</p>\n<p>For our production clusters, we specify a lower time limit so as to avoid the impacted Pods serving traffic abidingly. The <em>kube-exec-controller</em> internally sets and tracks a timer for each Pod that matches the associated TTL. Once the timer is up, the controller evicts that Pod using K8s API. The eviction (rather than deletion) is to ensure service availability, since the cluster respects any configured <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/disruptions/\">PodDisruptionBudget</a> (PDB). Let's say if a user has defined <em>x</em> number of Pods as critical in their PDB, the eviction (as requested by <em>kube-exec-controller</em>) does not continue when the target workload has fewer than <em>x</em> Pods running.</p>\n<p>Here comes a sequence diagram of the entire workflow mentioned above:</p>\n<!-- Mermaid Live Editor link - https://mermaid-js.github.io/mermaid-live-editor/edit/#pako:eNp9kjFPAzEMhf-KlalIbWd0QpUQdGJB3JrFTUyJmjhHzncFof53nGtpqYTYEuu958-Wv4zLnkxjenofiB09BtwWTJbRSS6QCLCHu01ZPdJIMXdUYNZTGYOjRd4zlRvLHRYJLnTIArvbtozV83TbAnZhUcVUrkXo04OU2I6uKu99Cn0fMsNDZik5Rm3SHntYTrRYrabUBl4GBmt2w4acRKAPcrBcLq0Bl1NC9pYnoRouHZopX9RX9aotddJeADaf4DDGwFuQN4IRY_Ao9bunzVvOO13COeYCcR9j3k-OCQDP9KfgC8TJsFbZIHSxnGljzp1lgKs2v9HXugMBwe2WPHTZ94CvottB6Ap5eg2s9cBaUnrLVEP_Yp5ynrOf3fxPV2V1lBOhmZtEJWHweiFfldQa1SWyptGnAuAQxRrLB5UOna6P1j7o4ZhGykBzg4Pk9pPdz_-oOR3ZsXj4BjrP5rU-->\n<p><img src=\"https://kubernetes.io/images/sequence_diagram.svg\" alt=\"Sequence Diagram\"></p>\n<h2 id=\"a-new-kubectl-plugin-for-better-user-experience\">A new kubectl plugin for better user experience</h2>\n<p>Our admission controller component works great for solving the container drift issue we had on the platform. It is also able to submit all related Events to the target Pod that has been affected. However, K8s clusters don't retain Events very long (the default retention period is one hour). We need to provide other ways for developers to get their Pod interaction activity. A <a href=\"https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/\">kubectl plugin</a> is a perfect choice for us to expose this information. We named our plugin <code>kubectl pi</code> (short for <code>pod-interaction</code>) and provide two subcommands: <code>get</code> and <code>extend</code>.</p>\n<p>When the <code>get</code> subcommand is called, the plugin checks the metadata attached by our admission controller and transfers it to human-readable information. Here is an example output from running <code>kubectl pi get</code>:</p>\n<pre tabindex=\"0\"><code>$ kubectl pi get test-pod\nPOD-NAME INTERACTOR POD-TTL EXTENSION EXTENSION-REQUESTER EVICTION-TIME\ntest-pod  username-1  1h0m0s   /          /                    2021-09-24 17:00:00 -0800 PST\n</code></pre><p>The plugin can also be used to extend the TTL for a Pod that is marked for future eviction. This is useful in case developers need extra time to debug ongoing issues. To achieve this, a developer uses the <code>kubectl pi extend</code> subcommand, where the plugin patches the relevant <em>annotations</em> for the given Pod. These <em>annotations</em> include the duration and username who made the extension request for transparency (displayed in the table returned from the <code>kubectl pi get</code> command).</p>\n<p>Correspondingly, there is another webhook defined in <em>kube-exec-controller</em> which admits valid annotation updates. Once admitted, those updates reset the eviction timer of the target Pod as requested. An example of requesting the extension from the developer side would be:</p>\n<pre tabindex=\"0\"><code>$ kubectl pi extend test-pod --duration=30m\nSuccessfully extended the termination time of pod/test-pod with a duration=30m\n \n$ kubectl pi get test-pod\nPOD-NAME  INTERACTOR  POD-TTL  EXTENSION  EXTENSION-REQUESTER  EVICTION-TIME\ntest-pod  username-1  1h0m0s   30m        username-2           2021-09-24 17:30:00 -0800 PST\n</code></pre><h2 id=\"future-improvement\">Future improvement</h2>\n<p>Although our admission controller service works great in handling interactive requests to a Pod, it could as well evict the Pod while the actual commands are no-op in these requests. For instance, developers sometimes run <code>kubectl exec</code> merely to check their service logs stored on hosts. Nevertheless, the target Pods would still get bounced despite the state of their containers not changing at all. One of the improvements here could be adding the ability to distinguish the commands that are passed to the interactive requests, so that no-op commands should not always force a Pod eviction. However, this becomes challenging when developers get a shell to a running container and execute commands inside the shell, since they will no longer be visible to our admission controller service.</p>\n<p>Another item worth pointing out here is the choice of using K8s <em>labels</em> and <em>annotations</em>. In our design, we decided to have all immutable metadata attached as <em>labels</em> for better enforcing the immutability in our admission control. Yet some of these metadata could fit better as <em>annotations</em>. For instance, we had a label with the key <code>box.com/podInitialInteractionTimestamp</code> used to list all affected Pods in <em>kube-exec-controller</em> code, although its value would be unlikely to query for. As a more ideal design in the K8s world, a single <em>label</em> could be preferable in our case for identification with other metadata applied as <em>annotations</em> instead.</p>\n<h2 id=\"summary\">Summary</h2>\n<p>With the power of admission controllers, we are able to secure our K8s clusters by detecting potentially mutated containers at runtime, and evicting their Pods without affecting service availability. We also utilize kubectl plugins to provide flexibility of the eviction time and hence, bringing a better and more self-independent experience to service owners. We are proud to announce that we have open-sourced the whole project for the community to leverage in their own K8s clusters. Any contribution is more than welcomed and appreciated. You can find this project hosted on GitHub at <a href=\"https://github.com/box/kube-exec-controller\">https://github.com/box/kube-exec-controller</a></p>\n<p><em>Special thanks to Ayush Sobti and Ethan Goldblum for their technical guidance on this project.</em></p>","PublishedAt":"2021-12-21 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/12/21/admission-controllers-for-container-drift/","SourceName":"Kubernetes"}},{"node":{"ID":464,"Title":"The ultimate guide to cohort analysis: How to reduce churn and strengthen your product","Description":"<figure><img src=\"https://mixpanel.com/wp-content/uploads/2018/08/20211208_BlogHero_Cohorts-1024x576.png\" class=\"type:primaryImage\" /></figure>\n<p>Cohort analysis is one of the best ways product analytics can help you both acquire and retain customers.&#160; By digging into actual behavioral data overtime, instead of relying solely on interviews and feedback, you can get a crystal clear picture of a user journey—including where the value moments and roadblocks lay.&#160; With this level of</p>\n<p>The post <a rel=\"nofollow\" href=\"https://mixpanel.com/blog/cohort-analysis/\">The ultimate guide to cohort analysis: How to reduce churn and strengthen your product</a> appeared first on <a rel=\"nofollow\" href=\"https://mixpanel.com\">Mixpanel</a>.</p>\n","PublishedAt":"2021-12-20 21:51:00+00:00","OriginURL":"https://mixpanel.com/blog/cohort-analysis/","SourceName":"Mixpanel"}},{"node":{"ID":82,"Title":"Felix, the “DevOps” between designers and developers","Description":"","PublishedAt":"2021-12-20 16:23:45+00:00","OriginURL":"https://medium.com/groupon-eng/felix-the-devops-between-designers-and-developers-c2976572e9f?source=rss----5c13a88f9872---4","SourceName":"Groupon"}},{"node":{"ID":240,"Title":"Recognizing Organizations Leading the Way in Data Security & Governance","Description":"<p>The importance of including centralized data management, security, and governance into data projects from the start</p>\n<p>The post <a rel=\"nofollow\" href=\"https://blog.cloudera.com/recognizing-organizations-leading-the-way-in-data-security-governance/\">Recognizing Organizations Leading the Way in Data Security &#038; Governance</a> appeared first on <a rel=\"nofollow\" href=\"https://blog.cloudera.com\">Cloudera Blog</a>.</p>\n","PublishedAt":"2021-12-20 13:55:36+00:00","OriginURL":"https://blog.cloudera.com/recognizing-organizations-leading-the-way-in-data-security-governance/","SourceName":"Cloudera"}},{"node":{"ID":1109,"Title":"Kubernetes based autoscaler for Cloud Spanner","Description":"<p>This article was published on day 22nd of Merpay Advent Calendar 2021, brought to you by @ravi from the Merpay SRE team. When it comes to DBaaS, all the cloud providers offer some solutions for meeting various kinds of user requirements. Google’s Cloud Spanner is one such solution from their list of available database options. [&hellip;]</p>\n","PublishedAt":"2021-12-20 10:00:23+00:00","OriginURL":"https://engineering.mercari.com/en/blog/entry/20211222-kubernetes-based-spanner-autoscaler/","SourceName":"Mercari"}},{"node":{"ID":465,"Title":"Mixpanel Community: Top posts from 2021","Description":"<figure><img src=\"https://mixpanel.com/wp-content/uploads/2021/12/MXP-Blog-TopPosts2021-1920x1080-1-1024x576.png\" class=\"type:primaryImage\" /></figure>\n<p>As Community Manager at Mixpanel, I get to engage with Mixpanel users on a daily basis.&#160; Honestly, it’s the best. From in-the-weeds guidance on specific integrations to big picture best practices on product analytics, I love the kind of knowledge our members seek out—and what they share in return. If you didn’t know Mixpanel had</p>\n<p>The post <a rel=\"nofollow\" href=\"https://mixpanel.com/blog/mixpanel-community-top-posts-from-2021/\">Mixpanel Community: Top posts from 2021</a> appeared first on <a rel=\"nofollow\" href=\"https://mixpanel.com\">Mixpanel</a>.</p>\n","PublishedAt":"2021-12-17 17:32:00+00:00","OriginURL":"https://mixpanel.com/blog/mixpanel-community-top-posts-from-2021/","SourceName":"Mixpanel"}},{"node":{"ID":241,"Title":"#ClouderaLife Spotlight: Manoj Shanmugasundaram – Principal Solutions Engineer","Description":"<p>Manoj Shanmugasundaram has been with Cloudera for 5 and a half years bringing his talents to our Solutions Engineering team.  As a Principal Solutions Engineer, he says his core responsibility is “to take Cloudera&#8217;s latest and greatest technology and meet a customer&#8217;s complex business requirements, across the data lifecycle, on any cloud or the datacenter.” [&#8230;]</p>\n<p>The post <a rel=\"nofollow\" href=\"https://blog.cloudera.com/clouderalife-spotlight-manoj-shanmugasundaram-principal-solutions-engineer/\">#ClouderaLife Spotlight: Manoj Shanmugasundaram &#8211; Principal Solutions Engineer</a> appeared first on <a rel=\"nofollow\" href=\"https://blog.cloudera.com\">Cloudera Blog</a>.</p>\n","PublishedAt":"2021-12-17 14:06:27+00:00","OriginURL":"https://blog.cloudera.com/clouderalife-spotlight-manoj-shanmugasundaram-principal-solutions-engineer/","SourceName":"Cloudera"}},{"node":{"ID":609,"Title":"Presenting @trivago/prettier-plugin-sort-imports","Description":"I’m happy to share that trivago has released a Prettier plugin which sorts import declarations in TypeSCript and JavaScript modules for a given configured order. Throughout this article, I’ll explain to you the motivation behind this Prettier plugin and how it works in detail.","PublishedAt":"2021-12-17 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2021-12-17-aprettierpluginthatsortsyourimports/","SourceName":"Trivago"}},{"node":{"ID":1238,"Title":"Blog: What's new in Security Profiles Operator v0.4.0","Description":"<p><strong>Authors:</strong> Jakub Hrozek, Juan Antonio Osorio, Paulo Gomes, Sascha Grunert</p>\n<hr>\n<p>The <a href=\"https://sigs.k8s.io/security-profiles-operator\">Security Profiles Operator (SPO)</a>\nis an out-of-tree Kubernetes enhancement to make the management of\n<a href=\"https://en.wikipedia.org/wiki/Seccomp\">seccomp</a>,\n<a href=\"https://en.wikipedia.org/wiki/Security-Enhanced_Linux\">SELinux</a> and\n<a href=\"https://en.wikipedia.org/wiki/AppArmor\">AppArmor</a> profiles easier and more\nconvenient. We're happy to announce that we recently <a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/releases/tag/v0.4.0\">released\nv0.4.0</a>\nof the operator, which contains a ton of new features, fixes and usability\nimprovements.</p>\n<h2 id=\"what-s-new\">What's new</h2>\n<p>It has been a while since the last\n<a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/releases/tag/v0.3.0\">v0.3.0</a>\nrelease of the operator. We added new features, fine-tuned existing ones and\nreworked our documentation in 290 commits over the past half year.</p>\n<p>One of the highlights is that we're now able to record seccomp and SELinux\nprofiles using the operators <a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#log-enricher-based-recording\">log enricher</a>.\nThis allows us to reduce the dependencies required for profile recording to have\n<a href=\"https://linux.die.net/man/8/auditd\">auditd</a> or\n<a href=\"https://en.wikipedia.org/wiki/Syslog\">syslog</a> (as fallback) running on the\nnodes. All profile recordings in the operator work in the same way by using the\n<code>ProfileRecording</code> CRD as well as their corresponding <a href=\"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\">label\nselectors</a>. The log\nenricher itself can be also used to gather meaningful insights about seccomp and\nSELinux messages of a node. Checkout the <a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#using-the-log-enricher\">official\ndocumentation</a>\nto learn more about it.</p>\n<h3 id=\"seccomp-related-improvements\">seccomp related improvements</h3>\n<p>Beside the log enricher based recording we now offer an alternative to record\nseccomp profiles by utilizing <a href=\"https://ebpf.io\">ebpf</a>. This optional feature can\nbe enabled by setting <code>enableBpfRecorder</code> to <code>true</code>. This results in running a\ndedicated container, which ships a custom bpf module on every node to collect\nthe syscalls for containers. It even supports older Kernel versions which do not\nexpose the <a href=\"https://www.kernel.org/doc/html/latest/bpf/btf.html\">BPF Type Format (BTF)</a> per\ndefault as well as the <code>amd64</code> and <code>arm64</code> architectures. Checkout\n<a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#ebpf-based-recording\">our documentation</a>\nto see it in action. By the way, we now add the seccomp profile architecture of\nthe recorder host to the recorded profile as well.</p>\n<p>We also graduated the seccomp profile API from <code>v1alpha1</code> to <code>v1beta1</code>. This\naligns with our overall goal to stabilize the CRD APIs over time. The only thing\nwhich has changed is that the seccomp profile type <code>Architectures</code> now points to\n<code>[]Arch</code> instead of <code>[]*Arch</code>.</p>\n<h3 id=\"selinux-enhancements\">SELinux enhancements</h3>\n<p>Managing SELinux policies (an equivalent to using <code>semodule</code> that\nyou would normally call on a single server) is not done by SPO\nitself, but by another container called selinuxd to provide better\nisolation. This release switched to using selinuxd containers from\na personal repository to images located under <a href=\"https://quay.io/organization/security-profiles-operator\">our team's quay.io\nrepository</a>.\nThe selinuxd repository has moved as well to <a href=\"https://github.com/containers/selinuxd\">the containers GitHub\norganization</a>.</p>\n<p>Please note that selinuxd links dynamically to <code>libsemanage</code> and mounts the\nSELinux directories from the nodes, which means that the selinuxd container\nmust be running the same distribution as the cluster nodes. SPO defaults\nto using CentOS-8 based containers, but we also build Fedora based ones.\nIf you are using another distribution and would like us to add support for\nit, please file <a href=\"https://github.com/containers/selinuxd/issues\">an issue against selinuxd</a>.</p>\n<h4 id=\"profile-recording\">Profile Recording</h4>\n<p>This release adds support for recording of SELinux profiles.\nThe recording itself is managed via an instance of a <code>ProfileRecording</code> Custom\nResource as seen in an\n<a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/blob/main/examples/profilerecording-selinux-logs.yaml\">example</a>\nin our repository. From the user's point of view it works pretty much the same\nas recording of seccomp profiles.</p>\n<p>Under the hood, to know what the workload is doing SPO installs a special\npermissive policy called <a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/blob/main/deploy/base/profiles/selinuxrecording.cil\">selinuxrecording</a>\non startup which allows everything and logs all AVCs to <code>audit.log</code>.\nThese AVC messages are scraped by the log enricher component and when\nthe recorded workload exits, the policy is created.</p>\n<h4 id=\"selinuxprofile-crd-graduation\"><code>SELinuxProfile</code> CRD graduation</h4>\n<p>An <code>v1alpha2</code> version of the <code>SelinuxProfile</code> object has been introduced. This\nremoves the raw Common Intermediate Language (CIL) from the object itself and\ninstead adds a simple policy language to ease the writing and parsing\nexperience.</p>\n<p>Alongside, a <code>RawSelinuxProfile</code> object was also introduced. This contains a\nwrapped and raw representation of the policy. This was intended for folks to be\nable to take their existing policies into use as soon as possible. However, on\nvalidations are done here.</p>\n<h3 id=\"apparmor-support\">AppArmor support</h3>\n<p>This version introduces the initial support for AppArmor, allowing users to load and\nunload AppArmor profiles into cluster nodes by using the new <a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/blob/main/deploy/base/crds/apparmorprofile.yaml\">AppArmorProfile</a> CRD.</p>\n<p>To enable AppArmor support use the <a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/blob/main/examples/config.yaml#L10\">enableAppArmor feature gate</a> switch of your SPO configuration.\nThen use our <a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/blob/main/examples/apparmorprofile.yaml\">apparmor example</a> to deploy your first profile across your cluster.</p>\n<h3 id=\"metrics\">Metrics</h3>\n<p>The operator now exposes metrics, which are described in detail in\nour new <a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#using-metrics\">metrics documentation</a>.\nWe decided to secure the metrics retrieval process by using\n<a href=\"https://github.com/brancz/kube-rbac-proxy\">kube-rbac-proxy</a>, while we ship an\nadditional <code>spo-metrics-client</code> cluster role (and binding) to retrieve the\nmetrics from within the cluster. If you're using\n<a href=\"https://www.redhat.com/en/technologies/cloud-computing/openshift\">OpenShift</a>,\nthen we provide an out of the box working\n<a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#automatic-servicemonitor-deployment\"><code>ServiceMonitor</code></a>\nto access the metrics.</p>\n<h4 id=\"debuggability-and-robustness\">Debuggability and robustness</h4>\n<p>Beside all those new features, we decided to restructure parts of the Security\nProfiles Operator internally to make it better to debug and more robust. For\nexample, we now maintain an internal <a href=\"https://grpc.io\">gRPC</a> API to communicate\nwithin the operator across different features. We also improved the performance\nof the log enricher, which now caches results for faster retrieval of the log\ndata. The operator can be put into a more <a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#set-logging-verbosity\">verbose log mode</a>\nby setting <code>verbosity</code> from <code>0</code> to <code>1</code>.</p>\n<p>We also print the used <code>libseccomp</code> and <code>libbpf</code> versions on startup, as well as\nexpose CPU and memory profiling endpoints for each container via the\n<a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#enable-cpu-and-memory-profiling\"><code>enableProfiling</code> option</a>.\nDedicated liveness and startup probes inside of the operator daemon will now\nadditionally improve the life cycle of the operator.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Thank you for reading this update. We're looking forward to future enhancements\nof the operator and would love to get your feedback about the latest release.\nFeel free to reach out to us via the Kubernetes slack\n<a href=\"https://kubernetes.slack.com/messages/security-profiles-operator\">#security-profiles-operator</a>\nfor any feedback or question.</p>","PublishedAt":"2021-12-17 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/12/17/security-profiles-operator/","SourceName":"Kubernetes"}},{"node":{"ID":763,"Title":"Cadence Multi-Tenant Task Processing","Description":"<h1><span style=\"font-weight: 400;\">Introduction</span></h1>\n<p><span style=\"font-weight: 400;\">Cadence is a multi-tenant orchestration framework that helps developers at Uber to write fault-tolerant, long-running applications, also known as workflows. It scales horizontally to handle millions of concurrent executions from various customers. It is currently used by hundreds of </span>&#8230;</p>\n<p>The post <a rel=\"nofollow\" href=\"https://eng.uber.com/cadence-multi-tenant-task-processing/\">Cadence Multi-Tenant Task Processing</a> appeared first on <a rel=\"nofollow\" href=\"https://eng.uber.com\">Uber Engineering Blog</a>.</p>\n","PublishedAt":"2021-12-16 17:30:14+00:00","OriginURL":"https://eng.uber.com/cadence-multi-tenant-task-processing/","SourceName":"Uber"}},{"node":{"ID":242,"Title":"How To Overcome Hybrid Cloud Migration Roadblocks","Description":"<p>Hybrid cloud data services and analytics is becoming a new enterprise standard as much of the global workforce has switched to a hybrid way of working. To implement hybrid data strategies, IT leaders need tactics to overcome internal and external hurdles.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://blog.cloudera.com/how-to-overcome-hybrid-cloud-migration-roadblocks/\">How To Overcome Hybrid Cloud Migration Roadblocks</a> appeared first on <a rel=\"nofollow\" href=\"https://blog.cloudera.com\">Cloudera Blog</a>.</p>\n","PublishedAt":"2021-12-16 14:24:34+00:00","OriginURL":"https://blog.cloudera.com/how-to-overcome-hybrid-cloud-migration-roadblocks/","SourceName":"Cloudera"}},{"node":{"ID":1110,"Title":"Mercari&#8217;s Opensource Supporter Project","Description":"<p>This is the 16th entry in Mercari Advent Calendar 2021, by @lestrrat from Engineering Office. In this Advent Calendar entry, I’d like to describe our activities to support FOSS projects and communities in a more direct manner, which is something we have been working on for the last few months. Without further ado, we are [&hellip;]</p>\n","PublishedAt":"2021-12-16 11:00:48+00:00","OriginURL":"https://engineering.mercari.com/en/blog/entry/mercaris-opensource-supporter-project/","SourceName":"Mercari"}},{"node":{"ID":1239,"Title":"Blog: Kubernetes 1.23: StatefulSet PVC Auto-Deletion (alpha)","Description":"<p><strong>Author:</strong> Matthew Cary (Google)</p>\n<p>Kubernetes v1.23 introduced a new, alpha-level policy for\n<a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\">StatefulSets</a> that controls the lifetime of\n<a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/\">PersistentVolumeClaims</a> (PVCs) generated from the\nStatefulSet spec template for cases when they should be deleted automatically when the StatefulSet\nis deleted or pods in the StatefulSet are scaled down.</p>\n<h2 id=\"what-problem-does-this-solve\">What problem does this solve?</h2>\n<p>A StatefulSet spec can include Pod and PVC templates. When a replica is first created, the\nKubernetes control plane creates a PVC for that replica if one does not already exist. The behavior\nbefore Kubernetes v1.23 was that the control plane never cleaned up the PVCs created for\nStatefulSets - this was left up to the cluster administrator, or to some add-on automation that\nyou’d have to find, check suitability, and deploy. The common pattern for managing PVCs, either\nmanually or through tools such as Helm, is that the PVCs are tracked by the tool that manages them,\nwith explicit lifecycle. Workflows that use StatefulSets must determine on their own what PVCs are\ncreated by a StatefulSet and what their lifecycle should be.</p>\n<p>Before this new feature, when a StatefulSet-managed replica disappears, either because the\nStatefulSet is reducing its replica count, or because its StatefulSet is deleted, the PVC and its\nbacking volume remains and must be manually deleted. While this behavior is appropriate when the\ndata is critical, in many cases the persistent data in these PVCs is either temporary, or can be\nreconstructed from another source. In those cases, PVCs and their backing volumes remaining after\ntheir StatefulSet or replicas have been deleted are not necessary, incur cost, and require manual\ncleanup.</p>\n<h2 id=\"the-new-statefulset-pvc-retention-policy\">The new StatefulSet PVC retention policy</h2>\n<p>If you enable the alpha feature, a StatefulSet spec includes a PersistentVolumeClaim retention\npolicy. This is used to control if and when PVCs created from a StatefulSet’s <code>volumeClaimTemplate</code>\nare deleted. This first iteration of the retention policy contains two situations where PVCs may be\ndeleted.</p>\n<p>The first situation is when the StatefulSet resource is deleted (which implies that all replicas are\nalso deleted). This is controlled by the <code>whenDeleted</code> policy. The second situation, controlled by\n<code>whenScaled</code> is when the StatefulSet is scaled down, which removes some but not all of the replicas\nin a StatefulSet. In both cases the policy can either be <code>Retain</code>, where the corresponding PVCs are\nnot touched, or <code>Delete</code>, which means that PVCs are deleted. The deletion is done with a normal\n<a href=\"https://kubernetes.io/docs/concepts/architecture/garbage-collection/\">object deletion</a>, so that, for example, all\nretention policies for the underlying PV are respected.</p>\n<p>This policy forms a matrix with four cases. I’ll walk through and give an example for each one.</p>\n<ul>\n<li>\n<p><strong><code>whenDeleted</code> and <code>whenScaled</code> are both <code>Retain</code>.</strong> This matches the existing behavior for\nStatefulSets, where no PVCs are deleted. This is also the default retention policy. It’s\nappropriate to use when data on StatefulSet volumes may be irreplaceable and should only be\ndeleted manually.</p>\n</li>\n<li>\n<p><strong><code>whenDeleted</code> is <code>Delete</code> and <code>whenScaled</code> is <code>Retain</code>.</strong> In this case, PVCs are deleted only when\nthe entire StatefulSet is deleted. If the StatefulSet is scaled down, PVCs are not touched,\nmeaning they are available to be reattached if a scale-up occurs with any data from the previous\nreplica. This might be used for a temporary StatefulSet, such as in a CI instance or ETL\npipeline, where the data on the StatefulSet is needed only during the lifetime of the\nStatefulSet lifetime, but while the task is running the data is not easily reconstructible. Any\nretained state is needed for any replicas that scale down and then up.</p>\n</li>\n<li>\n<p><strong><code>whenDeleted</code> and <code>whenScaled</code> are both <code>Delete</code>.</strong> PVCs are deleted immediately when their\nreplica is no longer needed. Note this does not include when a Pod is deleted and a new version\nrescheduled, for example when a node is drained and Pods need to migrate elsewhere. The PVC is\ndeleted only when the replica is no longer needed as signified by a scale-down or StatefulSet\ndeletion. This use case is for when data does not need to live beyond the life of its\nreplica. Perhaps the data is easily reconstructable and the cost savings of deleting unused PVCs\nis more important than quick scale-up, or perhaps that when a new replica is created, any data\nfrom a previous replica is not usable and must be reconstructed anyway.</p>\n</li>\n<li>\n<p><strong><code>whenDeleted</code> is <code>Retain</code> and <code>whenScaled</code> is <code>Delete</code>.</strong> This is similar to the previous case,\nwhen there is little benefit to keeping PVCs for fast reuse during scale-up. An example of a\nsituation where you might use this is an Elasticsearch cluster. Typically you would scale that\nworkload up and down to match demand, whilst ensuring a minimum number of replicas (for example:\n3). When scaling down, data is migrated away from removed replicas and there is no benefit to\nretaining those PVCs. However, it can be useful to bring the entire Elasticsearch cluster down\ntemporarily for maintenance. If you need to take the Elasticsearch system offline, you can do\nthis by temporarily deleting the StatefulSet, and then bringing the Elasticsearch cluster back\nby recreating the StatefulSet. The PVCs holding the Elasticsearch data will still exist and the\nnew replicas will automatically use them.</p>\n</li>\n</ul>\n<p>Visit the\n<a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-policies\">documentation</a> to\nsee all the details.</p>\n<h2 id=\"what-s-next\">What’s next?</h2>\n<p>Enable the feature and try it out! Enable the <code>StatefulSetAutoDeletePVC</code> feature gate on a cluster,\nthen create a StatefulSet using the new policy. Test it out and tell us what you think!</p>\n<p>I'm very curious to see if this owner reference mechanism works well in practice. For example, we\nrealized there is no mechanism in Kubernetes for knowing who set a reference, so it’s possible that\nthe StatefulSet controller may fight with custom controllers that set their own\nreferences. Fortunately, maintaining the existing retention behavior does not involve any new owner\nreferences, so default behavior will be compatible.</p>\n<p>Please tag any issues you report with the label <code>sig/apps</code> and assign them to Matthew Cary\n(<a href=\"https://github.com/mattcary\">@mattcary</a> at GitHub).</p>\n<p>Enjoy!</p>","PublishedAt":"2021-12-16 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/12/16/kubernetes-1-23-statefulset-pvc-auto-deletion/","SourceName":"Kubernetes"}},{"node":{"ID":1240,"Title":"Blog: Kubernetes 1.23: Prevent PersistentVolume leaks when deleting out of order","Description":"<p><strong>Author:</strong> Deepak Kinni (VMware)</p>\n<p><a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/\">PersistentVolume</a> (or PVs for short) are\nassociated with <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#reclaim-policy\">Reclaim Policy</a>.\nThe Reclaim Policy is used to determine the actions that need to be taken by the storage\nbackend on deletion of the PV.\nWhere the reclaim policy is <code>Delete</code>, the expectation is that the storage backend\nreleases the storage resource that was allocated for the PV. In essence, the reclaim\npolicy needs to honored on PV deletion.</p>\n<p>With the recent Kubernetes v1.23 release, an alpha feature lets you configure your\ncluster to behave that way and honor the configured reclaim policy.</p>\n<h2 id=\"how-did-reclaim-work-in-previous-kubernetes-releases\">How did reclaim work in previous Kubernetes releases?</h2>\n<p><a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#Introduction\">PersistentVolumeClaim</a> (or PVC for short) is\na request for storage by a user. A PV and PVC are considered <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#Binding\">Bound</a>\nif there is a newly created PV or a matching PV is found. The PVs themselves are\nbacked by a volume allocated by the storage backend.</p>\n<p>Normally, if the volume is to be deleted, then the expectation is to delete the\nPVC for a bound PV-PVC pair. However, there are no restrictions to delete a PV\nprior to deleting a PVC.</p>\n<p>First, I'll demonstrate the behavior for clusters that are running an older version of Kubernetes.</p>\n<h4 id=\"retrieve-an-pvc-that-is-bound-to-a-pv\">Retrieve an PVC that is bound to a PV</h4>\n<p>Retrieve an existing PVC <code>example-vanilla-block-pvc</code></p>\n<pre tabindex=\"0\"><code>kubectl get pvc example-vanilla-block-pvc\n</code></pre><p>The following output shows the PVC and it's <code>Bound</code> PV, the PV is shown under the <code>VOLUME</code> column:</p>\n<pre tabindex=\"0\"><code>NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE\nexample-vanilla-block-pvc Bound pvc-6791fdd4-5fad-438e-a7fb-16410363e3da 5Gi RWO example-vanilla-block-sc 19s\n</code></pre><h4 id=\"delete-pv\">Delete PV</h4>\n<p>When I try to delete a bound PV, the cluster blocks and the <code>kubectl</code> tool does\nnot return back control to the shell; for example:</p>\n<pre tabindex=\"0\"><code>kubectl delete pv pvc-6791fdd4-5fad-438e-a7fb-16410363e3da\n</code></pre><pre tabindex=\"0\"><code>persistentvolume &#34;pvc-6791fdd4-5fad-438e-a7fb-16410363e3da&#34; deleted\n^C\n</code></pre><p>Retrieving the PV:</p>\n<pre tabindex=\"0\"><code>kubectl get pv pvc-6791fdd4-5fad-438e-a7fb-16410363e3da\n</code></pre><p>It can be observed that the PV is in <code>Terminating</code> state</p>\n<pre tabindex=\"0\"><code>NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE\npvc-6791fdd4-5fad-438e-a7fb-16410363e3da 5Gi RWO Delete Terminating default/example-vanilla-block-pvc example-vanilla-block-sc 2m23s\n</code></pre><h4 id=\"delete-pvc\">Delete PVC</h4>\n<pre tabindex=\"0\"><code>kubectl delete pvc example-vanilla-block-pvc\n</code></pre><p>The following output is seen if the PVC gets successfully deleted:</p>\n<pre tabindex=\"0\"><code>persistentvolumeclaim &#34;example-vanilla-block-pvc&#34; deleted\n</code></pre><p>The PV object from the cluster also gets deleted. When attempting to retrieve the PV\nit will be observed that the PV is no longer found:</p>\n<pre tabindex=\"0\"><code>kubectl get pv pvc-6791fdd4-5fad-438e-a7fb-16410363e3da\n</code></pre><pre tabindex=\"0\"><code>Error from server (NotFound): persistentvolumes &#34;pvc-6791fdd4-5fad-438e-a7fb-16410363e3da&#34; not found\n</code></pre><p>Although the PV is deleted the underlying storage resource is not deleted, and\nneeds to be removed manually.</p>\n<p>To sum it up, the reclaim policy associated with the Persistent Volume is currently\nignored under certain circumstance. For a <code>Bound</code> PV-PVC pair the ordering of PV-PVC\ndeletion determines whether the PV reclaim policy is honored. The reclaim policy\nis honored if the PVC is deleted first, however, if the PV is deleted prior to\ndeleting the PVC then the reclaim policy is not exercised. As a result of this behavior,\nthe associated storage asset in the external infrastructure is not removed.</p>\n<h2 id=\"pv-reclaim-policy-with-kubernetes-v1-23\">PV reclaim policy with Kubernetes v1.23</h2>\n<p>The new behavior ensures that the underlying storage object is deleted from the backend when users attempt to delete a PV manually.</p>\n<h4 id=\"how-to-enable-new-behavior\">How to enable new behavior?</h4>\n<p>To make use of the new behavior, you must have upgraded your cluster to the v1.23 release of Kubernetes.\nYou need to make sure that you are running the CSI <a href=\"https://github.com/kubernetes-csi/external-provisioner\"><code>external-provisioner</code></a> version <code>4.0.0</code>, or later.\nYou must also enable the <code>HonorPVReclaimPolicy</code> <a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/\">feature gate</a> for the\n<code>external-provisioner</code> and for the <code>kube-controller-manager</code>.</p>\n<p>If you're not using a CSI driver to integrate with your storage backend, the fix isn't\navailable. The Kubernetes project doesn't have a current plan to fix the bug for in-tree\nstorage drivers: the future of those in-tree drivers is deprecation and migration to CSI.</p>\n<h4 id=\"how-does-it-work\">How does it work?</h4>\n<p>The new behavior is achieved by adding a finalizer <code>external-provisioner.volume.kubernetes.io/finalizer</code> on new and existing PVs, the finalizer is only removed after the storage from backend is deleted.</p>\n<p>An example of a PV with the finalizer, notice the new finalizer in the finalizers list</p>\n<pre tabindex=\"0\"><code>kubectl get pv pvc-a7b7e3ba-f837-45ba-b243-dec7d8aaed53 -o yaml\n</code></pre><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>PersistentVolume<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">annotations</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">pv.kubernetes.io/provisioned-by</span>:<span style=\"color:#bbb\"> </span>csi.vsphere.vmware.com<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">creationTimestamp</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;2021-11-17T19:28:56Z&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">finalizers</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- kubernetes.io/pv-protection<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- external-provisioner.volume.kubernetes.io/finalizer<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>pvc-a7b7e3ba-f837-45ba-b243-dec7d8aaed53<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resourceVersion</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;194711&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">uid</span>:<span style=\"color:#bbb\"> </span>087f14f2-4157-4e95-8a70-8294b039d30e<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">accessModes</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- ReadWriteOnce<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">capacity</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">storage</span>:<span style=\"color:#bbb\"> </span>1Gi<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">claimRef</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>PersistentVolumeClaim<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>example-vanilla-block-pvc<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespace</span>:<span style=\"color:#bbb\"> </span>default<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resourceVersion</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;194677&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">uid</span>:<span style=\"color:#bbb\"> </span>a7b7e3ba-f837-45ba-b243-dec7d8aaed53<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">csi</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">driver</span>:<span style=\"color:#bbb\"> </span>csi.vsphere.vmware.com<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">fsType</span>:<span style=\"color:#bbb\"> </span>ext4<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">volumeAttributes</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">storage.kubernetes.io/csiProvisionerIdentity</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">1637110610497-8081</span>-csi.vsphere.vmware.com<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">type</span>:<span style=\"color:#bbb\"> </span>vSphere CNS Block Volume<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">volumeHandle</span>:<span style=\"color:#bbb\"> </span>2dacf297-803f-4ccc-afc7-3d3c3f02051e<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">persistentVolumeReclaimPolicy</span>:<span style=\"color:#bbb\"> </span>Delete<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">storageClassName</span>:<span style=\"color:#bbb\"> </span>example-vanilla-block-sc<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">volumeMode</span>:<span style=\"color:#bbb\"> </span>Filesystem<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">status</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">phase</span>:<span style=\"color:#bbb\"> </span>Bound<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>The presence of the finalizer prevents the PV object from being removed from the\ncluster. As stated previously, the finalizer is only removed from the PV object\nafter it is successfully deleted from the storage backend. To learn more about\nfinalizers, please refer to <a href=\"https://kubernetes.io/blog/2021/05/14/using-finalizers-to-control-deletion/\">Using Finalizers to Control Deletion</a>.</p>\n<h4 id=\"what-about-csi-migrated-volumes\">What about CSI migrated volumes?</h4>\n<p>The fix is applicable to CSI migrated volumes as well. However, when the feature\n<code>HonorPVReclaimPolicy</code> is enabled on 1.23, and CSI Migration is disabled, the finalizer\nis removed from the PV object if it exists.</p>\n<h3 id=\"some-caveats\">Some caveats</h3>\n<ol>\n<li>The fix is applicable only to CSI volumes and migrated volumes. In-tree volumes will exhibit older behavior.</li>\n<li>The fix is introduced as an alpha feature in the <a href=\"https://github.com/kubernetes-csi/external-provisioner\">external-provisioner</a> under the feature gate <code>HonorPVReclaimPolicy</code>. The feature is disabled by default, and needs to be enabled explicitly.</li>\n</ol>\n<h3 id=\"references\">References</h3>\n<ul>\n<li><a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/2644-honor-pv-reclaim-policy\">KEP-2644</a></li>\n<li><a href=\"https://github.com/kubernetes-csi/external-provisioner/issues/546\">Volume leak issue</a></li>\n</ul>\n<h3 id=\"how-do-i-get-involved\">How do I get involved?</h3>\n<p>The Kubernetes Slack channel <a href=\"https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact\">SIG Storage communication channels</a> are great mediums to reach out to the SIG Storage and migration working group teams.</p>\n<p>Special thanks to the following people for the insightful reviews, thorough consideration and valuable contribution:</p>\n<ul>\n<li>Jan Šafránek (jsafrane)</li>\n<li>Xing Yang (xing-yang)</li>\n<li>Matthew Wong (wongma7)</li>\n</ul>\n<p>Those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the <a href=\"https://github.com/kubernetes/community/tree/master/sig-storage\">Kubernetes Storage Special Interest Group (SIG)</a>. We’re rapidly growing and always welcome new contributors.</p>","PublishedAt":"2021-12-15 18:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/12/15/kubernetes-1-23-prevent-persistentvolume-leaks-when-deleting-out-of-order/","SourceName":"Kubernetes"}},{"node":{"ID":243,"Title":"Why Company Data Strategies Are Indelibly Linked with DEI","Description":"<p>Data is the missing puzzle piece for organizations to couple DEI initiatives with actionable insights, identify areas where impact will be highest, and build metrics for greater accountability.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://blog.cloudera.com/why-company-data-strategies-are-indelibly-linked-with-dei/\">Why Company Data Strategies Are Indelibly Linked with DEI</a> appeared first on <a rel=\"nofollow\" href=\"https://blog.cloudera.com\">Cloudera Blog</a>.</p>\n","PublishedAt":"2021-12-15 14:00:20+00:00","OriginURL":"https://blog.cloudera.com/why-company-data-strategies-are-indelibly-linked-with-dei/","SourceName":"Cloudera"}},{"node":{"ID":1111,"Title":"Accessibility Testing 101","Description":"<p>This post is for Day 15 of Mercari Advent Calendar 2021, brought to you by Sahil Khokhar from the Mercari JP Web team. I would like to take this opportunity to continue my efforts towards helping everyone understand Web Accessibility. After explaining The Importance of Web Accessibility and Web Accessibility through Internationalization and Localization in [&hellip;]</p>\n","PublishedAt":"2021-12-15 09:28:25+00:00","OriginURL":"https://engineering.mercari.com/en/blog/entry/20211215-accessibility-testing-101/","SourceName":"Mercari"}},{"node":{"ID":244,"Title":"It’s Time to Listen More to Your Employees!","Description":"<p>Some of the most successful people I know listen more than they talk. We need leaders who focus on building the organizations they run, not their own egos.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://blog.cloudera.com/its-time-to-listen-more-to-your-employees/\">It&#8217;s Time to Listen More to Your Employees!</a> appeared first on <a rel=\"nofollow\" href=\"https://blog.cloudera.com\">Cloudera Blog</a>.</p>\n","PublishedAt":"2021-12-14 18:27:59+00:00","OriginURL":"https://blog.cloudera.com/its-time-to-listen-more-to-your-employees/","SourceName":"Cloudera"}},{"node":{"ID":245,"Title":"AI and ML: No Longer the Stuff of Science Fiction","Description":"<p>Global use cases for enterprise-scale machine learning and industrialized AI</p>\n<p>The post <a rel=\"nofollow\" href=\"https://blog.cloudera.com/ai-and-ml-no-longer-the-stuff-of-science-fiction/\">AI and ML: No Longer the Stuff of Science Fiction</a> appeared first on <a rel=\"nofollow\" href=\"https://blog.cloudera.com\">Cloudera Blog</a>.</p>\n","PublishedAt":"2021-12-14 16:58:15+00:00","OriginURL":"https://blog.cloudera.com/ai-and-ml-no-longer-the-stuff-of-science-fiction/","SourceName":"Cloudera"}},{"node":{"ID":1211,"Title":"Go 1.18 Beta 1 is available, with generics","Description":"Go 1.18 Beta 1 is our first preview of Go 1.18. Please try it and let us know if you find problems.","PublishedAt":"2021-12-14 00:00:00+00:00","OriginURL":"https://go.dev/blog/go1.18beta1","SourceName":"The Go Blog"}},{"node":{"ID":246,"Title":"Cloudera Response to CVE-2021-44228","Description":"<p>On December 10th 2021, the Apache Software Foundation released a security advisory for Apache Log4j 2.0-2.14. This vulnerability is critical and is rated 10 out of 10 on the CVSS 3.1 scoring scale.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://blog.cloudera.com/cloudera-response-to-cve-2021-44228/\">Cloudera Response to CVE-2021-44228</a> appeared first on <a rel=\"nofollow\" href=\"https://blog.cloudera.com\">Cloudera Blog</a>.</p>\n","PublishedAt":"2021-12-13 19:39:17+00:00","OriginURL":"https://blog.cloudera.com/cloudera-response-to-cve-2021-44228/","SourceName":"Cloudera"}}]}},"pageContext":{"limit":30,"skip":5070,"numPages":193,"currentPage":170}},"staticQueryHashes":["3649515864"]}