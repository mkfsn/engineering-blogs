{"componentChunkName":"component---src-templates-blog-list-tsx","path":"/page/57","result":{"data":{"allPost":{"edges":[{"node":{"ID":4084,"Title":"Developers use AI tools, they just don’t trust them (Ep. 586)","Description":"<p>The home team shares what our Developer Survey respondents said about AI, spicy opinions about recent Apple unveilings, and an update on crypto regulation.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://stackoverflow.blog/2023/07/04/developers-use-ai-tools-they-just-dont-trust-them-ep-586/\">Developers use AI tools, they just don’t trust them (Ep. 586)</a> appeared first on <a rel=\"nofollow\" href=\"https://stackoverflow.blog\">Stack Overflow Blog</a>.</p>\n","PublishedAt":"2023-07-04 04:40:00+00:00","OriginURL":"https://stackoverflow.blog/2023/07/04/developers-use-ai-tools-they-just-dont-trust-them-ep-586/","SourceName":"Stack Overflow"}},{"node":{"ID":4083,"Title":"HashiCorp State of Cloud Strategy Survey 2023: Inside the maturity model","Description":"HashiCorp’s 2023 State of Cloud Strategy Survey debuts a new cloud-maturity model, which reveals how more-mature organizations enjoy better business outcomes.","PublishedAt":"2023-07-03 18:30:00+00:00","OriginURL":"https://www.hashicorp.com/blog/hashicorp-state-of-cloud-strategy-survey-2023-inside-the-maturity-model","SourceName":"HashiCorp"}},{"node":{"ID":4080,"Title":"AWS Week in Review – Generative AI with LLM Hands-on Course, Amazon SageMaker Data Wrangler Updates, and More – July 3, 2023","Description":"In last week’s AWS Week in Review post, Danilo mentioned that it’s summer in London. Well, I’m based in Singapore, and it’s mostly summer here. But, June is a special month here as it marks the start of durian season. Starting next week, I’ll be travelling to Thailand, Malaysia, and the Philippines. But before I […]","PublishedAt":"2023-07-03 18:20:12+00:00","OriginURL":"https://aws.amazon.com/blogs/aws/aws-week-in-review-generative-ai-with-llm-hands-on-course-amazon-sagemaker-data-wrangler-updates-and-more-july-3-2023/","SourceName":"AWS"}},{"node":{"ID":4081,"Title":"Do large language models know what they are talking about?","Description":"<p>Large language models seem to possess the ability to reason intelligently, but does that mean they actually know things?</p>\n<p>The post <a rel=\"nofollow\" href=\"https://stackoverflow.blog/2023/07/03/do-large-language-models-know-what-they-are-talking-about/\">Do large language models know what they are talking about?</a> appeared first on <a rel=\"nofollow\" href=\"https://stackoverflow.blog\">Stack Overflow Blog</a>.</p>\n","PublishedAt":"2023-07-03 16:01:22+00:00","OriginURL":"https://stackoverflow.blog/2023/07/03/do-large-language-models-know-what-they-are-talking-about/","SourceName":"Stack Overflow"}},{"node":{"ID":4079,"Title":"Pruning incoming log volumes with Elastic","Description":"<p><em>To log or not to log?</em> has always been a difficult question that software engineers still struggle with, to the detriment of site reliability engineering, or SRE, colleagues. Developers don't always get the level or context of the warnings and errors they capture in applications right and often log messages that may not always be helpful for SREs. I can admit to being one of those developers! This confusion often leads to a flood of events being ingested into logging platforms, making application monitoring and issue investigation for SREs feel a bit like this: </p>\n<p><img src=\"https://static-www.elastic.co/v3/assets/bltefdd0b53724fa2ce/blt79f8c650151adcf0/649c630d63cca619c8d295bd/i_love_lucy.gif\" alt=\"![I Love Lucy Conveyor Belt Gif](./images/1.gif)\" /></p>\n<p>Source: <a href=\"https://giphy.com/explore/i-love-lucy-chocolate\">GIPHY</a></p>\n<p>When looking to reduce your log volume, it is possible to drop information on two dimensions: fields within an event, or the entire event itself. Removing dimensions of interest ensures we can focus on known events of interest and unknown events that may be of interest. </p>\n<p><img src=\"https://static-www.elastic.co/v3/assets/bltefdd0b53724fa2ce/blteba8436b74285018/649c69b49c69d8ce93c1cf33/event_vs_field.jpeg\" alt=\"![Log event versus field](./images/event-vs-field.jpg)\" />\nIn this blog, we will discuss various approaches for dropping known irrelevant events and fields from logs via various collectors. Specifically we will focus on <a href=\"https://www.elastic.co/beats/\">Beats</a>, <a href=\"https://www.elastic.co/logstash/\">Logstash</a>, <a href=\"https://www.elastic.co/guide/en/fleet/current/fleet-overview.html\">Elastic Agent</a>, <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest.html\">Ingest Pipelines</a>, and filtering with <a href=\"https://opentelemetry.io/docs/collector/\">OpenTelemetry Collectors</a>.</p>\n<h2 id=\"beats\">Beats</h2>\n<p><a href=\"https://www.elastic.co/beats/\">Beats</a> are a family of lightweight shippers that allows for the forwarding of events from a particular source. They are commonly used to ingest the events from a source into not just Elasticsearch, but also <a href=\"https://www.elastic.co/guide/en/beats/filebeat/current/configuring-output.html\">other outputs such as Logstash, Kafka, or Redis as shown in the Filebeat documentation</a>. There are six types of Beat available, which are summarized <a href=\"https://www.elastic.co/beats/\">here</a>.</p>\n<p>Our example will focus on Filebeat specifically, but both drop processors discussed here apply to all Beats. After following the <a href=\"https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-installation-configuration.html\">quick start guide within the Filebeat documentation</a>, you will have a running process using configuration file <code>filebeat.yml</code> dictating which log files you are monitoring with Filebeat from <a href=\"https://www.elastic.co/guide/en/beats/filebeat/current/configuration-filebeat-options.html#filebeat-input-types\">any of the supported input types</a>. Hopefully your configuration specifies a series of inputs in a format similar to the below:</p>\n<pre><code class=\"yml language-yml\">filebeat.inputs:\n- type: filestream\n  id: my-logging-app\n  paths:\n    - /var/log/*.log\n</code></pre>\n<p>Filebeat has many options available to configure, of which a full listing is given in the <a href=\"https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-reference-yml.html\"><code>filebeat.reference.yml</code></a> in the documentation. However, it is the <code>drop_event</code> and <code>drop_fields</code> processors in particular that can help us exclude unhelpful messages and isolate only the relevant fields in a given event respectively.\n&nbsp;\nWhen using the <code>drop_event</code> processor, you need to make sure at least one <a href=\"(https://www.elastic.co/guide/en/beats/filebeat/current/defining-processors.html#conditions\">condition</a>) is present to receive the messages you want; otherwise, if no condition is specified, the processor will drop all messages. If no condition is specified in the <code>drop_event</code> processor, all events will be dropped. Please ensure at least one condition is present to ensure you receive the messages you want. \nFor example, if we are not interested in HTTP requests against the <code>/profile</code> endpoint, we can amend the configuration to use the following condition:</p>\n<pre><code class=\"yml language-yml\">filebeat.inputs:\n- type: filestream\n  id: my-logging-app\n  paths:\n    - /var/tmp/other.log\n    - /var/log/*.log\nprocessors:\n  - drop_event:\n      when:\n          and:\n            - equals:\n              url.scheme: http\n            - equals:\n              url.path: /profile\n</code></pre>\n<p>Meanwhile, the <code>drop_fields</code> processor will drop the specified fields, except for the <code>@timestamp</code> and <code>type</code> fields, if the specified condition is fulfilled. Similar to the <code>drop_event</code> processor, if the condition is missing then the fields will always be dropped. If we wanted to exclude the error message field for successful HTTP requests, we could configure a processor similar to the below:</p>\n<pre><code class=\"yml language-yml\">filebeat.inputs:\n- type: filestream\n  id: my-logging-app\n  paths:\n    - /var/tmp/other.log\n    - /var/log/*.log\nprocessors:\n  - drop_fields:\n      when:\n          and:\n            - equals:\n              url.scheme: http\n            - equals:\n              http.response.status_code: 200\n          fields: [\"event.message\"]\n          ignore_missing: false\n</code></pre>\n<p>When dropping fields, there is always a possibility that the field might not exist on a given log message. If the field does not exist in all events being processed in Filebeat, an error will be raised if <code>ignore_missing</code> is specified as <code>true</code> rather than the default value of <code>false</code>.</p>\n<h2 id=\"logstashfiltering\">Logstash filtering</h2>\n<p><a href=\"https://www.elastic.co/logstash/\">Logstash</a> is a free and open data processing pipeline tool that allows you to ingest, transform, and output data between a myriad of sources. It sits within the Extract, Transform, and Load (or ETL) domain. With the prior discussion of Beats, it's important to note that usage of Logstash over Beats would be recommended if you want to centralize the transformation logic. Meanwhile, Beats or Elastic Agent allow dropping events early, which can reduce network traffic requirements early. \nLogstash provides a variety of transformation plugins out of the box that can be used to format and transform events from any source that Logstash is connected to. A typical pipeline within the Logstash configuration file <code>logstash.yml</code> contains three main sections: </p>\n<ol>\n<li><code>input</code> denotes the source of data to the pipeline.</li>\n<li><code>filter</code> contains the relevant data transformation logic.</li>\n<li>The target for the transformed data is configured in the <code>output</code> attribute.\nTo prevent events from making it to the output, the <a href=\"https://www.elastic.co/guide/en/logstash/current/plugins-filters-drop.html\">drop filter plugin will drop any events that meet the stated condition</a>. A typical example of reading from an input file, dropping INFO level events, and outputting to Elasticsearch is as follows:</li>\n</ol>\n<pre><code class=\"yml language-yml\">input {\n  file {\n    id =&gt; \"my-logging-app\"\n    path =&gt; [ \"/var/tmp/other.log\", \"/var/log/*.log\" ]\n  }\n}\nfilter {\n  if [url.scheme] == \"http\" &amp;&amp; [url.path] == \"/profile\" {\n    drop {\n      percentage =&gt; 80\n    }\n  }\n}\noutput {\n  elasticsearch {\n        hosts =&gt; \"https://my-elasticsearch:9200\"\n        data_stream =&gt; \"true\"\n    }\n}\n</code></pre>\n<p>One of the lesser-known options of this filter is the ability to configure a drop rate using the <a href=\"https://www.elastic.co/guide/en/logstash/current/plugins-filters-drop.html#plugins-filters-drop-percentage\"><code>percentage</code></a> option. One of the scary things about filtering out log events is the fear that you will inadvertently drop unknown but relevant entries that could be useful in an outage situation. Or, there is the possibility that your software sends a large volume of messages that could flood your instance, take up vital hot storage, and increase your costs. The percentage attribute covers this case by allowing a subset of the events to be ingested into Elasticsearch, which can address these challenges. In our example above, we ingest 20% of messages matching the criteria to Elasticsearch.\nSimilar to the <code>drop_fields</code> processor found in Beats, Logstash has a <code>remove_field</code> filter for removing individual fields. Although these can be used within many Logstash plugins, they are commonly used within the <code>mutate</code> plugin to transform events, similar to the below:</p>\n<pre><code class=\"yml language-yml\"># Input configuration omitted\nfilter {\n  if [url.scheme] == \"http\" &amp;&amp; [http.response.status_code] == 200 {\n    drop {\n      percentage =&gt; 80\n    }\n    mutate {\n      remove_field: [ \"event.message\" ]\n    }\n  }\n}\n# Output configuration omitted\n</code></pre>\n<p>Just like our Beats example, this will remove <code>event.message</code> from the events that are retained from the drop filter.</p>\n<h2 id=\"agent\">Agent</h2>\n<p>Elastic Agent is a single agent for logs, metrics, and security data that can execute on your host and send events from multiple services and infrastructure to Elasticsearch.\nSimilar to Beats, you can use the <a href=\"https://www.elastic.co/guide/en/fleet/current/drop_event-processor.html\"><code>drop_event</code></a> and <a href=\"https://www.elastic.co/guide/en/fleet/current/drop_fields-processor.html\"><code>drop_fields</code></a> processors in any integrations that support processors. For standalone installations, you should specify the processors within your <code>elastic-agent.yml</code> config. When using Fleet, the processing transforms are normally specified when configuring the integration under the <em>Advanced options</em> pop-out section, as shown below:</p>\n<p><img src=\"https://static-www.elastic.co/v3/assets/bltefdd0b53724fa2ce/bltaf0b98692dc7f215/649c6d8befa20db5d04bd8ba/elastic-agent-kafka-processor.png\" alt=\"![Elastic Agent Kafka Integration Sample Processor](./images/elastic-agent-kafka-processor.png)\" /></p>\n<p>Comparing the above example with our Beats example, you'll notice they are using the same YAML-based format for both processors. There are some limitations to be aware of when using Elastic Agent processors, which are covered in the <a href=\"https://www.elastic.co/guide/en/fleet/current/elastic-agent-processor-configuration.html#limitations\">Fleet documentation</a>. If you are unsure if processing data via Elastic Agent processors is the right thing for your use case, check out this <a href=\"https://www.elastic.co/guide/en/fleet/current/elastic-agent-processor-configuration.html#processing-options\">handy matrix</a>.  </p>\n<h2 id=\"ingestpipelines\">Ingest pipelines</h2>\n<p>The Elastic Agent processors discussed in the previous section will process raw event data, meaning they execute before ingest pipelines. As a result, when using both approaches, proceed with caution as removing or altering fields expected by an ingest pipeline can cause the pipeline to break.\nAs covered in the <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest.html#create-manage-ingest-pipelines\">Create and manage pipeline documentation</a>, new pipelines can be created either within the <strong>Stack Management > Ingest Pipelines</strong> screen or via the <code>_ingest</code> API, which we will use.\nJust like the other tools covered in this piece, the <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/drop-processor.html\"><code>drop</code> processor</a> will allow for any event that meets the required condition to be dropped. It's also the case that if no condition is specified, all events coming through will be dropped. What is different is that the conditional logic is written using <a href=\"https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-guide.html\">Painless</a>, a Java-like scripting language, rather than the YAML syntax we have used previously:</p>\n<pre><code class=\"yml language-yml\">PUT _ingest/pipeline/my-logging-app-pipeline\n{\n  \"description\": \"Event and field dropping for my-logging-app\",\n  \"processors\": [\n    {\n      \"drop\": {\n        \"description\" : \"Drop event\",\n        \"if\": \"ctx?.url?.scheme == 'http' &amp;&amp; ctx?.url?.path == '/profile'\",\n        \"ignore_failure\": true\n      }\n    },\n    {\n      \"remove\": {\n        \"description\" : \"Drop field\",\n        \"field\" : \"event.message\",\n        \"if\": \"ctx?.url?.scheme == 'http' &amp;&amp; ctx?.http?.response?.status_code == 200\",\n        \"ignore_failure\": false\n      }\n    }\n  ]\n}\n</code></pre>\n<p>The <a href=\"https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-ingest-processor-context.html\"><code>ctx</code> variable</a> is a map representation of the fields within the document coming through the pipeline, meaning our example will compare the values of the <code>url.scheme</code> and <code>http.response.status_code</code> fields. JavaScript developers will recognize the <a href=\"https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-operators-reference.html#null-safe-operator\"><code>?</code> denoted null safe operator</a>, which performs not null checks against the field access.\nAs visible in the second processor in the above example, the use of Painless conditional logic is also relevant to the <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/remove-processor.html\"><code>remove</code> processor</a>. This processor will drop the specified fields from the event when they match the specified condition.\nOne of the capabilities that give ingest processors an edge over the other approaches is the ability to specify failure processors, either on the pipeline or a specified processor. Although Beat's does have an <code>ignore_missing</code> option as discussed previously, Ingest Processors allow us to add exception handling such as adding error messages to give details of the processor exception:</p>\n<pre><code class=\"yml language-yml\">PUT _ingest/pipeline/my-logging-app-pipeline\n{\n  \"description\": \"Event and field dropping for my-logging-app with failures\",\n  \"processors\": [\n    {\n      \"drop\": {\n        \"description\" : \"Drop event\",\n        \"if\": \"ctx?.url?.scheme == 'http' &amp;&amp; ctx?.url?.path == '/profile'\",\n        \"ignore_failure\": true\n      }\n    },\n    {\n      \"remove\": {\n        \"description\" : \"Drop field\",\n        \"field\" : \"event.message\",\n        \"if\": \"ctx?.url?.scheme == 'http' &amp;&amp; ctx?.http?.response?.status_code == 200\",\n        \"ignore_failure\": false\n      }\n    }\n  ],\n  \"on_failure\": [\n    {\n      \"set\": {\n        \"description\": \"Set 'ingest.failure.message'\",\n        \"field\": \"ingest.failure.message\",\n        \"value\": \"Ingestion issue\"\n        }\n      }\n  ]\n}\n</code></pre>\n<p>The pipeline can then be used on a <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/master/ingest.html#add-pipeline-to-indexing-request\">single indexing request</a>, <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/master/ingest.html#set-default-pipeline\">set as the default pipeline</a> for an index, or even used alongside <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/master/ingest.html#pipelines-for-beats\">Beats</a> and <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/master/ingest.html#pipelines-for-fleet-elastic-agent\">Elastic Agent</a>.</p>\n<h2 id=\"opentelemetrycollectors\">OpenTelemetry collectors</h2>\n<p><a href=\"https://opentelemetry.io/docs/collector/transforming-telemetry/\">OpenTelemetry</a>, or OTel, is an open standard that provides APIs, tooling, and integrations to enable the capture of telemetry data such as logs, metrics, and traces from applications. Application developers commonly use the OpenTelemetry agent for their programming language of choice to send trace data and metrics directly to the Elastic Stack, as Elastic supports the OpenTelemetry protocol (OTLP).\nIn some cases, having every application send behavioral information directly to the observability platform may be unwise. Large enterprise ecosystems may have centralized observability capabilities or may run large microservice ecosystems where adopting a standard tracing practice can be difficult. However, the sanitization of events and traces is also challenging as the number of applications and services grows. These are the situations where using one or more collectors as a router of data to the Elastic Stack makes sense.\nAs demonstrated in the <a href=\"https://opentelemetry.io/docs/collector/configuration/\">OpenTelemetry documentation</a> and the <a href=\"https://www.elastic.co/guide/en/apm/guide/current/open-telemetry-direct.html#connect-open-telemetry-collector\">example collector in the Elastic APM documentation</a>, the basic configuration for an OTel collector has four main sections:</p>\n<ol>\n<li><code>receivers</code> that define the sources of data, which can be push or pull-based.</li>\n<li><code>processors</code> that can filter or transform the received data before export, which is what we are interested in doing.</li>\n<li><code>exporters</code> which define how the data is sent to the final destination, in this case Elastic!</li>\n<li>A <code>service</code> section to define the components enabled in the collector that is needed for the other elements.\nDropping events and fields can be achieved using the <a href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/filterprocessor\"><code>filter</code></a> and <a href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/attributesprocessor\"><code>attributes</code></a> processors, respectively, in the collector config. A selection of examples of both filters is shown below:</li>\n</ol>\n<pre><code class=\"yml language-yml\">receivers: \n  filelog:\n    include: [ /var/tmp/other.log, /var/log/*.log ]\nprocessors: \n  filter/denylist:\n    error_mode: ignore\n    logs:\n      log_record:\n        - 'url.scheme == \"info\"'\n        - 'url.path == \"/profile\"'\n        - 'http.response.status_code == 200'\n  attributes/errors:\n    actions:\n      - key: error.message\n        action: delete\n  memory_limiter:\n    check_interval: 1s\n    limit_mib: 2000\n  batch:\nexporters:\n  # Exporters configuration omitted \nservice:\n  pipelines:\n    # Pipelines configuration omitted\n</code></pre>\n<p>The <code>filter</code> processor applied to the telemetry type (logs, metrics, or traces) will drop the event if it matches any of the specified conditions. Meanwhile, the <code>attributes</code> processor applied to our error fields will delete the <code>error.message</code> attribute from all events. The <code>pattern</code> attribute can also be used in place of the <code>key</code> option to remove fields matching a specified regular expression. Field deletion based on conditions as we have done in our Beats, Logstash, and ingest pipeline examples is not part of the specification. However, an alternative would be to use the <a href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/transformprocessor\"><code>transform</code> processor</a> to specify a complex transformation to set the value of a field and then delete. </p>\n<h2 id=\"conclusions\">Conclusions</h2>\n<p>The aim of the DevOps movement is to align the processes and practices of software engineering and SRE. That includes working together to ensure that relevant logs, metrics, and traces are sent from applications to our Observability platform.\nAs we have seen first-hand with <a href=\"https://www.elastic.co/beats/\">Beats</a>, <a href=\"https://www.elastic.co/logstash/\">Logstash</a>, <a href=\"https://www.elastic.co/guide/en/fleet/current/fleet-overview.html\">Elastic Agent</a>, <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest.html\">Ingest Pipelines</a>, and <a href=\"https://opentelemetry.io/docs/collector/\">OTel collectors</a>, the approaches for dropping events and individual fields vary according to the tool used.\nYou may be wondering, which option is right for you? </p>\n<ol>\n<li>If the overhead of sending large messages over the network is a concern, transforming closer to the source using Beats or Logstash is the better option. If you're looking to minimize the system resources used in your collection and transformation, Beats may be preferred over Logstash as they have a small footprint.</li>\n<li>For centralizing transformation logic to apply to many application logs, using processors in an OTel collector may be the right approach.</li>\n<li>If you want to make use of centrally managed ingestion and transformation policies with popular services and systems such as Kafka, Nginx, or AWS, using Elastic Agent with Fleet is recommended.</li>\n<li>Ingest pipelines are great for transforming events at ingestion if you are less concerned about network overhead and would like your logic centralized within Elasticsearch. \nAlthough not covered here, other techniques such as runtime fields, index level compression, and <code>_synthetic_source</code> usage can also be used to reduce disk storage requirements and CPU overhead. If your favorite way to drop events or fields is not listed here, do let us know!</li>\n</ol>\n<h2 id=\"resources\">Resources</h2>\n<ol>\n<li><a href=\"https://www.elastic.co/beats/\">Elastic Beats</a></li>\n<li><a href=\"https://www.elastic.co/guide/en/beats/filebeat/current/filtering-and-enhancing-data.html\">Filebeat | Filter and enhance data with processors</a></li>\n<li><a href=\"https://www.elastic.co/logstash/\">Logstash</a></li>\n<li><a href=\"https://www.elastic.co/guide/en/logstash/current/filter-plugins.html\">Logstash | Filter plugins</a></li>\n<li><a href=\"https://www.elastic.co/guide/en/fleet/current/fleet-overview.html\">Elastic Agent</a></li>\n<li><a href=\"https://www.elastic.co/guide/en/fleet/current/elastic-agent-processor-configuration.html\">Elastic Agent | Processors</a></li>\n<li><a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest.html\">Ingest Pipelines</a> </li>\n<li><a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/processors.html\">Elaticsearch | Ingest processor reference</a></li>\n<li><a href=\"https://opentelemetry.io/docs/collector/\">OTel Collectors</a> </li>\n<li><a href=\"https://opentelemetry.io/docs/collector/transforming-telemetry/#advanced-transformations\">OTel Transforming telemetry</a></li>\n</ol>","PublishedAt":"2023-07-03 13:00:00+00:00","OriginURL":"https://www.elastic.co/blog/pruning-incoming-log-volumes-with-elastic","SourceName":"Elastic"}},{"node":{"ID":4082,"Title":"How to get the best of lexical and AI-powered search with Elastic’s vector database","Description":"","PublishedAt":"2023-07-03 00:00:00+00:00","OriginURL":"https://www.elastic.co/blog/lexical-ai-powered-search-elastic-vector-database","SourceName":"Elastic"}},{"node":{"ID":4073,"Title":"How Instacart Measures the True Value of Advertising: The Methodology of Ad Incrementality","Description":"","PublishedAt":"2023-06-30 17:47:22+00:00","OriginURL":"https://tech.instacart.com/how-instacart-measures-the-true-value-of-advertising-the-methodology-of-ad-incrementality-aac6c58e627c?source=rss----587883b5d2ee---4","SourceName":"Instacart"}},{"node":{"ID":4070,"Title":"The Overflow #184: Stress test your code","Description":"<p>Vertical farming, pure math for dummies, and git for solo devs.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://stackoverflow.blog/2023/06/30/the-overflow-184-stress-test-your-code/\">The Overflow #184: Stress test your code</a> appeared first on <a rel=\"nofollow\" href=\"https://stackoverflow.blog\">Stack Overflow Blog</a>.</p>\n","PublishedAt":"2023-06-30 13:00:00+00:00","OriginURL":"https://stackoverflow.blog/2023/06/30/the-overflow-184-stress-test-your-code/","SourceName":"Stack Overflow"}},{"node":{"ID":4077,"Title":"Mercari Hack Fest #7 : Introducing the Winners!","Description":"<p>Hello, my name is @afroscript from the Engineering Office. Mercari Hack Fest (“Hack Fest”), a technology festival for engineers was held for three days from April 19th-21st. *Related article : Organizing a Successful Internal Hackathon: Mercari Hack Fest Spring 2023 This article explains how the “Showcase Day”, the concluding event of Hack Fest was like, [&hellip;]</p>\n","PublishedAt":"2023-06-30 11:47:48+00:00","OriginURL":"https://engineering.mercari.com/en/blog/entry/20230621-e066032084/","SourceName":"Mercari"}},{"node":{"ID":4068,"Title":"Making computer science more humane at Carnegie Mellon (ep. 585)","Description":"<p>On this episode of the podcast, Ben and Ryan chat with Martial Hebert, dean of the School of Computer Science at Ryan’s alma mater, Carnegie Mellon University.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://stackoverflow.blog/2023/06/30/making-computer-science-more-humane-at-carnegie-mellon-ep-585/\">Making computer science more humane at Carnegie Mellon (ep. 585)</a> appeared first on <a rel=\"nofollow\" href=\"https://stackoverflow.blog\">Stack Overflow Blog</a>.</p>\n","PublishedAt":"2023-06-30 04:40:00+00:00","OriginURL":"https://stackoverflow.blog/2023/06/30/making-computer-science-more-humane-at-carnegie-mellon-ep-585/","SourceName":"Stack Overflow"}},{"node":{"ID":4067,"Title":"How We Migrated Our Acceptance Tests to Use Synthetic Monitoring","Description":"<img class=\"webfeedsFeaturedVisual rss\" src=\"https://imgix.datadoghq.com/img/blog/engineering/migrating-acceptance-tests-to-synthetic-monitoring/migrating-acceptance-tests-to-synthetic-monitoring.png\" width=\"100%\"/>The Frontend Developer Experience team strives to improve the lives of 300 frontend engineers at Datadog. We cover build systems, tests, deployments, code health, internal tools, and more—we’re here to remove any friction and pain points from our engineers’ workflows.One such pain point was difficult-to-maintain acceptance tests. This is the story of how we migrated a codebase from flaky, unmanageable acceptance testing with Puppeteer (Chromium Headless Browser) to more robust and maintainable Synthetic tests.","PublishedAt":"2023-06-30 00:00:00+00:00","OriginURL":"https://www.datadoghq.com/blog/engineering/migrating-acceptance-tests-to-synthetic-monitoring/","SourceName":"Datadog"}},{"node":{"ID":4069,"Title":"The power of generative AI for retail and CPG","Description":"","PublishedAt":"2023-06-30 00:00:00+00:00","OriginURL":"https://www.elastic.co/blog/generative-ai-retail-cpg","SourceName":"Elastic"}},{"node":{"ID":4071,"Title":"Improving the Elastic APM UI performance with continuous rollups and service metrics","Description":"","PublishedAt":"2023-06-30 00:00:00+00:00","OriginURL":"https://www.elastic.co/blog/improving-elastic-apm-ui-performance-continuous-rollups-service-metrics","SourceName":"Elastic"}},{"node":{"ID":4072,"Title":"How we sped up data ingestion in Elasticsearch 8.6, 8.7, and 8.8","Description":"","PublishedAt":"2023-06-30 00:00:00+00:00","OriginURL":"https://www.elastic.co/blog/data-ingestion-elasticsearch","SourceName":"Elastic"}},{"node":{"ID":4064,"Title":"Meta’s Evenstar is transitioning to OCP to accelerate open RAN adoption","Description":"<p>Meta is transferring its IP for Evenstar, a program to accelerate the adoption of open RAN technologies, to the Open Compute Project (OCP). Meta will contribute Evenstar’s radio unit design to OCP, giving the telecom industry its first open, white box radio unit solution. The TIP Open RAN community will leverage the Evenstar radio unit [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2023/06/29/connectivity/evenstar-meta-ocp-open-ran/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2023/06/29/connectivity/evenstar-meta-ocp-open-ran/\">Meta&#8217;s Evenstar is transitioning to OCP to accelerate open RAN adoption</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n","PublishedAt":"2023-06-29 16:00:13+00:00","OriginURL":"https://engineering.fb.com/2023/06/29/connectivity/evenstar-meta-ocp-open-ran/","SourceName":"Facebook"}},{"node":{"ID":4065,"Title":"How Bloomberg’s engineers built a culture of knowledge sharing","Description":"<p>Thousands of the company’s engineers, data scientists, designers, and developers have asked and answered questions about how things work inside their organization. </p>\n<p>The post <a rel=\"nofollow\" href=\"https://stackoverflow.blog/2023/06/29/how-bloombergs-engineers-built-a-culture-of-knowledge-sharing/\">How Bloomberg’s engineers built a culture of knowledge sharing</a> appeared first on <a rel=\"nofollow\" href=\"https://stackoverflow.blog\">Stack Overflow Blog</a>.</p>\n","PublishedAt":"2023-06-29 14:00:22+00:00","OriginURL":"https://stackoverflow.blog/2023/06/29/how-bloombergs-engineers-built-a-culture-of-knowledge-sharing/","SourceName":"Stack Overflow"}},{"node":{"ID":4060,"Title":"How to Manage Risk with Modern Data Architectures","Description":"<p>The recent failures of regional banks in the US, such as Silicon Valley Bank (SVB), Silvergate, Signature, and First Republic, were caused by multiple factors. To ensure the stability of the US financial system, the implementation of advanced liquidity risk models and stress testing using (MI/AI) could potentially serve as a protective measure. Technology alone [&#8230;]</p>\n<p>The post <a rel=\"nofollow\" href=\"https://blog.cloudera.com/how-to-manage-risk-with-modern-data-architectures/\">How to Manage Risk with Modern Data Architectures</a> appeared first on <a rel=\"nofollow\" href=\"https://blog.cloudera.com\">Cloudera Blog</a>.</p>\n","PublishedAt":"2023-06-29 13:35:14+00:00","OriginURL":"https://blog.cloudera.com/how-to-manage-risk-with-modern-data-architectures/","SourceName":"Cloudera"}},{"node":{"ID":4074,"Title":"Building a security-conscious CI/CD pipeline","Description":"In this post, we'll discuss CI/CD pipelines and how they can be configured to integrate security throughout the development process.","PublishedAt":"2023-06-29 05:00:00+00:00","OriginURL":"https://snyk.io/blog/building-security-conscious-ci-cd-pipeline/","SourceName":"Snyk"}},{"node":{"ID":4075,"Title":"The importance of verifying webhook signatures","Description":"Webhooks are a callback integration technique for sending and receiving information, such as event notifications, in close to real-time. In this walkthrough, we’ll implement a GitHub webhook in Node.js that detects when users push code to a repository.","PublishedAt":"2023-06-29 05:00:00+00:00","OriginURL":"https://snyk.io/blog/verifying-webhook-signatures/","SourceName":"Snyk"}},{"node":{"ID":4059,"Title":"Blog: Verifying Container Image Signatures Within CRI Runtimes","Description":"<p><strong>Author</strong>: Sascha Grunert</p>\n<p>The Kubernetes community has been signing their container image-based artifacts\nsince release v1.24. While the graduation of the <a href=\"https://github.com/kubernetes/enhancements/issues/3031\">corresponding enhancement</a>\nfrom <code>alpha</code> to <code>beta</code> in v1.26 introduced signatures for the binary artifacts,\nother projects followed the approach by providing image signatures for their\nreleases, too. This means that they either create the signatures within their\nown CI/CD pipelines, for example by using GitHub actions, or rely on the\nKubernetes <a href=\"https://github.com/kubernetes-sigs/promo-tools/blob/e2b96dd/docs/image-promotion.md\">image promotion</a> process to automatically sign the images by\nproposing pull requests to the <a href=\"https://github.com/kubernetes/k8s.io/tree/4b95cc2/k8s.gcr.io\">k/k8s.io</a> repository. A requirement for\nusing this process is that the project is part of the <code>kubernetes</code> or\n<code>kubernetes-sigs</code> GitHub organization, so that they can utilize the community\ninfrastructure for pushing images into staging buckets.</p>\n<p>Assuming that a project now produces signed container image artifacts, how can\none actually verify the signatures? It is possible to do it manually like\noutlined in the <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/verify-signed-artifacts/#verifying-image-signatures\">official Kubernetes documentation</a>. The problem with this\napproach is that it involves no automation at all and should be only done for\ntesting purposes. In production environments, tools like the <a href=\"https://docs.sigstore.dev/policy-controller/overview\">sigstore\npolicy-controller</a> can help with the automation. These tools\nprovide a higher level API by using <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources\">Custom Resource Definitions (CRD)</a> as\nwell as an integrated <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers\">admission controller and webhook</a> to verify\nthe signatures.</p>\n<p>The general usage flow for an admission controller based verification is:</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2023/06/29/container-image-signature-verification/flow.svg\"\nalt=\"Create an instance of the policy and annotate the namespace to validate the signatures. Then create the pod. The controller evaluates the policy and if it passes, then it does the image pull if necessary. If the policy evaluation fails, then it will not admit the pod.\"/>\n</figure>\n<p>A key benefit of this architecture is simplicity: A single instance within the\ncluster validates the signatures before any image pull can happen in the\ncontainer runtime on the nodes, which gets initiated by the kubelet. This\nbenefit also brings along the issue of separation: The node which should pull\nthe container image is not necessarily the same node that performs the admission. This\nmeans that if the controller is compromised, then a cluster-wide policy\nenforcement can no longer be possible.</p>\n<p>One way to solve this issue is doing the policy evaluation directly within the\n<a href=\"https://kubernetes.io/docs/concepts/architecture/cri\">Container Runtime Interface (CRI)</a> compatible container runtime. The\nruntime is directly connected to the <a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet\">kubelet</a> on a node and does all\nthe tasks like pulling images. <a href=\"https://github.com/cri-o/cri-o\">CRI-O</a> is one of those available runtimes\nand will feature full support for container image signature verification in v1.28.</p>\n<p>How does it work? CRI-O reads a file called <a href=\"https://github.com/containers/image/blob/b3e0ba2/docs/containers-policy.json.5.md#sigstoresigned\"><code>policy.json</code></a>, which\ncontains all the rules defined for container images. For example, you can define a\npolicy which only allows signed images <code>quay.io/crio/signed</code> for any tag or\ndigest like this:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-json\" data-lang=\"json\"><span style=\"display:flex;\"><span>{\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;default&#34;</span>: [{ <span style=\"color:#008000;font-weight:bold\">&#34;type&#34;</span>: <span style=\"color:#b44\">&#34;reject&#34;</span> }],\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;transports&#34;</span>: {\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;docker&#34;</span>: {\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;quay.io/crio/signed&#34;</span>: [\n</span></span><span style=\"display:flex;\"><span> {\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;type&#34;</span>: <span style=\"color:#b44\">&#34;sigstoreSigned&#34;</span>,\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;signedIdentity&#34;</span>: { <span style=\"color:#008000;font-weight:bold\">&#34;type&#34;</span>: <span style=\"color:#b44\">&#34;matchRepository&#34;</span> },\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;fulcio&#34;</span>: {\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;oidcIssuer&#34;</span>: <span style=\"color:#b44\">&#34;https://github.com/login/oauth&#34;</span>,\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;subjectEmail&#34;</span>: <span style=\"color:#b44\">&#34;sgrunert@redhat.com&#34;</span>,\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;caData&#34;</span>: <span style=\"color:#b44\">&#34;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUI5ekNDQVh5Z0F3SUJBZ0lVQUxaTkFQRmR4SFB3amVEbG9Ed3lZQ2hBTy80d0NnWUlLb1pJemowRUF3TXcKS2pFVk1CTUdBMVVFQ2hNTWMybG5jM1J2Y21VdVpHVjJNUkV3RHdZRFZRUURFd2h6YVdkemRHOXlaVEFlRncweQpNVEV3TURjeE16VTJOVGxhRncwek1URXdNRFV4TXpVMk5UaGFNQ294RlRBVEJnTlZCQW9UREhOcFozTjBiM0psCkxtUmxkakVSTUE4R0ExVUVBeE1JYzJsbmMzUnZjbVV3ZGpBUUJnY3Foa2pPUFFJQkJnVXJnUVFBSWdOaUFBVDcKWGVGVDRyYjNQUUd3UzRJYWp0TGszL09sbnBnYW5nYUJjbFlwc1lCcjVpKzR5bkIwN2NlYjNMUDBPSU9aZHhleApYNjljNWlWdXlKUlErSHowNXlpK1VGM3VCV0FsSHBpUzVzaDArSDJHSEU3U1hyazFFQzVtMVRyMTlMOWdnOTJqCll6QmhNQTRHQTFVZER3RUIvd1FFQXdJQkJqQVBCZ05WSFJNQkFmOEVCVEFEQVFIL01CMEdBMVVkRGdRV0JCUlkKd0I1ZmtVV2xacWw2ekpDaGt5TFFLc1hGK2pBZkJnTlZIU01FR0RBV2dCUll3QjVma1VXbFpxbDZ6SkNoa3lMUQpLc1hGK2pBS0JnZ3Foa2pPUFFRREF3TnBBREJtQWpFQWoxbkhlWFpwKzEzTldCTmErRURzRFA4RzFXV2cxdENNCldQL1dIUHFwYVZvMGpoc3dlTkZaZ1NzMGVFN3dZSTRxQWpFQTJXQjlvdDk4c0lrb0YzdlpZZGQzL1Z0V0I1YjkKVE5NZWE3SXgvc3RKNVRmY0xMZUFCTEU0Qk5KT3NRNHZuQkhKCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0=&#34;</span>\n</span></span><span style=\"display:flex;\"><span> },\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;rekorPublicKeyData&#34;</span>: <span style=\"color:#b44\">&#34;LS0tLS1CRUdJTiBQVUJMSUMgS0VZLS0tLS0KTUZrd0V3WUhLb1pJemowQ0FRWUlLb1pJemowREFRY0RRZ0FFMkcyWSsydGFiZFRWNUJjR2lCSXgwYTlmQUZ3cgprQmJtTFNHdGtzNEwzcVg2eVlZMHp1ZkJuaEM4VXIvaXk1NUdoV1AvOUEvYlkyTGhDMzBNOStSWXR3PT0KLS0tLS1FTkQgUFVCTElDIEtFWS0tLS0tCg==&#34;</span>\n</span></span><span style=\"display:flex;\"><span> }\n</span></span><span style=\"display:flex;\"><span> ]\n</span></span><span style=\"display:flex;\"><span> }\n</span></span><span style=\"display:flex;\"><span> }\n</span></span><span style=\"display:flex;\"><span>}\n</span></span></code></pre></div><p>CRI-O has to be started to use that policy as the global source of truth:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> sudo crio --log-level debug --signature-policy ./policy.json\n</span></span></code></pre></div><p>CRI-O is now able to pull the image while verifying its signatures. This can be\ndone by using <a href=\"https://github.com/kubernetes-sigs/cri-tools\"><code>crictl</code> (cri-tools)</a>, for example:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> sudo crictl -D pull quay.io/crio/signed\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">DEBU[…] get image connection\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">DEBU[…] PullImageRequest: &amp;PullImageRequest{Image:&amp;ImageSpec{Image:quay.io/crio/signed,Annotations:map[string]string{},},Auth:nil,SandboxConfig:nil,}\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">DEBU[…] PullImageResponse: &amp;PullImageResponse{ImageRef:quay.io/crio/signed@sha256:18b42e8ea347780f35d979a829affa178593a8e31d90644466396e1187a07f3a,}\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">Image is up to date for quay.io/crio/signed@sha256:18b42e8ea347780f35d979a829affa178593a8e31d90644466396e1187a07f3a\n</span></span></span></code></pre></div><p>The CRI-O debug logs will also indicate that the signature got successfully\nvalidated:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#888\">DEBU[…] IsRunningImageAllowed for image docker:quay.io/crio/signed:latest\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">DEBU[…] Using transport &#34;docker&#34; specific policy section quay.io/crio/signed\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">DEBU[…] Reading /var/lib/containers/sigstore/crio/signed@sha256=18b42e8ea347780f35d979a829affa178593a8e31d90644466396e1187a07f3a/signature-1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">DEBU[…] Looking for sigstore attachments in quay.io/crio/signed:sha256-18b42e8ea347780f35d979a829affa178593a8e31d90644466396e1187a07f3a.sig\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">DEBU[…] GET https://quay.io/v2/crio/signed/manifests/sha256-18b42e8ea347780f35d979a829affa178593a8e31d90644466396e1187a07f3a.sig\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">DEBU[…] Content-Type from manifest GET is &#34;application/vnd.oci.image.manifest.v1+json&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">DEBU[…] Found a sigstore attachment manifest with 1 layers\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">DEBU[…] Fetching sigstore attachment 1/1: sha256:8276724a208087e73ae5d9d6e8f872f67808c08b0acdfdc73019278807197c45\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">DEBU[…] Downloading /v2/crio/signed/blobs/sha256:8276724a208087e73ae5d9d6e8f872f67808c08b0acdfdc73019278807197c45\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">DEBU[…] GET https://quay.io/v2/crio/signed/blobs/sha256:8276724a208087e73ae5d9d6e8f872f67808c08b0acdfdc73019278807197c45\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">DEBU[…] Requirement 0: allowed\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">DEBU[…] Overall: allowed\n</span></span></span></code></pre></div><p>All of the defined fields like <code>oidcIssuer</code> and <code>subjectEmail</code> in the policy\nhave to match, while <code>fulcio.caData</code> and <code>rekorPublicKeyData</code> are the public\nkeys from the upstream <a href=\"https://github.com/sigstore/fulcio\">fulcio (OIDC PKI)</a> and <a href=\"https://github.com/sigstore/rekor\">rekor\n(transparency log)</a> instances.</p>\n<p>This means that if you now invalidate the <code>subjectEmail</code> of the policy, for example to\n<code>wrong@mail.com</code>:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> jq <span style=\"color:#b44\">&#39;.transports.docker.&#34;quay.io/crio/signed&#34;[0].fulcio.subjectEmail = &#34;wrong@mail.com&#34;&#39;</span> policy.json &gt; new-policy.json\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> mv new-policy.json policy.json\n</span></span></code></pre></div><p>Then remove the image, since it already exists locally:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> sudo crictl rmi quay.io/crio/signed\n</span></span></code></pre></div><p>Now when you pull the image, CRI-O complains that the required email is wrong:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> sudo crictl pull quay.io/crio/signed\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">FATA[…] pulling image: rpc error: code = Unknown desc = Source image rejected: Required email wrong@mail.com not found (got []string{&#34;sgrunert@redhat.com&#34;})\n</span></span></span></code></pre></div><p>It is also possible to test an unsigned image against the policy. For that you\nhave to modify the key <code>quay.io/crio/signed</code> to something like\n<code>quay.io/crio/unsigned</code>:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> sed -i <span style=\"color:#b44\">&#39;s;quay.io/crio/signed;quay.io/crio/unsigned;&#39;</span> policy.json\n</span></span></code></pre></div><p>If you now pull the container image, CRI-O will complain that no signature exists\nfor it:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> sudo crictl pull quay.io/crio/unsigned\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">FATA[…] pulling image: rpc error: code = Unknown desc = SignatureValidationFailed: Source image rejected: A signature was required, but no signature exists\n</span></span></span></code></pre></div><p>It is important to mention that CRI-O will match the\n<code>.critical.identity.docker-reference</code> field within the signature to match with\nthe image repository. For example, if you verify the image\n<code>registry.k8s.io/kube-apiserver-amd64:v1.28.0-alpha.3</code>, then the corresponding\n<code>docker-reference</code> should be <code>registry.k8s.io/kube-apiserver-amd64</code>:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> cosign verify registry.k8s.io/kube-apiserver-amd64:v1.28.0-alpha.3 <span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span><span style=\"color:#888\"> --certificate-identity krel-trust@k8s-releng-prod.iam.gserviceaccount.com \\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> --certificate-oidc-issuer https://accounts.google.com \\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> | jq -r &#39;.[0].critical.identity.&#34;docker-reference&#34;&#39;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">…\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"></span><span style=\"\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"\"></span><span style=\"color:#888\">registry.k8s.io/kubernetes/kube-apiserver-amd64\n</span></span></span></code></pre></div><p>The Kubernetes community introduced <code>registry.k8s.io</code> as proxy mirror for\nvarious registries. Before the release of <a href=\"https://github.com/kubernetes-sigs/promo-tools/releases/tag/v4.0.2\">kpromo v4.0.2</a>, images\nhad been signed with the actual mirror rather than <code>registry.k8s.io</code>:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> cosign verify registry.k8s.io/kube-apiserver-amd64:v1.28.0-alpha.2 <span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span><span style=\"color:#888\"> --certificate-identity krel-trust@k8s-releng-prod.iam.gserviceaccount.com \\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> --certificate-oidc-issuer https://accounts.google.com \\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> | jq -r &#39;.[0].critical.identity.&#34;docker-reference&#34;&#39;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">…\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"></span><span style=\"\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"\"></span><span style=\"color:#888\">asia-northeast2-docker.pkg.dev/k8s-artifacts-prod/images/kubernetes/kube-apiserver-amd64\n</span></span></span></code></pre></div><p>The change of the <code>docker-reference</code> to <code>registry.k8s.io</code> makes it easier for\nend users to validate the signatures, because they cannot know anything about the\nunderlying infrastructure being used. The feature to set the identity on image\nsigning has been added to <a href=\"https://github.com/sigstore/cosign/pull/2984\">cosign</a> via the flag <code>sign --sign-container-identity</code> as well and will be part of its upcoming release.</p>\n<p>The Kubernetes image pull error code <code>SignatureValidationFailed</code> got <a href=\"https://github.com/kubernetes/kubernetes/pull/117717\">recently added to\nKubernetes</a> and will be available from v1.28. This error code allows\nend-users to understand image pull failures directly from the kubectl CLI. For\nexample, if you run CRI-O together with Kubernetes using the policy which requires\n<code>quay.io/crio/unsigned</code> to be signed, then a pod definition like this:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>Pod<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>pod<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">containers</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>container<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">image</span>:<span style=\"color:#bbb\"> </span>quay.io/crio/unsigned<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Will cause the <code>SignatureValidationFailed</code> error when applying the pod manifest:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> kubectl apply -f pod.yaml\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">pod/pod created\n</span></span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> kubectl get pods\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">NAME READY STATUS RESTARTS AGE\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">pod 0/1 SignatureValidationFailed 0 4s\n</span></span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> kubectl describe pod pod | tail -n8\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> Type Reason Age From Message\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> ---- ------ ---- ---- -------\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> Normal Scheduled 58s default-scheduler Successfully assigned default/pod to 127.0.0.1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> Normal BackOff 22s (x2 over 55s) kubelet Back-off pulling image &#34;quay.io/crio/unsigned&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> Warning Failed 22s (x2 over 55s) kubelet Error: ImagePullBackOff\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> Normal Pulling 9s (x3 over 58s) kubelet Pulling image &#34;quay.io/crio/unsigned&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> Warning Failed 6s (x3 over 55s) kubelet Failed to pull image &#34;quay.io/crio/unsigned&#34;: SignatureValidationFailed: Source image rejected: A signature was required, but no signature exists\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> Warning Failed 6s (x3 over 55s) kubelet Error: SignatureValidationFailed\n</span></span></span></code></pre></div><p>This overall behavior provides a more Kubernetes native experience and does not\nrely on third party software to be installed in the cluster.</p>\n<p>There are still a few corner cases to consider: For example, what if you want to\nallow policies per namespace in the same way the policy-controller supports it?\nWell, there is an upcoming CRI-O feature in v1.28 for that! CRI-O will support\nthe <code>--signature-policy-dir</code> / <code>signature_policy_dir</code> option, which defines the\nroot path for pod namespace-separated signature policies. This means that CRI-O\nwill lookup that path and assemble a policy like <code>&lt;SIGNATURE_POLICY_DIR&gt;/&lt;NAMESPACE&gt;.json</code>,\nwhich will be used on image pull if existing. If no pod namespace is\nprovided on image pull (<a href=\"https://github.com/kubernetes/cri-api/blob/e5515a5/pkg/apis/runtime/v1/api.proto#L1448\">via the sandbox config</a>), or the\nconcatenated path is non-existent, then CRI-O's global policy will be used as\nfallback.</p>\n<p>Another corner case to consider is critical for the correct signature\nverification within container runtimes: The kubelet only invokes container image\npulls if the image does not already exist on disk. This means that an\nunrestricted policy from Kubernetes namespace A can allow pulling an image,\nwhile namespace B is not able to enforce the policy because it already exits on\nthe node. Finally, CRI-O has to verify the policy not only on image pull, but\nalso on container creation. This fact makes things even a bit more complicated,\nbecause the CRI does not really pass down the user specified image reference on\ncontainer creation, but an already resolved image ID, or digest. A <a href=\"https://github.com/kubernetes/kubernetes/pull/118652\">small\nchange to the CRI</a> can help with that.</p>\n<p>Now that everything happens within the container runtime, someone has to\nmaintain and define the policies to provide a good user experience around that\nfeature. The CRDs of the policy-controller are great, while we could imagine that\na daemon within the cluster can write the policies for CRI-O per namespace. This\nwould make any additional hook obsolete and moves the responsibility of\nverifying the image signature to the actual instance which pulls the image. <a href=\"https://groups.google.com/g/kubernetes-sig-node/c/kgpxqcsJ7Vc/m/7X7t_ElsAgAJ\">I\nevaluated</a> other possible paths toward a better container image\nsignature verification within plain Kubernetes, but I could not find a great fit\nfor a native API. This means that I believe that a CRD is the way to go, but\nusers still need an instance which actually serves it.</p>\n<p>Thank you for reading this blog post! If you're interested in more, providing\nfeedback or asking for help, then feel free to get in touch with me directly via\n<a href=\"https://kubernetes.slack.com/messages/crio\">Slack (#crio)</a> or the <a href=\"https://groups.google.com/forum/#!forum/kubernetes-sig-node\">SIG Node mailing list</a>.</p>","PublishedAt":"2023-06-29 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/06/29/container-image-signature-verification/","SourceName":"Kubernetes"}},{"node":{"ID":4061,"Title":"Elastic Stack container images signed with Sigstore!","Description":"","PublishedAt":"2023-06-29 00:00:00+00:00","OriginURL":"https://www.elastic.co/blog/elastic-stack-container-images-signed-sigstore","SourceName":"Elastic"}},{"node":{"ID":4062,"Title":"Elastic Stack 8.8.2 released","Description":"<p>Version 8.8.2 of the Elastic Stack was released today. We recommend you <a href=\"https://www.elastic.co/downloads\">upgrade to this latest version</a>. We recommend 8.8.2 over the previous patch versions in 7.17.x.</p>\n<p>From the 8.8.0 release onwards, container images are now signed with <a href=\"https://www.elastic.co/blog/elastic-stack-container-images-signed-sigstore\">cosign/Sigstore</a>.</p>\n<p>The 8.8.2 patch release contains a fix for a potential security vulnerability. Please see our <a href=\"https://discuss.elastic.co/c/announcements/security-announcements/31\">security advisory for more details</a>.</p>\n<p>For details of the issues that have been fixed and a full list of changes for each product in this version, please refer to <a href=\"https://www.elastic.co/guide/en/welcome-to-elastic/8.8/new.html\">the release notes</a>.</p>","PublishedAt":"2023-06-29 00:00:00+00:00","OriginURL":"https://www.elastic.co/blog/elastic-stack-8-8-2-released","SourceName":"Elastic"}},{"node":{"ID":4063,"Title":"Elastic Stack 7.17.11 released","Description":"<p>Version 7.17.11 of the Elastic Stack was released today. We recommend you <a href=\"https://www.elastic.co/downloads\">upgrade to this latest version</a>. We recommend 7.17.11 over the previous patch versions in 7.17.x.</p>\n<p>The 7.17.11 patch release contains a fix for a potential security vulnerability. Please see our <a href=\"https://discuss.elastic.co/c/announcements/security-announcements/31\">security advisory for more details</a>.</p>\n<p>For details of the issues that have been fixed and a full list of changes for each product in this version, please refer to <a href=\"https://www.elastic.co/guide/en/welcome-to-elastic/7.17/new.html\">the release notes</a>.</p>","PublishedAt":"2023-06-29 00:00:00+00:00","OriginURL":"https://www.elastic.co/blog/elastic-stack-7-17-11-released","SourceName":"Elastic"}},{"node":{"ID":4066,"Title":"How does Elastic Security drive value to your organization?","Description":"","PublishedAt":"2023-06-29 00:00:00+00:00","OriginURL":"https://www.elastic.co/blog/how-elastic-security-drive-value-organization","SourceName":"Elastic"}},{"node":{"ID":4056,"Title":"Get started using Google Cloud APIs to manage cloud infrastructure","Description":"","PublishedAt":"2023-06-28 19:03:35+00:00","OriginURL":"https://medium.com/better-practices/get-started-using-google-cloud-apis-to-manage-cloud-infrastructure-bace9cac42e1?source=rss----410f2fbc015d---4","SourceName":"Postman"}},{"node":{"ID":4053,"Title":"Generative AI with Large Language Models — New Hands-on Course by DeepLearning.AI and AWS","Description":"Generative AI has taken the world by storm, and we’re starting to see the next wave of widespread adoption of AI with the potential for every customer experience and application to be reinvented with generative AI. Generative AI lets you to create new content and ideas including conversations, stories, images, videos, and music. Generative AI […]","PublishedAt":"2023-06-28 15:54:55+00:00","OriginURL":"https://aws.amazon.com/blogs/aws/generative-ai-with-large-language-models-new-hands-on-course-by-deeplearning-ai-and-aws/","SourceName":"AWS"}},{"node":{"ID":4051,"Title":"#ClouderaLife Employee Spotlight: Peyton Kettering, Account Executive","Description":"<p>Making connections to help customers and community.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://blog.cloudera.com/clouderalife-employee-spotlight-peyton-kettering-account-executive/\">#ClouderaLife Employee Spotlight: Peyton Kettering, Account Executive</a> appeared first on <a rel=\"nofollow\" href=\"https://blog.cloudera.com\">Cloudera Blog</a>.</p>\n","PublishedAt":"2023-06-28 13:19:20+00:00","OriginURL":"https://blog.cloudera.com/clouderalife-employee-spotlight-peyton-kettering-account-executive/","SourceName":"Cloudera"}},{"node":{"ID":4058,"Title":"Career stories: The power of an impactful mentor","Description":"Initially a Chicago-based data analyst, Jelanah had her heart set on a more meaningful career in frontend (UI) engineering. She chronicles for us how a life-changing engineering mentor at LinkedIn unlocked her frontend technical skills, supported a relocation to Atlanta, and championed a promotion to software engineer. Finding the right fit at LinkedIn After I graduated from college with an interdisciplinary marketing, math and computer science degree, I wanted to find a role that combined my interests in computer science, math, and marketing. I started my career at Allstate in an [&#8230;]","PublishedAt":"2023-06-28 13:00:00+00:00","OriginURL":"https://engineering.linkedin.com/blog/2023/career-stories--the-power-of-an-impactful-mentor","SourceName":"Linkedin"}},{"node":{"ID":4078,"Title":"Designing iOS Screen Navigation for Best UX","Description":"<p>This article from day 17 of Merpay Tech Openness Month 2023 is brought to you by @kris from the Merpay iOS team. The Power of UX in iOS App Development An app’s user experience, or UX for short, simply refers to the experience a user has while interacting with an app. This is usually handled [&hellip;]</p>\n","PublishedAt":"2023-06-28 10:00:10+00:00","OriginURL":"https://engineering.mercari.com/en/blog/entry/20230627-designing-ios-screen-navigation-for-best-ux/","SourceName":"Mercari"}},{"node":{"ID":4076,"Title":"Using insecure npm package manager defaults to steal your macOS keyboard shortcuts","Description":"In this post, we'll discuss npm postinstall, recent security events involving it, and how to protect the sensitive data stored shortcuts stored in your keyboard shortcuts from insecure npm package manager defaults.","PublishedAt":"2023-06-28 05:01:00+00:00","OriginURL":"https://snyk.io/blog/using-insecure-npm-package-manager-defaults/","SourceName":"Snyk"}}]}},"pageContext":{"limit":30,"skip":1680,"numPages":193,"currentPage":57}},"staticQueryHashes":["3649515864"]}