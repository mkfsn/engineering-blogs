{"componentChunkName":"component---src-templates-blog-list-tsx","path":"/page/73","result":{"data":{"allPost":{"edges":[{"node":{"ID":3614,"Title":"AWS Week in Review – AWS Notifications, Serverless event, and More – May 8, 2023","Description":"At the end of this week, I’m flying to Seattle to take part in the AWS Serverless Innovation Day. Along with many customers and colleagues from AWS, we are going to be live on May 17 at a virtual free event. During the AWS Serverless Innovation Day we will share best practices related to building […]","PublishedAt":"2023-05-08 21:57:51+00:00","OriginURL":"https://aws.amazon.com/blogs/aws/aws-week-in-review-aws-notifications-serverless-event-and-more-may-8-2023/","SourceName":"AWS"}},{"node":{"ID":3611,"Title":"Visibility and Transparency","Description":"<p>CDO Spotlight</p>\n<p>The post <a rel=\"nofollow\" href=\"https://blog.cloudera.com/one-big-cluster-stuck-visibility-and-transparency/\">Visibility and Transparency</a> appeared first on <a rel=\"nofollow\" href=\"https://blog.cloudera.com\">Cloudera Blog</a>.</p>\n","PublishedAt":"2023-05-08 18:20:49+00:00","OriginURL":"https://blog.cloudera.com/one-big-cluster-stuck-visibility-and-transparency/","SourceName":"Cloudera"}},{"node":{"ID":3610,"Title":"Why marketers should choose Mixpanel over Google Analytics 4 (GA4)","Description":"<figure><img src=\"https://mixpanel.com/wp-content/uploads/2023/05/Compare-to-XYZ-1024x576.png\" class=\"type:primaryImage\" /></figure>\n<p>Growing a digital product and company is a multi-team sport. For marketing teams focused on getting more user traffic and signups, Google Analytics has been the tool of choice. But with customers spending more time researching and buying online, websites and product experiences are converging to accommodate customer needs, and the scope of analysis for</p>\n<p>The post <a rel=\"nofollow\" href=\"https://mixpanel.com/blog/mixpanel-marketing-analytics-vs-google-analytics/\">Why marketers should choose Mixpanel over Google Analytics 4 (GA4)</a> appeared first on <a rel=\"nofollow\" href=\"https://mixpanel.com\">Mixpanel</a>.</p>\n","PublishedAt":"2023-05-08 16:52:07+00:00","OriginURL":"https://mixpanel.com/blog/mixpanel-marketing-analytics-vs-google-analytics/","SourceName":"Mixpanel"}},{"node":{"ID":3609,"Title":"The European Network Usage Fees proposal is about much more than a fight between Big Tech and Big European telcos","Description":"There’s an important debate happening in Europe that could affect the future of the Internet. The European Commission is considering new rules for how networks connect to each other on the Internet.","PublishedAt":"2023-05-08 16:31:50+00:00","OriginURL":"http://blog.cloudflare.com/eu-network-usage-fees/","SourceName":"Cloudflare"}},{"node":{"ID":3608,"Title":"The 2023 Developer Survey is now live!","Description":"<p>We want to know about all the technology that makes you swoon and scoff.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://stackoverflow.blog/2023/05/08/the-2023-developer-survey-is-now-live/\">The 2023 Developer Survey is now live!</a> appeared first on <a rel=\"nofollow\" href=\"https://stackoverflow.blog\">Stack Overflow Blog</a>.</p>\n","PublishedAt":"2023-05-08 12:03:00+00:00","OriginURL":"https://stackoverflow.blog/2023/05/08/the-2023-developer-survey-is-now-live/","SourceName":"Stack Overflow"}},{"node":{"ID":3662,"Title":"Engineering culture at Snyk: The values that support and drive our teams","Description":"We want Snyk's engineering culture to shape how our software engineers approach problem-solving, innovation, and collaboration. Learn about our values, how we live by them, and how that helps us succeed.","PublishedAt":"2023-05-08 05:00:00+00:00","OriginURL":"https://snyk.io/blog/snyk-engineering-culture","SourceName":"Snyk"}},{"node":{"ID":3607,"Title":"Blog: Kubernetes 1.27: Introducing An API For Volume Group Snapshots","Description":"<p><strong>Author:</strong> Xing Yang (VMware)</p>\n<p>Volume group snapshot is introduced as an Alpha feature in Kubernetes v1.27.\nThis feature introduces a Kubernetes API that allows users to take crash consistent\nsnapshots for multiple volumes together. It uses a label selector to group multiple\n<code>PersistentVolumeClaims</code> for snapshotting.\nThis new feature is only supported for <a href=\"https://kubernetes-csi.github.io/docs/\">CSI</a> volume drivers.</p>\n<h2 id=\"an-overview-of-volume-group-snapshots\">An overview of volume group snapshots</h2>\n<p>Some storage systems provide the ability to create a crash consistent snapshot of\nmultiple volumes. A group snapshot represents “copies” from multiple volumes that\nare taken at the same point-in-time. A group snapshot can be used either to rehydrate\nnew volumes (pre-populated with the snapshot data) or to restore existing volumes to\na previous state (represented by the snapshots).</p>\n<h2 id=\"why-add-volume-group-snapshots-to-kubernetes\">Why add volume group snapshots to Kubernetes?</h2>\n<p>The Kubernetes volume plugin system already provides a powerful abstraction that\nautomates the provisioning, attaching, mounting, resizing, and snapshotting of block\nand file storage.</p>\n<p>Underpinning all these features is the Kubernetes goal of workload portability:\nKubernetes aims to create an abstraction layer between distributed applications and\nunderlying clusters so that applications can be agnostic to the specifics of the\ncluster they run on and application deployment requires no cluster specific knowledge.</p>\n<p>There is already a <a href=\"https://kubernetes.io/docs/concepts/storage/volume-snapshots/\">VolumeSnapshot</a> API\nthat provides the ability to take a snapshot of a persistent volume to protect against\ndata loss or data corruption. However, there are other snapshotting functionalities\nnot covered by the VolumeSnapshot API.</p>\n<p>Some storage systems support consistent group snapshots that allow a snapshot to be\ntaken from multiple volumes at the same point-in-time to achieve write order consistency.\nThis can be useful for applications that contain multiple volumes. For example,\nan application may have data stored in one volume and logs stored in another volume.\nIf snapshots for the data volume and the logs volume are taken at different times,\nthe application will not be consistent and will not function properly if it is restored\nfrom those snapshots when a disaster strikes.</p>\n<p>It is true that you can quiesce the application first, take an individual snapshot from\neach volume that is part of the application one after the other, and then unquiesce the\napplication after all the individual snapshots are taken. This way, you would get\napplication consistent snapshots.</p>\n<p>However, sometimes it may not be possible to quiesce an application or the application\nquiesce can be too expensive so you want to do it less frequently. Taking individual\nsnapshots one after another may also take longer time compared to taking a consistent\ngroup snapshot. Some users may not want to do application quiesce very often for these\nreasons. For example, a user may want to run weekly backups with application quiesce\nand nightly backups without application quiesce but with consistent group support which\nprovides crash consistency across all volumes in the group.</p>\n<h2 id=\"kubernetes-volume-group-snapshots-api\">Kubernetes Volume Group Snapshots API</h2>\n<p>Kubernetes Volume Group Snapshots introduce <a href=\"https://github.com/kubernetes-csi/external-snapshotter/blob/master/client/apis/volumegroupsnapshot/v1alpha1/types.go\">three new API\nobjects</a>\nfor managing snapshots:</p>\n<dl>\n<dt><code>VolumeGroupSnapshot</code></dt>\n<dd>Created by a Kubernetes user (or perhaps by your own automation) to request\ncreation of a volume group snapshot for multiple persistent volume claims.\nIt contains information about the volume group snapshot operation such as the\ntimestamp when the volume group snapshot was taken and whether it is ready to use.\nThe creation and deletion of this object represents a desire to create or delete a\ncluster resource (a group snapshot).</dd>\n<dt><code>VolumeGroupSnapshotContent</code></dt>\n<dd>Created by the snapshot controller for a dynamically created VolumeGroupSnapshot.\nIt contains information about the volume group snapshot including the volume group\nsnapshot ID.\nThis object represents a provisioned resource on the cluster (a group snapshot).\nThe VolumeGroupSnapshotContent object binds to the VolumeGroupSnapshot for which it\nwas created with a one-to-one mapping.</dd>\n<dt><code>VolumeGroupSnapshotClass</code></dt>\n<dd>Created by cluster administrators to describe how volume group snapshots should be\ncreated. including the driver information, the deletion policy, etc.</dd>\n</dl>\n<p>These three API kinds are defined as CustomResourceDefinitions (CRDs).\nThese CRDs must be installed in a Kubernetes cluster for a CSI Driver to support\nvolume group snapshots.</p>\n<h2 id=\"how-do-i-use-kubernetes-volume-group-snapshots\">How do I use Kubernetes Volume Group Snapshots</h2>\n<p>Volume group snapshots are implemented in the\n<a href=\"https://github.com/kubernetes-csi/external-snapshotter\">external-snapshotter</a> repository. Implementing volume\ngroup snapshots meant adding or changing several components:</p>\n<ul>\n<li>Added new CustomResourceDefinitions for VolumeGroupSnapshot and two supporting APIs.</li>\n<li>Volume group snapshot controller logic is added to the common snapshot controller.</li>\n<li>Volume group snapshot validation webhook logic is added to the common snapshot validation webhook.</li>\n<li>Adding logic to make CSI calls into the snapshotter sidecar controller.</li>\n</ul>\n<p>The volume snapshot controller, CRDs, and validation webhook are deployed once per\ncluster, while the sidecar is bundled with each CSI driver.</p>\n<p>Therefore, it makes sense to deploy the volume snapshot controller, CRDs, and validation\nwebhook as a cluster addon. I strongly recommend that Kubernetes distributors\nbundle and deploy the volume snapshot controller, CRDs, and validation webhook as part\nof their Kubernetes cluster management process (independent of any CSI Driver).</p>\n<h3 id=\"creating-a-new-group-snapshot-with-kubernetes\">Creating a new group snapshot with Kubernetes</h3>\n<p>Once a VolumeGroupSnapshotClass object is defined and you have volumes you want to\nsnapshot together, you may request a new group snapshot by creating a VolumeGroupSnapshot\nobject.</p>\n<p>The source of the group snapshot specifies whether the underlying group snapshot\nshould be dynamically created or if a pre-existing VolumeGroupSnapshotContent\nshould be used.</p>\n<p>A pre-existing VolumeGroupSnapshotContent is created by a cluster administrator.\nIt contains the details of the real volume group snapshot on the storage system which\nis available for use by cluster users.</p>\n<p>One of the following members in the source of the group snapshot must be set.</p>\n<ul>\n<li><code>selector</code> - a label query over PersistentVolumeClaims that are to be grouped\ntogether for snapshotting. This labelSelector will be used to match the label\nadded to a PVC.</li>\n<li><code>volumeGroupSnapshotContentName</code> - specifies the name of a pre-existing\nVolumeGroupSnapshotContent object representing an existing volume group snapshot.</li>\n</ul>\n<p>In the following example, there are two PVCs.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span>NAME STATUS VOLUME CAPACITY ACCESSMODES AGE<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span>pvc-0 Bound pvc-a42d7ea2-e3df-11ed-b5ea-0242ac120002 1Gi RWO 48s<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span>pvc-1 Bound pvc-a42d81b8-e3df-11ed-b5ea-0242ac120002 1Gi RWO 48s<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Label the PVCs.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span>% kubectl label pvc pvc-0 group=myGroup<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span>persistentvolumeclaim/pvc-0 labeled<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span>% kubectl label pvc pvc-1 group=myGroup<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span>persistentvolumeclaim/pvc-1 labeled<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>For dynamic provisioning, a selector must be set so that the snapshot controller can\nfind PVCs with the matching labels to be snapshotted together.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>groupsnapshot.storage.k8s.io/v1alpha1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>VolumeGroupSnapshot<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>new-group-snapshot-demo<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespace</span>:<span style=\"color:#bbb\"> </span>demo-namespace<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">volumeGroupSnapshotClassName</span>:<span style=\"color:#bbb\"> </span>csi-groupSnapclass<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">source</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">selector</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">matchLabels</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">group</span>:<span style=\"color:#bbb\"> </span>myGroup<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>In the VolumeGroupSnapshot spec, a user can specify the VolumeGroupSnapshotClass which\nhas the information about which CSI driver should be used for creating the group snapshot.</p>\n<p>Two individual volume snapshots will be created as part of the volume group snapshot creation.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span>snapshot-62abb5db7204ac6e4c1198629fec533f2a5d9d60ea1a25f594de0bf8866c7947-2023-04-26-2.20.4<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span>snapshot-2026811eb9f0787466171fe189c805a22cdb61a326235cd067dc3a1ac0104900-2023-04-26-2.20.4<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><h3 id=\"how-to-use-group-snapshot-for-restore-in-kubernetes\">How to use group snapshot for restore in Kubernetes</h3>\n<p>At restore time, the user can request a new PersistentVolumeClaim to be created from\na VolumeSnapshot object that is part of a VolumeGroupSnapshot. This will trigger\nprovisioning of a new volume that is pre-populated with data from the specified\nsnapshot. The user should repeat this until all volumes are created from all the\nsnapshots that are part of a group snapshot.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>PersistentVolumeClaim<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>pvc0-restore<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespace</span>:<span style=\"color:#bbb\"> </span>demo-namespace<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">storageClassName</span>:<span style=\"color:#bbb\"> </span>csi-hostpath-sc<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">dataSource</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>snapshot-62abb5db7204ac6e4c1198629fec533f2a5d9d60ea1a25f594de0bf8866c7947-2023-04-26-2.20.4<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>VolumeSnapshot<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">apiGroup</span>:<span style=\"color:#bbb\"> </span>snapshot.storage.k8s.io<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">accessModes</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- ReadWriteOnce<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resources</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">requests</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">storage</span>:<span style=\"color:#bbb\"> </span>1Gi<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><h2 id=\"as-a-storage-vendor-how-do-i-add-support-for-group-snapshots-to-my-csi-driver\">As a storage vendor, how do I add support for group snapshots to my CSI driver?</h2>\n<p>To implement the volume group snapshot feature, a CSI driver <strong>must</strong>:</p>\n<ul>\n<li>Implement a new group controller service.</li>\n<li>Implement group controller RPCs: <code>CreateVolumeGroupSnapshot</code>, <code>DeleteVolumeGroupSnapshot</code>, and <code>GetVolumeGroupSnapshot</code>.</li>\n<li>Add group controller capability <code>CREATE_DELETE_GET_VOLUME_GROUP_SNAPSHOT</code>.</li>\n</ul>\n<p>See the <a href=\"https://github.com/container-storage-interface/spec/blob/master/spec.md\">CSI spec</a>\nand the <a href=\"https://kubernetes-csi.github.io/docs/\">Kubernetes-CSI Driver Developer Guide</a>\nfor more details.</p>\n<p>a CSI Volume Driver as possible, it provides a suggested mechanism to deploy a\ncontainerized CSI driver to simplify the process.</p>\n<p>As part of this recommended deployment process, the Kubernetes team provides a number of\nsidecar (helper) containers, including the\n<a href=\"https://kubernetes-csi.github.io/docs/external-snapshotter.html\">external-snapshotter sidecar container</a>\nwhich has been updated to support volume group snapshot.</p>\n<p>The external-snapshotter watches the Kubernetes API server for the\n<code>VolumeGroupSnapshotContent</code> object and triggers <code>CreateVolumeGroupSnapshot</code> and\n<code>DeleteVolumeGroupSnapshot</code> operations against a CSI endpoint.</p>\n<h2 id=\"what-are-the-limitations\">What are the limitations?</h2>\n<p>The alpha implementation of volume group snapshots for Kubernetes has the following\nlimitations:</p>\n<ul>\n<li>Does not support reverting an existing PVC to an earlier state represented by\na snapshot (only supports provisioning a new volume from a snapshot).</li>\n<li>No application consistency guarantees beyond any guarantees provided by the storage system\n(e.g. crash consistency). See this <a href=\"https://github.com/kubernetes/community/blob/master/wg-data-protection/data-protection-workflows-white-paper.md#quiesce-and-unquiesce-hooks\">doc</a>\nfor more discussions on application consistency.</li>\n</ul>\n<h2 id=\"what-s-next\">What’s next?</h2>\n<p>Depending on feedback and adoption, the Kubernetes team plans to push the CSI\nGroup Snapshot implementation to Beta in either 1.28 or 1.29.\nSome of the features we are interested in supporting include volume replication,\nreplication group, volume placement, application quiescing, changed block tracking, and more.</p>\n<h2 id=\"how-can-i-learn-more\">How can I learn more?</h2>\n<ul>\n<li>The <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/3476-volume-group-snapshot\">design spec</a>\nfor the volume group snapshot feature.</li>\n<li>The <a href=\"https://github.com/kubernetes-csi/external-snapshotter\">code repository</a> for volume group\nsnapshot APIs and controller.</li>\n<li>CSI <a href=\"https://kubernetes-csi.github.io/docs/\">documentation</a> on the group snapshot feature.</li>\n</ul>\n<h2 id=\"how-do-i-get-involved\">How do I get involved?</h2>\n<p>This project, like all of Kubernetes, is the result of hard work by many contributors\nfrom diverse backgrounds working together. On behalf of SIG Storage, I would like to\noffer a huge thank you to the contributors who stepped up these last few quarters\nto help the project reach alpha:</p>\n<ul>\n<li>Alex Meade (<a href=\"https://github.com/ameade\">ameade</a>)</li>\n<li>Ben Swartzlander (<a href=\"https://github.com/bswartz\">bswartz</a>)</li>\n<li>Humble Devassy Chirammal (<a href=\"https://github.com/humblec\">humblec</a>)</li>\n<li>James Defelice (<a href=\"https://github.com/jdef\">jdef</a>)</li>\n<li>Jan Šafránek (<a href=\"https://github.com/jsafrane\">jsafrane</a>)</li>\n<li>Jing Xu (<a href=\"https://github.com/jingxu97\">jingxu97</a>)</li>\n<li>Michelle Au (<a href=\"https://github.com/msau42\">msau42</a>)</li>\n<li>Niels de Vos (<a href=\"https://github.com/nixpanic\">nixpanic</a>)</li>\n<li>Rakshith R (<a href=\"https://github.com/Rakshith-R\">Rakshith-R</a>)</li>\n<li>Raunak Shah (<a href=\"https://github.com/RaunakShah\">RaunakShah</a>)</li>\n<li>Saad Ali (<a href=\"https://github.com/saad-ali\">saad-ali</a>)</li>\n<li>Thomas Watson (<a href=\"https://github.com/rbo54\">rbo54</a>)</li>\n<li>Xing Yang (<a href=\"https://github.com/xing-yang\">xing-yang</a>)</li>\n<li>Yati Padia (<a href=\"https://github.com/yati1998\">yati1998</a>)</li>\n</ul>\n<p>We also want to thank everyone else who has contributed to the project, including others\nwho helped review the <a href=\"https://github.com/kubernetes/enhancements/pull/1551\">KEP</a>\nand the <a href=\"https://github.com/container-storage-interface/spec/pull/519\">CSI spec PR</a>.</p>\n<p>For those interested in getting involved with the design and development of CSI or\nany part of the Kubernetes Storage system, join the\n<a href=\"https://github.com/kubernetes/community/tree/master/sig-storage\">Kubernetes Storage Special Interest Group</a> (SIG).\nWe always welcome new contributors.</p>\n<p>We also hold regular <a href=\"https://docs.google.com/document/d/15tLCV3csvjHbKb16DVk-mfUmFry_Rlwo-2uG6KNGsfw/edit#\">Data Protection Working Group meetings</a>.\nNew attendees are welcome to join our discussions.</p>","PublishedAt":"2023-05-08 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/05/08/kubernetes-1-27-volume-group-snapshot-alpha/","SourceName":"Kubernetes"}},{"node":{"ID":3612,"Title":"Using AIOps effectively with Elastic Observability","Description":"","PublishedAt":"2023-05-08 00:00:00+00:00","OriginURL":"https://www.elastic.co/blog/aiops-automation-analytics-elastic-observability-use-cases","SourceName":"Elastic"}},{"node":{"ID":3613,"Title":"Streamline network investigations with an enhanced querying and map experience","Description":"<img class=\"webfeedsFeaturedVisual rss\" src=\"https://imgix.datadoghq.com/img/blog/datadog-npm-search-map-updates/npm_features_new_hero.png\" width=\"100%\"/>Effective network troubleshooting requires collecting and correlating thousands of data points across your entire stack. The more data you ingest, however, the more data you have to search through in order to locate important signals. This can make it hard to find the information you need during time-sensitive investigations.Datadog Network Performance Monitoring (NPM) provides enhanced search and mapping capabilities that help you quickly pinpoint critical metrics, giving you clear visibility into the health of your network at any given time.","PublishedAt":"2023-05-08 00:00:00+00:00","OriginURL":"https://www.datadoghq.com/blog/datadog-npm-search-map-updates/","SourceName":"Datadog"}},{"node":{"ID":3595,"Title":"Web Summit Rio 2023: Building an app in 18 minutes with GitHub Copilot X","Description":"GitHub CEO Thomas Domke demonstrated the power of GitHub Copilot X live on stage.","PublishedAt":"2023-05-05 14:07:04+00:00","OriginURL":"https://github.blog/2023-05-05-web-summit-rio-2023-building-an-app-in-18-minutes-with-github-copilot-x/","SourceName":"GitHub"}},{"node":{"ID":3596,"Title":"The Overflow #176: Jobs that save the world","Description":"<p>KYC, JPG size mysteries, and fluid typography</p>\n<p>The post <a rel=\"nofollow\" href=\"https://stackoverflow.blog/2023/05/05/the-overflow-176-jobs-that-save-the-world/\">The Overflow #176: Jobs that save the world </a> appeared first on <a rel=\"nofollow\" href=\"https://stackoverflow.blog\">Stack Overflow Blog</a>.</p>\n","PublishedAt":"2023-05-05 12:16:00+00:00","OriginURL":"https://stackoverflow.blog/2023/05/05/the-overflow-176-jobs-that-save-the-world/","SourceName":"Stack Overflow"}},{"node":{"ID":3594,"Title":"How the Miro Developer Platform leverages contract testing","Description":"","PublishedAt":"2023-05-05 09:38:33+00:00","OriginURL":"https://medium.com/miro-engineering/how-the-miro-developer-platform-leverages-contract-testing-91fae20987d1?source=rss----555f7fd62a50---4","SourceName":"Miro Engineering"}},{"node":{"ID":3593,"Title":"Debugging React Native Apps","Description":"","PublishedAt":"2023-05-05 05:41:02+00:00","OriginURL":"https://medium.com/engineering-housing/debugging-react-native-apps-41e2c2bfaf8e?source=rss----3a69e32e2594---4","SourceName":"Housing.com"}},{"node":{"ID":3597,"Title":"Building golden paths for developers (Ep. 567)","Description":"<p>The home team talks with Luca Galante of Humanitec about how platform engineering is more art than science, how self-service platforms empower developers with “golden paths,” and why he’s excited, not anxious, about AI tools (at least for now).</p>\n<p>The post <a rel=\"nofollow\" href=\"https://stackoverflow.blog/2023/05/05/building-golden-paths-for-developers-ep-567/\">Building golden paths for developers (Ep. 567)</a> appeared first on <a rel=\"nofollow\" href=\"https://stackoverflow.blog\">Stack Overflow Blog</a>.</p>\n","PublishedAt":"2023-05-05 04:40:00+00:00","OriginURL":"https://stackoverflow.blog/2023/05/05/building-golden-paths-for-developers-ep-567/","SourceName":"Stack Overflow"}},{"node":{"ID":3591,"Title":"Blog: Kubernetes 1.27: Quality-of-Service for Memory Resources (alpha)","Description":"<p><strong>Authors:</strong> Dixita Narang (Google)</p>\n<p>Kubernetes v1.27, released in April 2023, introduced changes to\nMemory QoS (alpha) to improve memory management capabilites in Linux nodes.</p>\n<p>Support for Memory QoS was initially added in Kubernetes v1.22, and later some\n<a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2570-memory-qos#reasons-for-changing-the-formula-of-memoryhigh-calculation-in-alpha-v127\">limitations</a>\naround the formula for calculating <code>memory.high</code> were identified. These limitations are\naddressed in Kubernetes v1.27.</p>\n<h2 id=\"background\">Background</h2>\n<p>Kubernetes allows you to optionally specify how much of each resources a container needs\nin the Pod specification. The most common resources to specify are CPU and Memory.</p>\n<p>For example, a Pod manifest that defines container resource requirements could look like:</p>\n<pre tabindex=\"0\"><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: example\nspec:\ncontainers:\n- name: nginx\nresources:\nrequests:\nmemory: &#34;64Mi&#34;\ncpu: &#34;250m&#34;\nlimits:\nmemory: &#34;64Mi&#34;\ncpu: &#34;500m&#34;\n</code></pre><ul>\n<li>\n<p><code>spec.containers[].resources.requests</code></p>\n<p>When you specify the resource request for containers in a Pod, the\n<a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/#kube-scheduler\">Kubernetes scheduler</a>\nuses this information to decide which node to place the Pod on. The scheduler\nensures that for each resource type, the sum of the resource requests of the\nscheduled containers is less than the total allocatable resources on the node.</p>\n</li>\n<li>\n<p><code>spec.containers[].resources.limits</code></p>\n<p>When you specify the resource limit for containers in a Pod, the kubelet enforces\nthose limits so that the running containers are not allowed to use more of those\nresources than the limits you set.</p>\n</li>\n</ul>\n<p>When the kubelet starts a container as a part of a Pod, kubelet passes the\ncontainer's requests and limits for CPU and memory to the container runtime.\nThe container runtime assigns both CPU request and CPU limit to a container.\nProvided the system has free CPU time, the containers are guaranteed to be\nallocated as much CPU as they request. Containers cannot use more CPU than\nthe configured limit i.e. containers CPU usage will be throttled if they\nuse more CPU than the specified limit within a given time slice.</p>\n<p>Prior to Memory QoS feature, the container runtime only used the memory\nlimit and discarded the memory <code>request</code> (requests were, and still are,\nalso used to influence <a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/#scheduling\">scheduling</a>).\nIf a container uses more memory than the configured limit,\nthe Linux Out Of Memory (OOM) killer will be invoked.</p>\n<p>Let's compare how the container runtime on Linux typically configures memory\nrequest and limit in cgroups, with and without Memory QoS feature:</p>\n<ul>\n<li>\n<p><strong>Memory request</strong></p>\n<p>The memory request is mainly used by kube-scheduler during (Kubernetes) Pod\nscheduling. In cgroups v1, there are no controls to specify the minimum amount\nof memory the cgroups must always retain. Hence, the container runtime did not\nuse the value of requested memory set in the Pod spec.</p>\n<p>cgroups v2 introduced a <code>memory.min</code> setting, used to specify the minimum\namount of memory that should remain available to the processes within\na given cgroup. If the memory usage of a cgroup is within its effective\nmin boundary, the cgroup’s memory won’t be reclaimed under any conditions.\nIf the kernel cannot maintain at least <code>memory.min</code> bytes of memory for the\nprocesses within the cgroup, the kernel invokes its OOM killer. In other words,\nthe kernel guarantees at least this much memory is available or terminates\nprocesses (which may be outside the cgroup) in order to make memory more available.\nMemory QoS maps <code>memory.min</code> to <code>spec.containers[].resources.requests.memory</code>\nto ensure the availability of memory for containers in Kubernetes Pods.</p>\n</li>\n<li>\n<p><strong>Memory limit</strong></p>\n<p>The <code>memory.limit</code> specifies the memory limit, beyond which if the container tries\nto allocate more memory, Linux kernel will terminate a process with an\nOOM (Out of Memory) kill. If the terminated process was the main (or only) process\ninside the container, the container may exit.</p>\n<p>In cgroups v1, <code>memory.limit_in_bytes</code> interface is used to set the memory usage limit.\nHowever, unlike CPU, it was not possible to apply memory throttling: as soon as a\ncontainer crossed the memory limit, it would be OOM killed.</p>\n<p>In cgroups v2, <code>memory.max</code> is analogous to <code>memory.limit_in_bytes</code> in cgroupv1.\nMemory QoS maps <code>memory.max</code> to <code>spec.containers[].resources.limits.memory</code> to\nspecify the hard limit for memory usage. If the memory consumption goes above this\nlevel, the kernel invokes its OOM Killer.</p>\n<p>cgroups v2 also added <code>memory.high</code> configuration . Memory QoS uses <code>memory.high</code>\nto set memory usage throttle limit. If the <code>memory.high</code> limit is breached,\nthe offending cgroups are throttled, and the kernel tries to reclaim memory\nwhich may avoid an OOM kill.</p>\n</li>\n</ul>\n<h2 id=\"how-it-works\">How it works</h2>\n<h3 id=\"cgroups-v2-memory-controller-interfaces-kubernetes-container-resources-mapping\">Cgroups v2 memory controller interfaces &amp; Kubernetes container resources mapping</h3>\n<p>Memory QoS uses the memory controller of cgroups v2 to guarantee memory resources in\nKubernetes. cgroupv2 interfaces that this feature uses are:</p>\n<ul>\n<li><code>memory.max</code></li>\n<li><code>memory.min</code></li>\n<li><code>memory.high</code>.</li>\n</ul>\n<figure>\n<img src=\"https://kubernetes.io/blog/2023/05/05/qos-memory-resources/memory-qos-cal.svg\"\nalt=\"Memory QoS Levels\"/> <figcaption>\n<h4>Memory QoS Levels</h4>\n</figcaption>\n</figure>\n<p><code>memory.max</code> is mapped to <code>limits.memory</code> specified in the Pod spec. The kubelet and\nthe container runtime configure the limit in the respective cgroup. The kernel\nenforces the limit to prevent the container from using more than the configured\nresource limit. If a process in a container tries to consume more than the\nspecified limit, kernel terminates a process(es) with an out of\nmemory Out of Memory (OOM) error.</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2023/05/05/qos-memory-resources/container-memory-max.svg\"\nalt=\"memory.max maps to limits.memory\"/> <figcaption>\n<h4>memory.max maps to limits.memory</h4>\n</figcaption>\n</figure>\n<p><code>memory.min</code> is mapped to <code>requests.memory</code>, which results in reservation of memory resources\nthat should never be reclaimed by the kernel. This is how Memory QoS ensures the availability of\nmemory for Kubernetes pods. If there's no unprotected reclaimable memory available, the OOM\nkiller is invoked to make more memory available.</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2023/05/05/qos-memory-resources/container-memory-min.svg\"\nalt=\"memory.min maps to requests.memory\"/> <figcaption>\n<h4>memory.min maps to requests.memory</h4>\n</figcaption>\n</figure>\n<p>For memory protection, in addition to the original way of limiting memory usage, Memory QoS\nthrottles workload approaching its memory limit, ensuring that the system is not overwhelmed\nby sporadic increases in memory usage. A new field, <code>memoryThrottlingFactor</code>, is available in\nthe KubeletConfiguration when you enable MemoryQoS feature. It is set to 0.9 by default.\n<code>memory.high</code> is mapped to throttling limit calculated by using <code>memoryThrottlingFactor</code>,\n<code>requests.memory</code> and <code>limits.memory</code> as in the formula below, and rounding down the\nvalue to the nearest page size:</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2023/05/05/qos-memory-resources/container-memory-high.svg\"\nalt=\"memory.high formula\"/> <figcaption>\n<h4>memory.high formula</h4>\n</figcaption>\n</figure>\n<p><strong>Note</strong>: If a container has no memory limits specified, <code>limits.memory</code> is substituted for node allocatable memory.</p>\n<p><strong>Summary:</strong></p>\n<table>\n<tr>\n<th style=\"text-align:center\">File</th>\n<th style=\"text-align:center\">Description</th>\n</tr>\n<tr>\n<td>memory.max</td>\n<td><code>memory.max</code> specifies the maximum memory limit,\na container is allowed to use. If a process within the container\ntries to consume more memory than the configured limit,\nthe kernel terminates the process with an Out of Memory (OOM) error.\n<br>\n<br>\n<i>It is mapped to the container's memory limit specified in Pod manifest.</i>\n</td>\n</tr>\n<tr>\n<td>memory.min</td>\n<td><code>memory.min</code> specifies a minimum amount of memory\nthe cgroups must always retain, i.e., memory that should never be\nreclaimed by the system.\nIf there's no unprotected reclaimable memory available, OOM kill is invoked.\n<br>\n<br>\n<i>It is mapped to the container's memory request specified in the Pod manifest.</i>\n</td>\n</tr>\n<tr>\n<td>memory.high</td>\n<td><code>memory.high</code> specifies the memory usage throttle limit.\nThis is the main mechanism to control a cgroup's memory use. If\ncgroups memory use goes over the high boundary specified here,\nthe cgroups processes are throttled and put under heavy reclaim pressure.\n<br>\n<br>\n<i>Kubernetes uses a formula to calculate <code>memory.high</code>,\ndepending on container's memory request, memory limit or node allocatable memory\n(if container's memory limit is empty) and a throttling factor.\nPlease refer to the <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2570-memory-qos\">KEP</a>\nfor more details on the formula.</i>\n</td>\n</tr>\n</table>\n<p><strong>Note</strong> <code>memory.high</code> is set only on container level cgroups while <code>memory.min</code> is set on\ncontainer, pod, and node level cgroups.</p>\n<h3 id=\"memory-min-calculations-for-cgroups-heirarchy\"><code>memory.min</code> calculations for cgroups heirarchy</h3>\n<p>When container memory requests are made, kubelet passes <code>memory.min</code> to the back-end\nCRI runtime (such as containerd or CRI-O) via the <code>Unified</code> field in CRI during\ncontainer creation. The <code>memory.min</code> in container level cgroups will be set to:</p>\n<p>$memory.min = pod.spec.containers[i].resources.requests[memory]$<br>\n<sub>for every i<sup>th</sup> container in a pod</sub>\n<br>\n<br>\nSince the <code>memory.min</code> interface requires that the ancestor cgroups directories are all\nset, the pod and node cgroups directories need to be set correctly.</p>\n<p><code>memory.min</code> in pod level cgroup:<br>\n$memory.min = \\sum_{i=0}^{no. of pods}pod.spec.containers[i].resources.requests[memory]$<br>\n<sub>for every i<sup>th</sup> container in a pod</sub>\n<br>\n<br>\n<code>memory.min</code> in node level cgroup:<br>\n$memory.min = \\sum_{i}^{no. of nodes}\\sum_{j}^{no. of pods}pod[i].spec.containers[j].resources.requests[memory]$<br>\n<sub>for every j<sup>th</sup> container in every i<sup>th</sup> pod on a node</sub>\n<br>\n<br>\nKubelet will manage the cgroups hierarchy of the pod level and node level cgroups\ndirectly using the libcontainer library (from the runc project), while container\ncgroups limits are managed by the container runtime.</p>\n<h3 id=\"support-for-pod-qos-classes\">Support for Pod QoS classes</h3>\n<p>Based on user feedback for the Alpha feature in Kubernetes v1.22, some users would like\nto opt out of MemoryQoS on a per-pod basis to ensure there is no early memory throttling.\nTherefore, in Kubernetes v1.27 Memory QOS also supports memory.high to be set as per\nQuality of Service(QoS) for Pod classes. Following are the different cases for memory.high\nas per QOS classes:</p>\n<ol>\n<li>\n<p><strong>Guaranteed pods</strong> by their QoS definition require memory requests=memory limits and are\nnot overcommitted. Hence MemoryQoS feature is disabled on those pods by not setting\nmemory.high. This ensures that Guaranteed pods can fully use their memory requests up\nto their set limit, and not hit any throttling.</p>\n</li>\n<li>\n<p><strong>Burstable pods</strong> by their QoS definition require at least one container in the Pod with\nCPU or memory request or limit set.</p>\n<ul>\n<li>\n<p>When requests.memory and limits.memory are set, the formula is used as-is:</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2023/05/05/qos-memory-resources/container-memory-high-limit.svg\"\nalt=\"memory.high when requests and limits are set\"/> <figcaption>\n<h4>memory.high when requests and limits are set</h4>\n</figcaption>\n</figure>\n</li>\n<li>\n<p>When requests.memory is set and limits.memory is not set, limits.memory is substituted\nfor node allocatable memory in the formula:</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2023/05/05/qos-memory-resources/container-memory-high-no-limits.svg\"\nalt=\"memory.high when requests and limits are not set\"/> <figcaption>\n<h4>memory.high when requests and limits are not set</h4>\n</figcaption>\n</figure>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>BestEffort</strong> by their QoS definition do not require any memory or CPU limits or requests.\nFor this case, kubernetes sets requests.memory = 0 and substitute limits.memory for node allocatable\nmemory in the formula:</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2023/05/05/qos-memory-resources/container-memory-high-best-effort.svg\"\nalt=\"memory.high for BestEffort Pod\"/> <figcaption>\n<h4>memory.high for BestEffort Pod</h4>\n</figcaption>\n</figure>\n</li>\n</ol>\n<p><strong>Summary</strong>: Only Pods in Burstable and BestEffort QoS classes will set <code>memory.high</code>.\nGuaranteed QoS pods do not set <code>memory.high</code> as their memory is guaranteed.</p>\n<h2 id=\"how-do-i-use-it\">How do I use it?</h2>\n<p>The prerequisites for enabling Memory QoS feature on your Linux node are:</p>\n<ol>\n<li>Verify the <a href=\"https://kubernetes.io/docs/concepts/architecture/cgroups/#requirements\">requirements</a>\nrelated to <a href=\"https://kubernetes.io/docs/concepts/architecture/cgroups\">Kubernetes support for cgroups v2</a>\nare met.</li>\n<li>Ensure CRI Runtime supports Memory QoS. At the time of writing, only containerd\nand CRI-O provide support compatible with Memory QoS (alpha). This was implemented\nin the following PRs:\n<ul>\n<li>Containerd: <a href=\"https://github.com/containerd/containerd/pull/5627\">Feature: containerd-cri support LinuxContainerResources.Unified #5627</a>.</li>\n<li>CRI-O: <a href=\"https://github.com/cri-o/cri-o/pull/5207\">implement kube alpha features for 1.22 #5207</a>.</li>\n</ul>\n</li>\n</ol>\n<p>Memory QoS remains an alpha feature for Kubernetes v1.27. You can enable the feature by setting\n<code>MemoryQoS=true</code> in the kubelet configuration file:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>kubelet.config.k8s.io/v1beta1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>KubeletConfiguration<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">featureGates</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">MemoryQoS</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#a2f;font-weight:bold\">true</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><h2 id=\"how-do-i-get-involved\">How do I get involved?</h2>\n<p>Huge thank you to all the contributors who helped with the design, implementation,\nand review of this feature:</p>\n<ul>\n<li>Dixita Narang (<a href=\"https://github.com/ndixita\">ndixita</a>)</li>\n<li>Tim Xu (<a href=\"https://github.com/xiaoxubeii\">xiaoxubeii</a>)</li>\n<li>Paco Xu (<a href=\"https://github.com/pacoxu\">pacoxu</a>)</li>\n<li>David Porter(<a href=\"https://github.com/bobbypage\">bobbypage</a>)</li>\n<li>Mrunal Patel(<a href=\"https://github.com/mrunalp\">mrunalp</a>)</li>\n</ul>\n<p>For those interested in getting involved in future discussions on Memory QoS feature,\nyou can reach out SIG Node by several means:</p>\n<ul>\n<li>Slack: <a href=\"https://kubernetes.slack.com/messages/sig-node\">#sig-node</a></li>\n<li><a href=\"https://groups.google.com/forum/#!forum/kubernetes-sig-node\">Mailing list</a></li>\n<li><a href=\"https://github.com/kubernetes/community/labels/sig%2Fnode\">Open Community Issues/PRs</a></li>\n</ul>","PublishedAt":"2023-05-05 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/05/05/qos-memory-resources/","SourceName":"Kubernetes"}},{"node":{"ID":3592,"Title":"Migrating Critical Traffic At Scale with No Downtime — Part 1","Description":"","PublishedAt":"2023-05-04 21:32:37+00:00","OriginURL":"https://netflixtechblog.com/migrating-critical-traffic-at-scale-with-no-downtime-part-1-ba1c7a1c7835?source=rss----2615bd06b42e---4","SourceName":"Netflix"}},{"node":{"ID":3588,"Title":"How to land a job in climate tech","Description":"<p>Climate tech is a niche industry and requires specific strategies to get a job in. </p>\n<p>The post <a rel=\"nofollow\" href=\"https://stackoverflow.blog/2023/05/04/how-to-land-a-job-in-climate-tech/\">How to land a job in climate tech</a> appeared first on <a rel=\"nofollow\" href=\"https://stackoverflow.blog\">Stack Overflow Blog</a>.</p>\n","PublishedAt":"2023-05-04 14:23:50+00:00","OriginURL":"https://stackoverflow.blog/2023/05/04/how-to-land-a-job-in-climate-tech/","SourceName":"Stack Overflow"}},{"node":{"ID":3598,"Title":"Three considerations for building an effective security program","Description":"This is a guest post by Alex Bovee, CEO and Co-Founder of ConductorOne, an identity security company.","PublishedAt":"2023-05-04 05:00:00+00:00","OriginURL":"https://snyk.io/blog/three-considerations-building-security-program","SourceName":"Snyk"}},{"node":{"ID":3599,"Title":"Fixing half a million security vulnerabilities","Description":"In this post, we review the results of this year's Big Fix and the impact our participants made.","PublishedAt":"2023-05-04 05:00:00+00:00","OriginURL":"https://snyk.io/blog/fixing-half-a-million-vulnerabilities","SourceName":"Snyk"}},{"node":{"ID":3600,"Title":"Snyk in a galaxy far away","Description":"In honor of May the 4th, we’re featuring a narrative from an Imperial trooper in a faraway galaxy as he reflects on his organization’s worst day and how it could’ve gone differently.","PublishedAt":"2023-05-04 05:00:00+00:00","OriginURL":"https://snyk.io/blog/snyk-in-a-galaxy-far-away","SourceName":"Snyk"}},{"node":{"ID":3583,"Title":"Blog: Kubernetes 1.27: StatefulSet PVC Auto-Deletion (beta)","Description":"<p><strong>Author:</strong> Matthew Cary (Google)</p>\n<p>Kubernetes v1.27 graduated to beta a new policy mechanism for\n<a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\"><code>StatefulSets</code></a> that controls the lifetime of\ntheir <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/\"><code>PersistentVolumeClaims</code></a> (PVCs). The new PVC\nretention policy lets users specify if the PVCs generated from the <code>StatefulSet</code> spec template should\nbe automatically deleted or retrained when the <code>StatefulSet</code> is deleted or replicas in the <code>StatefulSet</code>\nare scaled down.</p>\n<h2 id=\"what-problem-does-this-solve\">What problem does this solve?</h2>\n<p>A <code>StatefulSet</code> spec can include <code>Pod</code> and PVC templates. When a replica is first created, the\nKubernetes control plane creates a PVC for that replica if one does not already exist. The behavior\nbefore the PVC retention policy was that the control plane never cleaned up the PVCs created for\n<code>StatefulSets</code> - this was left up to the cluster administrator, or to some add-on automation that\nyou’d have to find, check suitability, and deploy. The common pattern for managing PVCs, either\nmanually or through tools such as Helm, is that the PVCs are tracked by the tool that manages them,\nwith explicit lifecycle. Workflows that use <code>StatefulSets</code> must determine on their own what PVCs are\ncreated by a <code>StatefulSet</code> and what their lifecycle should be.</p>\n<p>Before this new feature, when a StatefulSet-managed replica disappears, either because the\n<code>StatefulSet</code> is reducing its replica count, or because its <code>StatefulSet</code> is deleted, the PVC and its\nbacking volume remains and must be manually deleted. While this behavior is appropriate when the\ndata is critical, in many cases the persistent data in these PVCs is either temporary, or can be\nreconstructed from another source. In those cases, PVCs and their backing volumes remaining after\ntheir <code>StatefulSet</code> or replicas have been deleted are not necessary, incur cost, and require manual\ncleanup.</p>\n<h2 id=\"the-new-statefulset-pvc-retention-policy\">The new <code>StatefulSet</code> PVC retention policy</h2>\n<p>The new <code>StatefulSet</code> PVC retention policy is used to control if and when PVCs created from a\n<code>StatefulSet</code>’s <code>volumeClaimTemplate</code> are deleted. There are two contexts when this may occur.</p>\n<p>The first context is when the <code>StatefulSet</code> resource is deleted (which implies that all replicas are\nalso deleted). This is controlled by the <code>whenDeleted</code> policy. The second context, controlled by\n<code>whenScaled</code> is when the <code>StatefulSet</code> is scaled down, which removes some but not all of the replicas\nin a <code>StatefulSet</code>. In both cases the policy can either be <code>Retain</code>, where the corresponding PVCs are\nnot touched, or <code>Delete</code>, which means that PVCs are deleted. The deletion is done with a normal\n<a href=\"https://kubernetes.io/docs/concepts/architecture/garbage-collection/\">object deletion</a>, so that, for example, all\nretention policies for the underlying PV are respected.</p>\n<p>This policy forms a matrix with four cases. I’ll walk through and give an example for each one.</p>\n<ul>\n<li>\n<p><strong><code>whenDeleted</code> and <code>whenScaled</code> are both <code>Retain</code>.</strong></p>\n<p>This matches the existing behavior for <code>StatefulSets</code>, where no PVCs are deleted. This is also\nthe default retention policy. It’s appropriate to use when data on <code>StatefulSet</code> volumes may be\nirreplaceable and should only be deleted manually.</p>\n</li>\n<li>\n<p><strong><code>whenDeleted</code> is <code>Delete</code> and <code>whenScaled</code> is <code>Retain</code>.</strong></p>\n<p>In this case, PVCs are deleted only when the entire <code>StatefulSet</code> is deleted. If the\n<code>StatefulSet</code> is scaled down, PVCs are not touched, meaning they are available to be reattached\nif a scale-up occurs with any data from the previous replica. This might be used for a temporary\n<code>StatefulSet</code>, such as in a CI instance or ETL pipeline, where the data on the <code>StatefulSet</code> is\nneeded only during the lifetime of the <code>StatefulSet</code> lifetime, but while the task is running the\ndata is not easily reconstructible. Any retained state is needed for any replicas that scale\ndown and then up.</p>\n</li>\n<li>\n<p><strong><code>whenDeleted</code> and <code>whenScaled</code> are both <code>Delete</code>.</strong></p>\n<p>PVCs are deleted immediately when their replica is no longer needed. Note this does not include\nwhen a <code>Pod</code> is deleted and a new version rescheduled, for example when a node is drained and\n<code>Pods</code> need to migrate elsewhere. The PVC is deleted only when the replica is no longer needed\nas signified by a scale-down or <code>StatefulSet</code> deletion. This use case is for when data does not\nneed to live beyond the life of its replica. Perhaps the data is easily reconstructable and the\ncost savings of deleting unused PVCs is more important than quick scale-up, or perhaps that when\na new replica is created, any data from a previous replica is not usable and must be\nreconstructed anyway.</p>\n</li>\n<li>\n<p><strong><code>whenDeleted</code> is <code>Retain</code> and <code>whenScaled</code> is <code>Delete</code>.</strong></p>\n<p>This is similar to the previous case, when there is little benefit to keeping PVCs for fast\nreuse during scale-up. An example of a situation where you might use this is an Elasticsearch\ncluster. Typically you would scale that workload up and down to match demand, whilst ensuring a\nminimum number of replicas (for example: 3). When scaling down, data is migrated away from\nremoved replicas and there is no benefit to retaining those PVCs. However, it can be useful to\nbring the entire Elasticsearch cluster down temporarily for maintenance. If you need to take the\nElasticsearch system offline, you can do this by temporarily deleting the <code>StatefulSet</code>, and\nthen bringing the Elasticsearch cluster back by recreating the <code>StatefulSet</code>. The PVCs holding\nthe Elasticsearch data will still exist and the new replicas will automatically use them.</p>\n</li>\n</ul>\n<p>Visit the\n<a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-policies\">documentation</a> to\nsee all the details.</p>\n<h2 id=\"what-s-next\">What’s next?</h2>\n<p>Try it out! The <code>StatefulSetAutoDeletePVC</code> feature gate is beta and enabled by default on\ncluster running Kubernetes 1.27. Create a <code>StatefulSet</code> using the new policy, test it out and tell\nus what you think!</p>\n<p>I'm very curious to see if this owner reference mechanism works well in practice. For example, I\nrealized there is no mechanism in Kubernetes for knowing who set a reference, so it’s possible that\nthe <code>StatefulSet</code> controller may fight with custom controllers that set their own\nreferences. Fortunately, maintaining the existing retention behavior does not involve any new owner\nreferences, so default behavior will be compatible.</p>\n<p>Please tag any issues you report with the label <code>sig/apps</code> and assign them to Matthew Cary\n(<a href=\"https://github.com/mattcary\">@mattcary</a> at GitHub).</p>\n<p>Enjoy!</p>","PublishedAt":"2023-05-04 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/05/04/kubernetes-1-27-statefulset-pvc-auto-deletion-beta/","SourceName":"Kubernetes"}},{"node":{"ID":3585,"Title":"Industrial control systems security with Elastic Security and Zeek","Description":"","PublishedAt":"2023-05-04 00:00:00+00:00","OriginURL":"https://www.elastic.co/blog/industrial-control-systems-elastic-security-zeek","SourceName":"Elastic"}},{"node":{"ID":3589,"Title":"Monitor OTel-instrumented apps with support for W3C Trace Context","Description":"<img class=\"webfeedsFeaturedVisual rss\" src=\"https://imgix.datadoghq.com/img/blog/monitor-otel-with-w3c-trace-context/w3c_hero.png\" width=\"100%\"/>To get visibility into highly distributed applications, organizations often use various tracing tools that are best suited to each individual service owner’s specifications. However, when a request travels between services that have been instrumented with different tools, the trace data may be formatted differently, resulting in broken traces.W3C Trace Context aims to address this problem by defining a standardized format for unifying trace data from distributed tracing solutions. Datadog APM supports W3C Trace Context, allowing teams to capture complete traces from services that have been instrumented with any system that follows this standard, including OpenTelemetry (OTel) libraries, Datadog’s tracing libraries, Jaeger, and other vendors.","PublishedAt":"2023-05-04 00:00:00+00:00","OriginURL":"https://www.datadoghq.com/blog/monitor-otel-with-w3c-trace-context/","SourceName":"Datadog"}},{"node":{"ID":3590,"Title":"How to use Elasticsearch and Time Series Data Streams for observability metrics","Description":"","PublishedAt":"2023-05-04 00:00:00+00:00","OriginURL":"https://www.elastic.co/blog/elasticsearch-time-series-data-streams-observability-metrics","SourceName":"Elastic"}},{"node":{"ID":3582,"Title":"GitHub Availability Report: April 2023","Description":"In April, we experienced four incidents that resulted in degraded performance across GitHub services. This report also sheds light into three March incidents that resulted in degraded performance across GitHub services. ","PublishedAt":"2023-05-03 21:00:32+00:00","OriginURL":"https://github.blog/2023-05-03-github-availability-report-april-2023/","SourceName":"GitHub"}},{"node":{"ID":3581,"Title":"Introducing Bob’s Used Books—a New, Real-World, .NET Sample Application","Description":"Today, I’m happy to announce that a new open-source sample application, a fictitious used books eCommerce store we call Bob’s Used Books, is available for .NET developers working with AWS. The .NET advocacy and development teams at AWS talk to customers regularly and, during those conversations, often receive requests for more in-depth samples. Customers tell […]","PublishedAt":"2023-05-03 20:56:16+00:00","OriginURL":"https://aws.amazon.com/blogs/aws/introducing-bobs-used-books-a-new-real-world-net-sample-application/","SourceName":"AWS"}},{"node":{"ID":3601,"Title":"Can AI write secure code?","Description":"","PublishedAt":"2023-05-03 20:00:00+00:00","OriginURL":"https://snyk.io/blog/security-risks-coding-with-ai","SourceName":"Snyk"}},{"node":{"ID":3580,"Title":"New – Set Up Your AWS Notifications in One Place","Description":"Today we are launching AWS User Notifications, a single place in the AWS console to set up and view AWS notifications across multiple AWS accounts, Regions, and services. You can centrally set up and view notifications from over 100 AWS services, such as Amazon Simple Storage Service (Amazon S3) objects events, Amazon Elastic Compute Cloud […]","PublishedAt":"2023-05-03 17:35:59+00:00","OriginURL":"https://aws.amazon.com/blogs/aws/new-set-up-your-aws-notifications-in-one-place/","SourceName":"AWS"}},{"node":{"ID":3584,"Title":"Bridging the gap between technology and FinOps with HashiCorp and Intel","Description":"Intel Cloud Optimized Modules for Terraform and Sentinel standardize infrastructure deployment and increase performance with purpose-built policy as code.","PublishedAt":"2023-05-03 16:00:00+00:00","OriginURL":"https://www.hashicorp.com/blog/bridging-the-gap-between-technology-and-finops-with-hashicorp-and-intel","SourceName":"HashiCorp"}},{"node":{"ID":3576,"Title":"Bringing Elastic to robotics developers: Integrating with ROS","Description":"","PublishedAt":"2023-05-03 15:00:00+00:00","OriginURL":"https://www.elastic.co/blog/bringing-elastic-to-robotics-developers-integrating-with-ros","SourceName":"Elastic"}}]}},"pageContext":{"limit":30,"skip":2160,"numPages":193,"currentPage":73}},"staticQueryHashes":["3649515864"]}