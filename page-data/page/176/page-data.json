{"componentChunkName":"component---src-templates-blog-list-tsx","path":"/page/176","result":{"data":{"allPost":{"edges":[{"node":{"ID":796,"Title":"Dependency Inversion as a Driver to Scale Mobile Development","Description":"SoundCloud’s iOS codebase faced a radical change a couple of years ago. The team had decided to modularize the codebase into frameworks…","PublishedAt":"2021-02-24 00:00:00+00:00","OriginURL":"https://developers.soundcloud.com/blog/dependency-inversion-as-a-driver-to-scale-mobile-development","SourceName":"Soundcloud"}},{"node":{"ID":483,"Title":"Chasing (and finding) product-market fit","Description":"<figure><img src=\"https://mixpanel.com/wp-content/uploads/2021/02/Blog-15-1024x577.png\" class=\"type:primaryImage\" /></figure>\n<p>It’s been a long-held notion in startup circles that lack of product-market fit will doom even the scrappiest of founders to fail. There’s a reason, after all, that Y Combinator coined the slogan “make something people want” shortly after they were founded in 2005. And beyond the anecdotal, an often-cited 2019 study by CB Insights</p>\n<p>The post <a rel=\"nofollow\" href=\"https://mixpanel.com/blog/what-14-startup-investors-and-advisors-taught-us-about-chasing-and-finding-product-market-fit/\">Chasing (and finding) product-market fit</a> appeared first on <a rel=\"nofollow\" href=\"https://mixpanel.com\">Mixpanel</a>.</p>\n","PublishedAt":"2021-02-23 03:05:00+00:00","OriginURL":"https://mixpanel.com/blog/what-14-startup-investors-and-advisors-taught-us-about-chasing-and-finding-product-market-fit/","SourceName":"Mixpanel"}},{"node":{"ID":619,"Title":"Rethinking the Next-gen Analytics Web App at trivago","Description":"After almost a decade, we decided to rebuild our in-house Business Intelligence web application to better support the organization. It is always challenging to replace  software with a long history and a high degree of complexity. Nevertheless, we successfully completed the project because we fundamentally challenged and re-thought all aspects of the project.","PublishedAt":"2021-02-09 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2021-02-09-rethinknextgenanalyticswebappattrivago/","SourceName":"Trivago"}},{"node":{"ID":425,"Title":"Indeed’s FOSS Contributor Fund: 2021 Updates","Description":"<p>&#160; &#160; Indeed’s FOSS Contributor Fund is now live for 2021. The program enables Indeed employees who make open source contributions to nominate and vote for projects to receive a donation from the fund. We’re proud that the fund has introduced more Indeedians to the open source community. This post shares updates for those interested [&#8230;]</p>\n<p> </p>\n","PublishedAt":"2021-02-08 22:34:48+00:00","OriginURL":"https://engineering.indeedblog.com/blog/2021/02/indeeds-foss-contributor-fund-2021-updates/","SourceName":"Indeed"}},{"node":{"ID":797,"Title":"Tests Under the Magnifying Lens","Description":"Testing is at the heart of engineering practices at SoundCloud. We strive to build well-balanced test pyramids within our code repositories…","PublishedAt":"2021-02-02 00:00:00+00:00","OriginURL":"https://developers.soundcloud.com/blog/tests-under-the-magnifying-lens","SourceName":"Soundcloud"}},{"node":{"ID":620,"Title":"Keeping up the Open Source promise for 2021","Description":"COVID-19 has impacted the travel industry very severely. Even in these hard times, trivago remains committed to contributing to open source. As a tech company working on large-scale projects, we feel a responsibility towards supporting the open source community. webpack, Preact and the CNCF This year we will continue supporting projects and organisations that are having a major impact on the products we build and the happiness of our engineers who use them on a daily basis.","PublishedAt":"2021-01-25 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2021-01-25-keepinguptheopensourcepromisefor2021/","SourceName":"Trivago"}},{"node":{"ID":486,"Title":"The role that Data as a Service has at PayPay","Description":"<p>Paypay uses different data storage solutions in it’s microservices, such as SQL </p>","PublishedAt":"2021-01-21 06:35:18+00:00","OriginURL":"https://blog.paypay.ne.jp/en/paypay-data-as-a-service-daas/","SourceName":"Paypay"}},{"node":{"ID":487,"Title":"Elasticsearch at PayPay","Description":"<p>As you can imagine, PayPay deals with a lot of information and </p>","PublishedAt":"2021-01-18 05:55:44+00:00","OriginURL":"https://blog.paypay.ne.jp/en/elasticsearch-at-paypay/","SourceName":"Paypay"}},{"node":{"ID":488,"Title":"What are the different levels of automation testing? : Mobile Testing","Description":"<p>This is Part 6 of our automation series. Read Part 1 to </p>","PublishedAt":"2021-01-12 07:41:55+00:00","OriginURL":"https://blog.paypay.ne.jp/en/mobile-automation-testing/","SourceName":"Paypay"}},{"node":{"ID":257,"Title":"Part 2: Computer Vision @ GIPHY: How we Created an AutoTagging Model Using Deep Learning","Description":"This is part two of the GIPHY Autotagging blog post series, where we’ll cover modeling, configuration of our training environment, and share our results. In part one, we outlined our motivation for this product and provided an overview of existing related approaches. Additionally, we described the training and evaluation data we have at hand and [&#8230;]","PublishedAt":"2021-01-11 17:43:34+00:00","OriginURL":"https://engineering.giphy.com/part-2-computer-vision-giphy-how-we-created-an-autotagging-model-using-deep-learning/","SourceName":"GIPHY"}},{"node":{"ID":621,"Title":"Trivago Tech Week","Description":"","PublishedAt":"2021-01-06 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2021-01-06-trivagotechweek/","SourceName":"Trivago"}},{"node":{"ID":351,"Title":"Building a Monorepo with Yarn 2","Description":"<p>In true JavaScript fashion, there was no shortage of releases in the JavaScript ecosystem this year. This includes the Yarn project’s release of Yarn 2 with a compressed cache of JavaScript dependencies, including a Yarn binary to reference,  that can be used for a zero-install deployment. </p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1608668413-yarn.png\" alt=\"Ball of yarn and knitting needles illustration\"></p>\n\n<p>Yarn is a package manager that also provides developers a project management toolset. Now, Yarn 2 is now officially supported by Heroku, and Heroku developers are able to take advantage of leveraging zero-installs during their Node.js builds. We’ll go over a popular use case for Yarn that is enhanced by Yarn 2: using workspaces to manage dependencies for your monorepo.</p>\n\n<p>We will cover taking advantage of Yarn 2’s cache to manage monorepo dependencies. Prerequisites for this include a development environment with Node installed. To follow these guides, set up an existing Node project that makes use of a <code>package.json</code> too. If you don’t have one, use the <a href=\"https://github.com/heroku/node-js-getting-started\">Heroku Getting Started with Node.js Project</a>.</p>\n<h2 class=\"anchored\">\n  <a name=\"workspaces\" href=\"#workspaces\">Workspaces</a>\n</h2>\n\n<p>First off, what are workspaces? Workspaces is Yarn’s solution to a monorepo structure for a JavaScript app or Node.js project. A monorepo refers to a project, in this case, a JavaScript project, that has more than one section of the code base. For example, you may have the following set up:</p>\n\n<pre><code class=\"language-javascript\">/app\n - package.json\n - /server\n   - package.json\n - /ui\n   - package.json\n</code></pre>\n\n<p>Your JavaScript server has source code, but there’s an additional front end application that will be built and made available to users separately. This is a popular pattern for setting up a separation of concerns with a custom API client, a build or testing tool, or something else that may not have a place in the application logic. Each of the subdirectory’s <code>package.json</code> will have their own dependencies. How can we manage them? How do we optimize caching? This is where Yarn workspaces comes in.</p>\n\n<p>In the root <code>package.json</code>, set up the subdirectories under the <code>workspaces</code> key. You should add this to your <code>package.json</code>:</p>\n\n<pre><code class=\"language-javascript\">\"workspaces\": [\n    \"server\",\n    \"ui\"\n]\n</code></pre>\n\n<p>For more on workspaces, visit here: <a href=\"https://yarnpkg.com/features/workspaces\">https://yarnpkg.com/features/workspaces</a></p>\n\n<p>Additionally, add the <code>workspaces-tools</code> plugin. This will be useful when running workspace scripts that you’ll use later. You can do this by running:</p>\n\n<pre><code class=\"language-javascript\">yarn plugin import workspace-tools\n</code></pre>\n<h2 class=\"anchored\">\n  <a name=\"setting-up-yarn\" href=\"#setting-up-yarn\">Setting up Yarn</a>\n</h2>\n\n<p>If you’re already using Yarn, you have a <code>yarn.lock</code> file already checked into your code base’s git repository. There’s other files and directories that you’ll need up to set up the cache. If you aren’t already using Yarn, install it globally.</p>\n\n<pre><code class=\"language-javascript\">npm install -g yarn\n</code></pre>\n\n<p><em>Note: If you don’t have Yarn &gt;=1.22.10 installed on your computer, update it with the same install command.</em></p>\n\n<p>Next, set up your Yarn version for this code base. One of the benefits of using Yarn 2 is that you’ll have a checked in Yarn binary that will be used by anyone that works on this code base and eliminates version conflicts between environments.</p>\n\n<pre><code class=\"language-javascript\">yarn set version berry\n</code></pre>\n\n<p>A <code>.yarn</code> directory and <code>.yarnrc.yml</code> file will both be created that need to be checked into git. These are the files that will set up your project’s local Yarn instance.</p>\n<h2 class=\"anchored\">\n  <a name=\"setting-up-the-dependency-cache\" href=\"#setting-up-the-dependency-cache\">Setting Up the Dependency Cache</a>\n</h2>\n\n<p>Once Yarn is set up, you can set up your cache. Run yarn install:</p>\n\n<pre><code class=\"language-javascript\">yarn\n</code></pre>\n\n<p>Before anything else, make sure to add the following to the <code>.gitignore</code>:</p>\n\n<pre><code class=\"language-javascript\"># Yarn\n.yarn/*\n!.yarn/cache\n!.yarn/releases\n!.yarn/plugins\n!.yarn/sdks\n!.yarn/versions\n</code></pre>\n\n<p>The files that are ignored will be machine specific, and the remaining files you’ll want to check in. If you run <code>git status</code>, you’ll see the following:</p>\n\n<pre><code class=\"language-javascript\">Untracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n    .gitignore\n    .pnp.js\n    .yarn/cache/\n    yarn.lock\n</code></pre>\n\n<p>You’ve created new files that will speed up your install process:</p>\n\n<ul>\n<li>\n<code>.pnp.js</code> - This is the Plug’n’Play (PnP) file. The PnP file tells your Node app or build how to find the dependencies that are stored in <code>.yarn/cache</code>.</li>\n<li>\n<code>.yarn/cache</code> - This directory will have the dependencies that are needed to run and build your app.</li>\n<li>\n<code>yarn.lock</code> - The lock file still is used to lock the versions that are resolved from the <code>package.json</code>.</li>\n</ul>\n\n<p>Check all of this in to git, and you’re set. For more information about Yarn 2’s zero-install philosophy, read here: <a href=\"https://yarnpkg.com/features/zero-installs\">https://yarnpkg.com/features/zero-installs</a></p>\n<h2 class=\"anchored\">\n  <a name=\"adding-dependencies-to-subdirectories\" href=\"#adding-dependencies-to-subdirectories\">Adding Dependencies to Subdirectories</a>\n</h2>\n\n<p>Now that Yarn and the cache are set up, we can start adding dependencies. As initially shown, we have a <code>server</code> directory and a <code>ui</code> directory. We can assume that each of these will be built and hosted differently. For example, my server is written in TypeScript, using Express.js for routing, and running on a Heroku web dyno. For the front end app, it is using Next.js. The build will be run during the app’s build process.</p>\n\n<p>Add <code>express</code> to the server <code>dependencies</code>. </p>\n\n<pre><code class=\"language-javascript\">yarn workspace server add express\n</code></pre>\n\n<p>Additionally, add <code>@types/express</code> and <code>typescript</code> to the <code>devDependencies</code>. You can use the <code>-D</code> flag to indicate that you’re adding <code>devDependencies</code>. </p>\n\n<pre><code class=\"language-javascript\">yarn workspace server add @types/express typescript -D\n</code></pre>\n\n<p>We now have our dependencies in our <code>server</code> workspace. We just need to create our <code>ui</code> workspace. Next, build a Next.js app with the <code>yarn create</code> command.</p>\n\n<pre><code class=\"language-javascript\">yarn create next-app ui\n</code></pre>\n\n<p>Finally, run <code>yarn</code> again to update the cache and check these changes into git.</p>\n<h2 class=\"anchored\">\n  <a name=\"running-scripts-with-workspaces\" href=\"#running-scripts-with-workspaces\">Running Scripts with Workspaces</a>\n</h2>\n\n<p>The last piece is to run scripts within the workspaces. If you look through your source code, you’ll see that there’s one global cache for all dependencies under your app’s root directory. Run the following to see all the compressed dependencies:</p>\n\n<pre><code class=\"language-javascript\">ls .yarn/cache\n</code></pre>\n\n<p>Now, lets run build scripts with workspaces. First, set up the workspace. For server, use <code>tsc</code> to build the TypeScript app. You’ll need to set up a TypeScript config and a <code>.ts</code> file first:</p>\n\n<pre><code class=\"language-javascript\">cd server\nyarn dlx --package typescript tsc --init\ntouch index.ts\n</code></pre>\n\n<p><code>yarn dlx</code> will run a command from a package so that it doesn’t need to be installed globally. It’s useful for one-off initializing commands, like initializing a TypeScript app.</p>\n\n<p>Next, add the build step to the <code>server/package.json</code>.</p>\n\n<pre><code class=\"language-javascript\">\"scripts\": {\n    \"build\": \"tsc\",\n    \"start\": \"node index.js\"\n},\n</code></pre>\n\n<p>Change directories back to the application level, and run the build.</p>\n\n<pre><code class=\"language-javascript\">cd ..\nyarn workspace server build\n</code></pre>\n\n<p>You’ll see that a <code>server/index.js</code> file is created. Add <code>server/*.js</code> to the <code>.gitignore</code>.</p>\n\n<p>Since we already have <code>build</code> and <code>start</code> scripts in our Next.js app (created by the <code>yarn create</code> command), add a build script at the root level <code>package.json</code>.</p>\n\n<pre><code class=\"language-javascript\">\"scripts\": {\n    \"build\": \"yarn workspaces foreach run build\"\n},\n</code></pre>\n\n<p>This is when the <code>workspaces-tool</code> plugin is used. Run <code>yarn build</code> from your app’s root, and both of your workspaces will build. Open a second terminal, and you’ll be able to run <code>yarn workspace server start</code> and <code>yarn workspace ui start</code> in each terminal and run the Express and Next servers in parallel.</p>\n<h2 class=\"anchored\">\n  <a name=\"deploy-to-heroku\" href=\"#deploy-to-heroku\">Deploy to Heroku</a>\n</h2>\n\n<p>Finally, we can deploy our code to Heroku. Since Heroku will run the script is in the <code>package.json</code> under <code>start</code>, add a script to the <code>package.json</code>.</p>\n\n<pre><code class=\"language-javascript\">\"scripts\": {\n    \"build\": \"yarn workspaces foreach run build\",\n    \"start\": \"yarn workspaces server start\"\n},\n</code></pre>\n\n<p><a href=\"https://devcenter.heroku.com/articles/deploying-nodejs#specifying-a-start-script\">Heroku will use the <code>start</code> script</a> from the <code>package.json</code> to start the <code>web</code> process on your app.</p>\n<h2 class=\"anchored\">\n  <a name=\"conclusion\" href=\"#conclusion\">Conclusion</a>\n</h2>\n\n<p>There are <a href=\"https://yarnpkg.com/features\">plenty more features</a> that Yarn, and specifically Yarn 2, offers that are useful for Heroku developers. Check out the Yarn docs to see if there are additional workspace features that may work nicely with Heroku integration. As always, if you have any feedback or issues, please <a href=\"https://github.com/heroku/heroku-buildpack-nodejs/issues/new/choose\">open an Issue on GitHub</a>.</p>","PublishedAt":"2020-12-22 20:53:08+00:00","OriginURL":"https://blog.heroku.com/building-a-monorepo-with-yarn-2","SourceName":"Heroku"}},{"node":{"ID":489,"Title":"How to parse and verify JPQR, the standardized QR code of Japan","Description":"<p>Like PayPay, other cashless payment services each have their unique QR code, </p>","PublishedAt":"2020-12-22 03:43:04+00:00","OriginURL":"https://blog.paypay.ne.jp/en/paypay-adapting-to-jpqr/","SourceName":"Paypay"}},{"node":{"ID":258,"Title":"GIPHY Search Trends During the 2020 Presidential Election","Description":"Major social and cultural events mean “all hands on deck” here at GIPHY, as millions of people across the world use GIFs to express themselves as these events unfold. Award shows, championship games, and holidays (such as New Year’s Eve) require resources across the company — from our Editorial team “live-GIFing” video feeds in real-time [&#8230;]","PublishedAt":"2020-12-14 20:23:05+00:00","OriginURL":"https://engineering.giphy.com/giphy-search-trends-during-the-2020-presidential-election/","SourceName":"GIPHY"}},{"node":{"ID":352,"Title":"Extend Flows with Heroku Compute: An Event-Driven Pattern","Description":"<p><em>This post <a href=\"https://medium.com/salesforce-architects/extend-flows-with-heroku-compute-an-event-driven-pattern-a9840a91ce5b\">previously appeared</a> on the Salesforce Architects blog.</em></p>\n\n<p>Event-driven application architectures have proven to be effective for implementing enterprise solutions using loosely coupled services that interact by exchanging asynchronous events. Salesforce enables event-driven architectures (EDAs) with Platform Events and Change Data Capture (CDC) events as well as triggers and Apex callouts, which makes the Salesforce Platform a great way to build all of your <a href=\"https://www.salesforce.com/products/platform/best-practices/understanding-digital-customer-experience/\">digital customer experiences</a>. This post is the first in a series that covers various EDA patterns, considerations for using them, and examples deployed on the Salesforce Platform.</p>\n<h2 class=\"anchored\">\n  <a name=\"expanding-the-event-driven-architecture-of-the-salesforce-platform\" href=\"#expanding-the-event-driven-architecture-of-the-salesforce-platform\">Expanding the event-driven architecture of the Salesforce Platform</a>\n</h2>\n\n<p>Back in April, Frank Caron wrote a <a href=\"https://developer.salesforce.com/blogs/2020/04/event-driven-app-architecture-on-the-customer-360-platform.html\">blog post</a> describing the power of EDAs. In it, he covered the event-driven approach and the benefits of loosely coupled service interactions. He focused mainly on use cases where events triggered actions across platform services as well as how incorporating third-party external services can greatly expand the power of applications developed using declarative low-code tools like Salesforce Flow.</p>\n\n<p>As powerful as flows can be for accessing third-party services, even greater power comes when your own custom applications, running your own business logic on the Salesforce Platform, are part of flows. </p>\n\n<p>API-first, event-driven <a href=\"https://medium.com/adobetech/three-principles-of-api-first-design-fa6666d9f694\">design</a> is the kind of development that frequently requires collaboration across different members of you team. Low-code builders with domain expertise who are familiar with the business requirements can build the flows. Programmers are typically necessary to develop the back-end services that implement the business logic. An enterprise architect may get involved as well to design the service APIs.</p>\n\n<p>However you are organized, you will need to expose your services with APIs and enable them to produce and consume events. The Salesforce Platform enables this with the <a href=\"https://www.salesforce.com/video/1771211/\">Salesforce Event Bus</a>, <a href=\"https://developer.salesforce.com/blogs/2019/11/introducing-salesforce-evergreen.html\">Salesforce Functions</a>, and <a href=\"https://developer.salesforce.com/docs/atlas.en-us.228.0.api_streaming.meta/api_streaming/intro_stream.htm\">Streaming API</a> as well as support for OpenAPI specification for external services. </p>\n\n<p>Heroku capabilities on the Salesforce Platform include event streaming, relational data stores, and key-value caches seamlessly integrated with elastic compute. These capabilities, combined with deployment automation and hands-off operational excellence, lets your developers focus entirely on delivering your unique business requirements. Seamless integration with the rest of Salesforce makes your apps deployed on Heroku the foundation for complete, compelling, economical, secure, and successful solutions.</p>\n\n<p>This post focuses on expanding flows with Heroku compute. Specifically, how to expose Heroku apps as external services and securely access them via flows using Flow Builder as the low-code development environment. Subsequent posts will expand this idea to include event-driven interactions between Heroku apps and the rest of the Salesforce Platform as well as other examples of how Salesforce Platform based EDAs address common challenges we see across many of our customers including:</p>\n\n<ul>\n<li>Multi-organization visibility and reporting</li>\n<li>Shared event bus designs</li>\n<li>B2C apps with Lightning Web Components</li>\n</ul>\n<h2 class=\"anchored\">\n  <a name=\"building-salesforce-flows-with-your-own-business-logic\" href=\"#building-salesforce-flows-with-your-own-business-logic\">Building Salesforce flows with your own business logic</a>\n</h2>\n\n<p>Salesforce external services are a great way to access third-party services from a flow. All you need are the services’ OpenAPI spec schema (OAS schema), and you’re set to go. There are some great examples of how to register your external services <a href=\"https://andyinthecloud.com/2017/07/23/simplified-api-integrations-with-external-services/\">here</a>, with a more detailed example of how to generate an Apex client and explore your schema <a href=\"https://andyinthecloud.com/2017/09/30/swagger-open-api-salesforce-like/\">here</a>. </p>\n\n<p>But what if you want to incorporate custom business logic into your flow app? What if you wanted to extend and complement the declarative programming model of flows with an imperative model with full programming semantics? What if you wanted to make your app available to flow developers in other organizations, or possibly accessed as a stand-alone service behind a Lightning Web Components based app?</p>\n\n<p>This kind of service deployment typically requires off-platform development, bringing with it all the complexity and operational overhead that goes with meeting the scalability, availability, and reliability requirements of your business critical apps. </p>\n\n<p>The following steps show you how you can deploy your own apps using Heroku on the Salesforce Platform without any of this operational overhead. We’re going to walk through an example of how to build and deploy custom business logic into your own service and access it in a flow. Deployment will be via a Heroku app, which brings the power and flexibility to write your own code, without having to worry about the operational burden of production app deployment or DevOps toolchains. </p>\n\n<p>This approach works well in scenarios where you have programmers and low-code builders working together to deploy a new app. The team first collaborates on what the app needs to do and defines the API that a flow can access. Once the API is designed, this specification then becomes the <a href=\"https://swagger.io/blog/api-strategy/benefits-of-openapi-api-development/\">contract</a> between the two teams. As progress is made on each side, they iterate, perfect their design, and ultimately deliver the app. All the code used for this example is available on <a href=\"https://github.com/chrismarino/flowapp\">GitHub</a>, so that you can try it out for yourself.</p>\n\n<p><em>Note: Apex is a great way to customize Salesforce, but there are times when a standalone app might be the better way to go. If your team prefers Python, Node, or some other programming language, or perhaps you already have an app running on premises or in the cloud, and you want to run it all within the secure perimeter of the Salesforce Platform, a standalone Heroku app is the way to go.</em></p>\n<h2 class=\"anchored\">\n  <a name=\"api-spec-defines-the-interface\" href=\"#api-spec-defines-the-interface\">API spec defines the interface</a>\n</h2>\n\n<p>The example application an on-line shopping site that lets users login, browse products, and make a purchase. We’ll describe the process of building out this app in a number of posts, but for this first part we’ll simply build a flow and an external service that lists products and updates inventory in Salesforce. For the API, we’re using a sample API available on <a href=\"https://swagger.io/tools/swaggerhub/\">Swagger Hub</a>. There are a variety of tools and systems that can do this, including the MuleSoft <a href=\"https://www.mulesoft.com/platform/api-design\">Anypoint Platform API Designer</a>. For this example, however, we’re using this simple shopping cart spec to bootstrap the API design and provide the initial application stub for development. </p>\n\n<p>From the API spec, API Portals can produce server side application stubs to jumpstart application development. In this example, we’ve downloaded the node.js API stub as the starting point for API and app development. We’ve also modified the code so that it can run on Heroku by adding a <a href=\"https://devcenter.heroku.com/articles/procfile\">Procfile</a> and changing the port configuration. If you want to try it yourself, you can <a href=\"https://heroku.com/deploy?template=https://github.com/chrismarino/flowapp\">Deploy to Heroku</a>, or click the Deploy to Heroku button on the <a href=\"https://github.com/chrismarino/flowapp/blob/master/README.md\">GitHub</a> page. </p>\n\n<p>Let’s begin by looking at the initial <a href=\"http://swagger-shop.herokuapp.com/docs/#/Internal_calls\">API spec</a> for the application. These API docs are being served from a deployment of the app stub on Heroku. The actual YAML spec (which we will modify later in this post) is included in the <a href=\"https://github.com/chrismarino/flowapp/blob/master/api/swagger.yaml\">repo as well</a>.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1607639439-1%2AYhqGS0YRXgD3lAYVqyuhQQ.png\" alt=\"\"></p>\n\n<p>As you can see in the spec, there are definitions for each of the methods that specify which parameters are required and what the response payload will look like. Since this is a valid OpenAPI spec, we can register this API as an external service as described in <a href=\"https://trailhead.salesforce.com/en/content/learn/modules/external-services/get-started-with-external-services\">Get Started with External Services</a>.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1607639550-1%2A5NKtci6bYvqVp1DHex7VfQ.png\" alt=\"\"></p>\n<h2 class=\"anchored\">\n  <a name=\"external-service-authorization\" href=\"#external-service-authorization\">External service authorization</a>\n</h2>\n\n<p>The flow needs a Named Credential in Salesforce to access the external service. Salesforce offers many alternatives for how the app can use the Named Credential including per-user credentials that can help you track and control access. For this example, though, we’re going to use a single login for all flow access using basic HTTP authentication.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1607639589-1%2AU2DxWullRClXrHinOJ7_6g.png\" alt=\"\"></p>\n\n<p>You can find the code that implements this method is in the app included in the <a href=\"https://github.com/chrismarino/flowapp/blob/master/controllers/InternalCalls.js#L6\">repo</a>. </p>\n\n<p>App access to the organization is authorized via a Salesforce JWT account token and implemented in the app in <a href=\"https://github.com/chrismarino/flowapp/blob/master/service/SFAuthService.js\">SFAuthService.js</a>:</p>\n\n<pre><code class=\"lang-javascript\">'use strict';\n\nconst jwt = require('salesforce-jwt-bearer-token-flow');\nconst jsforce = require('jsforce');\n\nrequire('dotenv').config();\n\nconst { SF_CONSUMER_KEY, SF_USERNAME, SF_LOGIN_URL } = process.env;\n\nlet SF_PRIVATE_KEY = process.env.SF_PRIVATE_KEY;\nif (!SF_PRIVATE_KEY) {\n    SF_PRIVATE_KEY = require('fs').readFileSync('private.pem').toString('utf8');\n}\n\nexports.getSalesforceConnection = function () {\n    return new Promise(function (resolve, reject) {\n        jwt.getToken(\n            {\n                iss: SF_CONSUMER_KEY,\n                sub: SF_USERNAME,\n                aud: SF_LOGIN_URL,\n                privateKey: SF_PRIVATE_KEY\n            },\n            (err, tokenResponse) =&gt; {\n                if (tokenResponse) {\n                    let conn = new jsforce.Connection({\n                        instanceUrl: tokenResponse.instance_url,\n                        accessToken: tokenResponse.access_token\n                    });\n                    resolve(conn);\n                } else {\n                    reject('Authentication to Salesforce failed');\n                }\n            }\n        );\n    });\n};\n</code></pre>\n\n<p>The private key is configured in Heroku as a configuration variable and is installed when the app is deployed.</p>\n<h2 class=\"anchored\">\n  <a name=\"register-the-external-service-methods\" href=\"#register-the-external-service-methods\">Register the external service methods</a>\n</h2>\n\n<p>Individual methods for the ShoppingService external service are easily added to a flow just as they would be for any external service. Here we’ve added the Get Products and Get Order methods, as shown in Flow Builder below. But since Flow Builder can register an external service method using only the API spec, they are just references to the stub methods that we still need to build out. We’ll program something for them later in this post.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1607639692-1%2A5uLV0XtFHdqwubLxGn74UA.png\" alt=\"\"></p>\n\n<p>These are familiar steps to anyone that has registered an external service for a low, but if you want more detail on how to do this, check out the <a href=\"https://trailhead.salesforce.com/en/content/learn/modules/external-services/get-started-with-external-services\">Get Started with External Services Trailhead</a>.</p>\n<h2 class=\"anchored\">\n  <a name=\"define-the-api-and-build-out-the-methods\" href=\"#define-the-api-and-build-out-the-methods\">Define the API and build out the methods</a>\n</h2>\n\n<p>With the authorizations in place and the methods defined, we are now ready to build out the external service in a way that meets our company’s specific needs. For this, we need to implement each of the API methods. </p>\n\n<p>To illustrate this, here is the Node function that has been stubbed out by the API for the Get Order method. It is here that your business logic is implemented.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1607639765-1%2AM8e6Ge18JmmIMe0hHTUBKw.png\" alt=\"\"></p>\n\n<p>For each of the API methods, we’ve implemented some simple logic that we will use to test interactions with the flow. For example, here’s the code for getting a list of all orders:</p>\n\n<pre><code class=\"lang-javascript\">/**\n * Get list of all orders for the user\n *\n * type String json or xml\n * pOSTDATA List Creates a new employee in DB (optional)\n * returns List\n **/\nexports.typePost_orderPOST = function(type,pOSTDATA) {\n  return new Promise(function(resolve, reject) {\n    var examples = {};\n    examples['application/json'] = [ {\n  \"Item Total Price\" : 1998.0,\n  \"Order Item ID\" : 643,\n  \"Order ID\" : 298,\n  \"Total Order Price\" : 3996.0\n}, {\n  \"Item Total Price\" : 1998.0,\n  \"Order Item ID\" : 643,\n  \"Order ID\" : 298,\n  \"Total Order Price\" : 3996.0\n} ];\n    if (Object.keys(examples).length &gt; 0) {\n      resolve(examples[Object.keys(examples)[0]]);\n    } else {\n      resolve();\n    }\n  });\n}\n</code></pre>\n\n<p>You can examine the code that implements each of the methods in the repo:</p>\n\n<ul>\n<li>Get Products Method</li>\n<li>Post Order Method</li>\n<li>Get Order Method</li>\n<li>Get Orders Method</li>\n</ul>\n\n<p>Now that we have some simple logic executing in each of these methods, we can build a simple flow that logs in using the Named Credential, accesses the external service, and returns product data.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1607639841-1%2AnmANPe2Z75KWm1M76YLjCg.png\" alt=\"\"></p>\n\n<p>Running this flow shows the product data from the stub app. The successful display of product data here indicates that the flow has been able to successfully log in to the app, call the Get Product method, and get the proper response.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1607639862-1%2AMPICEeMp8XjIMsyNRkTL7Q.png\" alt=\"\"></p>\n<h2 class=\"anchored\">\n  <a name=\"update-the-api\" href=\"#update-the-api\">Update the API</a>\n</h2>\n\n<p>So now that we have our basic flow defined and accessing the app, we can complete the API with new methods necessary for the app to do what it needs to do. </p>\n\n<p>Let’s imagine that the app has up-to-date product inventory data and we want to use that data to update the Product object in Salesforce with the current quantity in stock. For this, the app would need to be able to access Salesforce and update the Product object. </p>\n\n<p>To do this, the flow needs to make a request to a Get Inventory method. But that method does not yet exist. However, we can modify the API to include any new methods we need. Here our teams work together to determine what the flow needs and what methods are necessary in the app. </p>\n\n<p>After discussion, we determine that a single Get Inventory method will satisfy the requirements. So, now we update the API spec to include a new <a href=\"https://github.com/chrismarino/flowapp/blob/ExternalService/api/swagger.yaml#L174-L205\">method</a>:</p>\n\n<pre><code class=\"lang-yaml\">/{type}/get_inventory/:\n    get:\n      tags:\n      - \"Internal calls\"\n      description: \"Get Inventory\"\n      operationId: \"typeGet_inventoryGET\"\n      consumes:\n      - \"application/json\"\n      produces:\n      - \"application/json\"\n      - \"application/xml\"\n      parameters:\n      - name: \"type\"\n        in: \"path\"\n        description: \"json or xml\"\n        required: true\n        type: \"string\"\n      - name: \"product_id\"\n        in: \"query\"\n        description: \"Product Id\"\n        required: true\n        type: \"integer\"\n      responses:\n        \"200\":\n          description: \"OK\"\n          schema:\n            type: \"array\"\n            items:\n              $ref: \"#/definitions/Inventory\"\n      security:\n      - basic: []\n</code></pre>\n\n<p>With this updated API, we can update the external service so that we can use it in a flow.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1607639924-1%2A8I3LMY2IrvvZXItY219arg.png\" alt=\"\"></p>\n\n<p>And with the updated API spec, we can automatically generate a stub method as well. From the empty stub method we can <a href=\"https://github.com/chrismarino/flowapp/blob/ExternalService/service/InternalCallsService.js#L4-L25\">complete the function</a> with the necessary logic to access Salesforce directly and update the Product object. Note that it uses the SFAuthService.js code from above and an API token to access the organization data.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1607639965-1%2AVALT9oPUv4gQEocwMuxhgA.png\" alt=\"\"></p>\n<h2 class=\"anchored\">\n  <a name=\"platform-events-and-eda\" href=\"#platform-events-and-eda\">Platform events and EDA</a>\n</h2>\n\n<p>Now that this inventory method is available, we can check the operation with a simple flow that triggers on a Platform Event and updates the Product object. When we run this test flow, it updates the iPhone Product object in the organization.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1607639993-1%2A9w82sZ_SIfNW1Ve_S-A0TA.png\" alt=\"\"></p>\n\n<p>How and when the flow might need to update the product inventory would be up to the actual business needs. However, triggering the flow to update the object can be done using Platform Events. The flow can respond to any Platform Event with a call to the Get Inventory method.</p>\n<h2 class=\"anchored\">\n  <a name=\"deploy-the-business-logic\" href=\"#deploy-the-business-logic\">Deploy the business logic</a>\n</h2>\n\n<p>The process described in this post can go on until the flow and app API converge on a stable design. Once stable, our programmer and low-code builder can complete their work independently to complete the app. The flow designer can build in all the decision logic that surrounds the flow and build out screens for the user to interact with.</p>\n\n<p>Separately, and independently from the flow designer, the programmer can code each of the methods in the stub app to implement the business logic.</p>\n<h2 class=\"anchored\">\n  <a name=\"summary\" href=\"#summary\">Summary</a>\n</h2>\n\n<p>We’ve just started building out this app and running it as an external service. In this post, however, you’ve already seen the the basic steps that would be part of every development cycle: defining an API, registering methods that a flow can call, building out the stub app, and authorizing access for the flow, app, and Platform Event triggers. </p>\n\n<p>Future posts in this series will take these basic elements and methodology to expand the flow to execute the business logic contained in the app via user interface elements for a complete process automation solution running entirely on the Salesforce Platform.</p>\n\n<p>To learn more, see the <a href=\"https://trailhead.salesforce.com/en/content/learn/modules/heroku_enterprise_baiscs?trail_id=heroku_enterprise\">Heroku Enterprise Basics Trailhead module</a> and the <a href=\"https://trailhead.salesforce.com/en/content/learn/modules/flow-basics\">Flow Basics Trailhead module</a>. Please share your feedback with <a href=\"https://twitter.com/SalesforceArchs\">@SalesforceArchs</a> on Twitter.</p>","PublishedAt":"2020-12-11 16:30:00+00:00","OriginURL":"https://blog.heroku.com/extend-flows-heroku-event-driven","SourceName":"Heroku"}},{"node":{"ID":622,"Title":"How To Get Fooled By Metrics","Description":"Metrics are one of the main building blocks in the topic of observability and we use them heavily. This story is about an incident where we tried to find and resolve a problem that we saw in these metrics. We went down a rabbit hole of potential fixes, only to discover that the metrics were correct all along.","PublishedAt":"2020-12-04 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2020-12-04-howtogetfooledbymetrics/","SourceName":"Trivago"}},{"node":{"ID":259,"Title":"Part 1: Computer Vision @ GIPHY: How we Created an AutoTagging Model Using Deep Learning","Description":"Motivation GIPHY search is powered by ElasticSearch, where GIFs are represented as documents with various features, such as tags and captions. Tags in particular have a great impact on the search performance. Therefore, at GIPHY Engineering, we continuously track and improve their quality with both manual and automated verification. However, for the last couple of [&#8230;]","PublishedAt":"2020-12-03 19:42:02+00:00","OriginURL":"https://engineering.giphy.com/computer-vision-giphy-how-we-created-an-autotagging-model-using-deep-learning/","SourceName":"GIPHY"}},{"node":{"ID":426,"Title":"Indeed + Hacktoberfest 2020: By The Numbers","Description":"<p>Indeed + Hacktoberfest 2020 is in the books! We’re thrilled to share our results. External focus Internal focus As a Hacktoberfest Community Partner, we engaged directly with the external community. 1 external landing page 1 case study 6 supported open source projects tagged with the ‘hacktoberfest’ label 11 virtual office hours 437 commits into our [&#8230;]</p>\n<p> </p>\n","PublishedAt":"2020-12-03 18:20:44+00:00","OriginURL":"https://engineering.indeedblog.com/blog/2020/12/hacktoberfest-2020-by-the-numbers/","SourceName":"Indeed"}},{"node":{"ID":798,"Title":"Leveraging a Manager Weekly Newsletter for Team Communication","Description":"I started my journey as an Engineering Manager at SoundCloud close to a year ago. This came after working as a Software Engineer for more…","PublishedAt":"2020-11-25 00:00:00+00:00","OriginURL":"https://developers.soundcloud.com/blog/manager-weekly-newsletter-for-team-communication","SourceName":"Soundcloud"}},{"node":{"ID":623,"Title":"Exploring the Page Visibility API for Detecting Page Background State","Description":"","PublishedAt":"2020-11-17 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2020-11-17-exploringthepagevisibilityapifordetectin/","SourceName":"Trivago"}},{"node":{"ID":427,"Title":"k8dash: Indeed’s Open Source Kubernetes Dashboard","Description":"<p>So you’ve got your first Kubernetes (also known as k8s) cluster up and running. Congratulations! Now, how do you operate the thing? Deployments, replica sets, stateful sets, pods, ingress, oh my! Getting Kubernetes running can feel like a big enough challenge in and of itself, but does day two of operations need to be just [&#8230;]</p>\n<p> </p>\n","PublishedAt":"2020-11-05 17:13:25+00:00","OriginURL":"https://engineering.indeedblog.com/blog/2020/11/k8dash-indeeds-open-source-kubernetes-dashboard/","SourceName":"Indeed"}},{"node":{"ID":260,"Title":"GIPHY SDK Debuts its Latest Service, GIPHY Text, with Select Launch Partners","Description":"GIPHY users have relied on us to help them better express themselves since day one. First, with GIFs, and later followed by Stickers, animated emoji &#38; text, Video, etc. We know, based on the frequent use of captions on GIFs, that sometimes text is important to contextualize visual media. This led the way to the [&#8230;]","PublishedAt":"2020-10-28 07:00:00+00:00","OriginURL":"https://engineering.giphy.com/giphy-sdk-debuts-its-latest-service-giphy-text-with-select-launch-partners/","SourceName":"GIPHY"}},{"node":{"ID":490,"Title":"The PayPay Mini App Platform: Build and deploy your service easily into PayPay","Description":"<p>One of the biggest challenges developers face is getting their app discovered </p>","PublishedAt":"2020-10-26 01:14:40+00:00","OriginURL":"https://blog.paypay.ne.jp/en/paypay-miniapp-platform/","SourceName":"Paypay"}},{"node":{"ID":624,"Title":"Deep Dive Into Data Science at trivago","Description":"","PublishedAt":"2020-10-22 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2020-10-22-deepdiveintodatascienceattrivago/","SourceName":"Trivago"}},{"node":{"ID":491,"Title":"What are the different levels of automation testing? : Web Testing","Description":"<p>This is Part 5 of our automation series. Read Part 1 to </p>","PublishedAt":"2020-10-21 01:51:36+00:00","OriginURL":"https://blog.paypay.ne.jp/en/what-are-the-different-levels-of-web-automation-testing/","SourceName":"Paypay"}},{"node":{"ID":428,"Title":"Jackson: A Growing User Base Presents New Challenges","Description":"<p>Jackson is a mature and feature-rich open source project that we use, support, and contribute to here at Indeed. In my previous post, I introduced Jackson’s core competency as a JSON library for Java. I went on to describe the additional data formats, data types, and JVM languages Jackson supports. In this post, I will [&#8230;]</p>\n<p> </p>\n","PublishedAt":"2020-10-19 13:00:02+00:00","OriginURL":"https://engineering.indeedblog.com/blog/2020/10/jackson-a-growing-user-base-presents-new-challenges/","SourceName":"Indeed"}},{"node":{"ID":799,"Title":"Testing SQL for BigQuery","Description":"“To me, legacy code is simply code without tests.” — Michael Feathers If untested code is legacy code, why aren’t we testing data pipelines or ETLs (extract, transform, load)? In particular, data pipelines built in SQL are rarely tested. However, as software engineers, we know all our code should be tested. So in this post, I’ll describe how we started testing SQL data pipelines at SoundCloud.","PublishedAt":"2020-10-16 00:00:00+00:00","OriginURL":"https://developers.soundcloud.com/blog/testing-sql-for-bigquery","SourceName":"Soundcloud"}},{"node":{"ID":719,"Title":"New “Fully Customizable Video” Zoom SDK Announced","Description":"","PublishedAt":"2020-10-14 17:03:04+00:00","OriginURL":"https://medium.com/zoom-developer-blog/new-fully-customizable-video-zoom-sdk-announced-fb9c05ce0d6a?source=rss----4a85731adaff---4","SourceName":"Zoom"}},{"node":{"ID":353,"Title":"Incident Response at Heroku","Description":"<p><em>This post is an update on a <a href=\"https://blog.heroku.com/incident-response-at-heroku\">previous post</a> about how Heroku handles incident response.</em></p>\n\n<p>As a service provider, when things go wrong, you try to get them fixed as quickly as possible. In addition to technical troubleshooting, there’s a lot of coordination and communication that needs to happen in resolving issues with systems like Heroku’s.</p>\n\n<p>At Heroku we’ve codified our practices around these aspects into an incident response framework. Whether you’re just interested in how incident response works at Heroku, or looking to adopt and apply some of these practices for yourself, we hope you find this inside look helpful.</p>\n<h2 class=\"anchored\">\n  <a name=\"incident-response-and-the-incident-commander-role\" href=\"#incident-response-and-the-incident-commander-role\">Incident Response and the Incident Commander Role</a>\n</h2>\n\n<p>We describe Heroku’s incident response framework below. It’s based on the <a href=\"https://en.wikipedia.org/wiki/Incident_Command_System\">Incident Command System</a> used in natural disaster response and other emergency response fields. Our response framework and the Incident Commander role in particular help us to successfully respond to a variety of incidents.</p>\n\n<p>When an incident occurs, we follow these steps:</p>\n<h3 class=\"anchored\">\n  <a name=\"page-an-incident-commander\" href=\"#page-an-incident-commander\">Page an Incident Commander</a>\n</h3>\n\n<p>They will assess the issue, and decide if it’s worth investigating further</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1601548057-Slack%20Page.png\" alt=\"Slack Page\"></p>\n<h3 class=\"anchored\">\n  <a name=\"move-to-a-dedicated-chat-room\" href=\"#move-to-a-dedicated-chat-room\">Move to a dedicated chat room</a>\n</h3>\n\n<p>The Incident Commander creates a new room in Slack, to centralize all the information for this specific incident</p>\n<h3 class=\"anchored\">\n  <a name=\"update-public-status-site\" href=\"#update-public-status-site\">Update public status site</a>\n</h3>\n\n<p>Our customers want information about incidents as quickly as possible, even if it is preliminary. As soon as possible, the IC designates someone to take on the communications role (“comms”) with a first responsibility of updating the <a href=\"https://status.heroku.com/\">status site</a> with our current understanding of the incident and how it’s affecting customers. The admin section of Heroku’s status site helps the comms operator to get this update out quickly:</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1601548128-public-post.png\" alt=\"public-post\"></p>\n\n<p>The status update then appears on <a href=\"https://status.heroku.com/\">status.heroku.com</a> and is sent to customers and internal communication channels via SMS, email, and Slack bot. It also shows on twitter:</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1601548203-twitter-post.png\" alt=\"twitter-post\"></p>\n<h3 class=\"anchored\">\n  <a name=\"send-out-internal-situation-report\" href=\"#send-out-internal-situation-report\">Send out internal Situation Report</a>\n</h3>\n\n<p>Next the IC compiles and sends out the first situation report (“sitrep”) to the internal team describing the incident. It includes what we know about the problem, who is working on it and in what roles, and open issues. As the incident evolves, the sitrep acts as a concise description of the current state of the incident and our response to it. A good sitrep provides information to active incident responders, helps new responders get quickly up to date about the situation, and gives context to other observers like customer support staff.</p>\n\n<p>The Heroku status site has a form for the sitrep, so that the IC can update it and the public-facing status details at the same time. When a sitrep is created or updated, it’s automatically distributed internally via email and Slack bot. A versioned log of sitreps is also maintained for later review:</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1601548277-sitrep2.png\" alt=\"sitrep2\">\n<img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1601548298-Sitrep-emails.png\" alt=\"Sitrep-emails\"></p>\n<h3 class=\"anchored\">\n  <a name=\"assess-problem\" href=\"#assess-problem\">Assess problem</a>\n</h3>\n\n<p>The next step is to assess the problem in more detail. The goals here are to gain better information for the public status communication (e.g. what users are affected and how, what they can do to work around the problem) and more detail that will help engineers fix the problem (e.g. what internal components are affected, the underlying technical cause). The IC collects this information and reflects it in the sitrep so that everyone involved can see it. It includes the severity, going from SEV0 (critical disruption), to SEV4 (minor feature impacted)</p>\n<h3 class=\"anchored\">\n  <a name=\"mitigate-problem\" href=\"#mitigate-problem\">Mitigate problem</a>\n</h3>\n\n<p>Once the response team has some sense of the problem, it will try to mitigate customer-facing effects if possible. For example, we may put the Platform API in maintenance mode to reduce load on infrastructure systems, or boot additional instances in our fleet to temporarily compensate for capacity issues. A successful mitigation will reduce the impact of the incident on customer apps and actions, or at least prevent the customer-facing issues from getting worse.</p>\n<h3 class=\"anchored\">\n  <a name=\"coordinate-response\" href=\"#coordinate-response\">Coordinate response</a>\n</h3>\n\n<p>In coordinating the response, the IC focuses on bringing in the right people to solve the problem and making sure that they have the information they need. The IC can use a Slack bot to page in additional teams as needed (the page will route to the on-call person for that team), or page teams directly.</p>\n<h3 class=\"anchored\">\n  <a name=\"manage-ongoing-response\" href=\"#manage-ongoing-response\">Manage ongoing response</a>\n</h3>\n\n<p>As the response evolves, the IC acts as an information radiator to keep the team informed about what’s going on. The IC will keep track of who’s active on the response, what problems have been solved and are still open, the current resolution methods being attempted, when we last communicated with customers, and reflect this back to the team regularly with the sitrep mechanism. Finally, the IC is making sure that nothing falls through the cracks: that no problems go unaddressed and that decisions are made in a timely manner.</p>\n<h3 class=\"anchored\">\n  <a name=\"post-incident-cleanup\" href=\"#post-incident-cleanup\">Post-incident cleanup</a>\n</h3>\n\n<p>Once the immediate incident has been resolved, the IC calls for the team to unwind any temporary changes made during the response. For example, alerts may have been silenced and need to be turned back on. The team double-checks that all monitors are green and that all incidents in PagerDuty have been resolved.</p>\n<h3 class=\"anchored\">\n  <a name=\"post-incident-follow-up\" href=\"#post-incident-follow-up\">Post-incident follow-up</a>\n</h3>\n\n<p>Finally, the Production Engineering Department will tee up a post-incident follow up. Depending on the severity of the incident, this could be a quick discussion in the normal weekly operational review or a dedicated internal post-mortem with associated public post-mortem post. The post-mortem process often informs changes that we should make to our infrastructure, testing, and process; these are tracked over time within engineering as incident remediation items.</p>\n<h2 class=\"anchored\">\n  <a name=\"when-everything-goes-south\" href=\"#when-everything-goes-south\">When everything goes south</a>\n</h2>\n\n<p>As Heroku is part of the Salesforce Platform, we leverage Salesforce Incident Response, and Crisis communication center when things gets really bad.</p>\n\n<p>If the severity decided by the IC is SEV1 or worse, Salesforce’s Critical Incident Center (CIC) gets involved. Their role is to assist the Heroku Incident Commander with support around customer communication, and keep the executives informed of the situation. They also can engage the legal teams if needed, mostly for customer communication.</p>\n\n<p>In the case where the incident is believed to be a SEV0 ( major disruption for example ), the Heroku Incident Commander can also request assistance from the Universal Command (UC) Leadership. They will help to assess the issue, and determine if the incident really rises to the level of Sev 0. </p>\n\n<p>Once it is determined to be the case, the UC will spin up a conference call ( called  bridge ) involving executives, in order for them to have a single source of truth to follow-up on the incident’s evolution. One of the goals is that executives don’t first learn failures from outsides sources. This may seem obvious, but amidst the stress of a significant incident when we're solely focused on fixing a problem impacting customers, it's easy to overlook communicating status to those not directly involved with solving the problem. They are also much better suited to answer to customers requests, and keep them informed of the incident response.</p>\n<h2 class=\"anchored\">\n  <a name=\"incident-response-in-other-fields\" href=\"#incident-response-in-other-fields\">Incident Response in Other Fields</a>\n</h2>\n\n<p>The incident response framework described above draws from decades of related work in emergency response: natural disaster response, firefighting, aviation, and other fields that need to manage response to critical incidents. We try to learn from this body of work where possible to avoid inventing our incident response policy from first principles.\nTwo areas of previous work particularly influenced how we approach incident response:</p>\n<h3 class=\"anchored\">\n  <a name=\"incident-command-system\" href=\"#incident-command-system\">Incident Command System</a>\n</h3>\n\n<p>Our framework draws most directly from the <a href=\"http://en.wikipedia.org/wiki/Incident_Command_System\">Incident Command System</a> used to manage natural disaster and other large-scale incident responses. This prior art informs our Incident Commander role and our explicit focus on facilitating incident response in addition to directly addressing the technical issues.</p>\n<h3 class=\"anchored\">\n  <a name=\"crew-resource-management\" href=\"#crew-resource-management\">Crew Resource Management</a>\n</h3>\n\n<p>The ideas of <a href=\"http://en.wikipedia.org/wiki/Crew_resource_management\">Crew Resource Management</a> (a different “CRM”) originated in aviation but have since been successfully applied to other fields such as medicine and firefighting. We draw lessons on communication, leadership, and decision-making from CRM into our incident response thinking.\nWe believe that learning from fields outside of software engineering is a valuable practice, both for operations and other aspects of our business.</p>\n<h2 class=\"anchored\">\n  <a name=\"summary\" href=\"#summary\">Summary</a>\n</h2>\n\n<p>Heroku’s incident response framework helps us quickly resolve issues while keeping customers informed about what’s happening. We hope you’ve found these details about our incident response framework interesting and that they may even inspire changes in how you think about incident response at your own company.\nAt Heroku we’re continuing to learn from our own experiences and the work of others in related fields. Over time this will mean even better incident response for our platform and better experiences for our customers.</p>","PublishedAt":"2020-10-08 12:53:43+00:00","OriginURL":"https://blog.heroku.com/incident-response-at-heroku-2020","SourceName":"Heroku"}},{"node":{"ID":354,"Title":"How I Broke `git push heroku main`","Description":"<p>Incidents are inevitable. Any platform, large or small will have them. While resiliency work will definitely be an important factor in reducing the number of incidents, hoping to remove all of them (and therefore reach 100% uptime) is not an achievable goal.</p>\n\n<p>We should, however, learn as much as we can from incidents, so we can avoid repeating them.</p>\n\n<p>In this post, we will look at one of those incidents, <a href=\"https://status.heroku.com/incidents/2105\">#2105</a>, see how it happened (spoiler: I messed up), and what we’re doing to avoid it from happening again (spoiler: I’m not fired).</p>\n\n<!-- more -->\n<h2 class=\"anchored\">\n  <a name=\"git-push-inception\" href=\"#git-push-inception\">Git push inception</a>\n</h2>\n\n<p>Our Git server is a component written in Go which can listen for HTTP and SSH connections to process a Git command.\nWhile we try to run all our components as Heroku apps on our platform just like Heroku customers, this component is different, as it has several constraints which make it unsuitable for running on the Heroku platform. Indeed, Heroku currently only provides HTTP routing, so it can’t handle incoming SSH connections.</p>\n\n<p>This component is therefore hosted as a “kernel app” using an internal framework which mimics the behavior of Heroku, but runs directly on virtual servers.</p>\n\n<p>Whenever we deploy new code for this component, we will mark instances running the previous version of the code as poisoned. They won’t be able to receive new requests but will have the time they need to finish processing any ongoing requests (every Git push is one request, and those can take up to one hour).\nOnce they don’t have any active requests open, the process will stop and restart using the new code.</p>\n\n<p>When all selected instances have been deployed to, we can move to another batch, and repeat until all instances are running the new code.</p>\n<h2 class=\"anchored\">\n  <a name=\"it-was-such-a-nice-morning\" href=\"#it-was-such-a-nice-morning\">It was such a nice morning</a>\n</h2>\n\n<p>On September 3, I had to deploy a change to switch from calling one internal API endpoint to another. It included a new authentication method between components.</p>\n\n<p>This deploy was unusual because it required setting a new configuration variable, which includes the following manual actions:</p>\n\n<ol>\n<li>Set the new config variable with the framework handling our instances</li>\n<li>Run a command to have the new config variable transmitted to every instance</li>\n<li>Trigger the deploy so the config variables starts being used</li>\n</ol>\n\n<p>So, on that morning, I started deploying our staging instance. I set the new configuration variable on both staging and production.\nThen, I had the config variables transmitted to every instance, but only in staging as I figured I’d avoid touching production right now.\nFinally, I kicked off the staging deployment, and started monitoring that everything went smoothly, which it did.</p>\n\n<p>A few hours later, I went on to production.</p>\n<h2 class=\"anchored\">\n  <a name=\"houston-we-have-a-problem\" href=\"#houston-we-have-a-problem\">Houston, we have a problem</a>\n</h2>\n\n<p>I started my production deployment. Since I had set the configuration variable earlier, I went straight to deploying the new code.</p>\n\n<p>You may see what I did wrong now.</p>\n\n<p>So my code change went to a batch of instances. I didn’t move to another batch though, as I was about to go to lunch. There was no rush to move forward right away, especially since deploying every instance can take several hours.</p>\n\n<p>So I went to lunch, but came back a few minutes later as an alert had gone off.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1601509354-Screenshot%202020-09-09%20at%2010.24.13.png\" alt=\"Screenshot 2020-09-09 at 10\"></p>\n\n<p>The spike you can see on this graph is HTTP 401 responses.</p>\n\n<p>If you read carefully the previous section, you may have noticed that I set the new configuration variable in production, but didn’t apply it to the instances.\nSo my deploy to a batch of servers didn’t have the new configuration variable, meaning we were making unauthenticated calls to a private API, which gave us 401 responses. Hence the 401s being sent back publicly.</p>\n\n<p>Once I realized that, I ran the script to transmit the configuration variables to the instances, killed the impacted processes, which restarted using the updated configuration variables, and the problem was resolved.</p>\n<h2 class=\"anchored\">\n  <a name=\"did-i-mess-up\" href=\"#did-i-mess-up\">Did I mess up?</a>\n</h2>\n\n<p>An untrained eye could say “wow, you messed up bad. Why didn’t you run that command?”, and they would be right. Except they actually wouldn’t.</p>\n\n<p>The problem isn’t that I forgot to run one command. It’s that the system has allowed me to go forward with the deployment when it could have helped me avoid the issue.</p>\n\n<p>Before figuring out any solution, the real fix is to do a truly blameless retrospective. If we had been blaming me for forgetting to run a command instead of focusing on why the system still permitted the deployment, I would probably have felt unsafe reporting this issue, and we would not have been able to improve our systems so that this doesn’t happen again.</p>\n\n<p>Then we can focus on solutions. In this specific case, we are going to merge the two steps of updating configuration variables and deploying code into a single step.\nThat way there isn’t an additional step to remember to run from time to time.</p>\n\n<p>If we didn’t want or were unable to merge the two steps, we could also have added a safeguard in the form of a confirmation warning if we’re trying to deploy the application’s code while configuration variables aren’t synchronized.</p>\n<h2 class=\"anchored\">\n  <a name=\"computers-are-dumb-but-they-don-t-make-mistakes\" href=\"#computers-are-dumb-but-they-don-t-make-mistakes\">Computers are dumb, but they don’t make mistakes</a>\n</h2>\n\n<p>Relying on humans to perform multiple manual actions,  especially when some of them are only required rarely (we don’t change configuration variables often) is a recipe for incidents.</p>\n\n<p>Our job as engineers is to  build systems that avoid those human flaws, so we can do our human job of thinking about new things, and computers can do theirs: performing laborious and repetitive tasks.</p>\n\n<p>This incident shows how a blameless culture benefits everyone in a company (and customers!). Yes, I messed up. But the fix is to improve the process, not to assign blame. We can’t expect folks to be robots who never make mistakes. Instead, we need to build a system that’s safe enough so those mistakes can’t happen.</p>","PublishedAt":"2020-10-01 15:30:00+00:00","OriginURL":"https://blog.heroku.com/how-i-broke-git-push-heroku-main","SourceName":"Heroku"}}]}},"pageContext":{"limit":30,"skip":5250,"numPages":193,"currentPage":176}},"staticQueryHashes":["3649515864"]}