{"componentChunkName":"component---src-templates-blog-list-tsx","path":"/page/7","result":{"data":{"allPost":{"edges":[{"node":{"ID":5591,"Title":"Boundary 0.15 adds new storage policies and desktop/CLI features","Description":"HashiCorp Boundary 0.15 improves governance and end user workflows with session recording storage policies and UX upgrades such as target search and filtering.","PublishedAt":"2024-02-05 17:30:00+00:00","OriginURL":"https://www.hashicorp.com/blog/boundary-0-15-adds-new-storage-policies-and-desktop-cli-features","SourceName":"HashiCorp"}},{"node":{"ID":5589,"Title":"Universal Profiling: Detecting CO2 and energy efficiency","Description":"","PublishedAt":"2024-02-05 16:54:36+00:00","OriginURL":"https://www.elastic.co/blog/universal-profiling-detecting-co2-energy-efficiency","SourceName":"Elastic"}},{"node":{"ID":5587,"Title":"AWS named as a Leader in 2023 Gartner Magic Quadrant for Strategic Cloud Platform Services for thirteenth year in a row","Description":"On December 4, 2023, AWS was named as a Leader in the 2023 Magic Quadrant for Strategic Cloud Platform Services (SCPS). AWS is the longest-running Magic Quadrant Leader, with Gartner naming AWS a Leader for the thirteenth consecutive year. AWS is placed highest on the Ability to Execute axis. SCPS, previously known as Magic Quadrant […]","PublishedAt":"2024-02-05 16:41:10+00:00","OriginURL":"https://aws.amazon.com/blogs/aws/read-the-2023-gartner-magic-quadrant-for-strategic-cloud-platform-services/","SourceName":"AWS"}},{"node":{"ID":5592,"Title":"Debugging & Optimizing Elasticsearch Top Hits Aggregation","Description":"","PublishedAt":"2024-02-05 12:48:28+00:00","OriginURL":"https://medium.com/engineering-housing/debugging-optimizing-elasticsearch-top-hits-aggregation-6d5cb13b9687?source=rss----3a69e32e2594---4","SourceName":"Housing.com"}},{"node":{"ID":5588,"Title":"Choosing an LLM: The 2024 getting started guide to open-source LLMs","Description":"","PublishedAt":"2024-02-05 00:00:00+00:00","OriginURL":"https://www.elastic.co/blog/open-source-llms-guide","SourceName":"Elastic"}},{"node":{"ID":5586,"Title":"Cloudera Named Strong Performer in New Forrester Wave for Streaming Platforms","Description":"<p>Cloudera ranked among highest for strength of current offering</p>\n<p>The post <a href=\"https://blog.cloudera.com/cloudera-named-strong-performer-in-new-forrester-wave-for-streaming-platforms/\">Cloudera Named Strong Performer in New Forrester Wave for Streaming Platforms</a> appeared first on <a href=\"https://blog.cloudera.com\">Cloudera Blog</a>.</p>\n","PublishedAt":"2024-02-02 19:15:10+00:00","OriginURL":"https://blog.cloudera.com/cloudera-named-strong-performer-in-new-forrester-wave-for-streaming-platforms/","SourceName":"Cloudera"}},{"node":{"ID":5585,"Title":"Visually replay user-facing issues with Zendesk and Datadog Session Replay","Description":"<img class=\"webfeedsFeaturedVisual rss\" src=\"https://imgix.datadoghq.com/img/blog/zendesk-session-replay-integration/zendesk-integration.png\" width=\"100%\"/>Zendesk provides support teams with an integrated solution for processing all types of customer inquiries and feedback. But as organizations scale, support tickets can multiply, making it difficult to parse customer feedback and investigate issues promptly and thoroughly. Customers often report problems without providing the detailed context needed for effective troubleshooting. Support engineers work painstakingly to piece together an accurate picture of user experience, following up repeatedly with individual customers for additional information and aids such as screenshots in order to reproduce bugs.","PublishedAt":"2024-02-02 00:00:00+00:00","OriginURL":"https://www.datadoghq.com/blog/zendesk-session-replay-integration/","SourceName":"Datadog"}},{"node":{"ID":5582,"Title":"Thanksgiving 2023 security incident","Description":" On Thanksgiving Day, November 23, 2023, Cloudflare detected a threat actor on our self-hosted Atlassian server. Our security team immediately began an investigation, cut off the threat actor’s access, and no Cloudflare customer data or systems were impacted by this event ","PublishedAt":"2024-02-01 20:00:24+00:00","OriginURL":"https://blog.cloudflare.com/thanksgiving-2023-security-incident","SourceName":"Cloudflare"}},{"node":{"ID":5583,"Title":"Mastering Day 2 Operations with Cloudera","Description":"<p>How Does Cloudera Support Day 2 Operations?</p>\n<p>The post <a href=\"https://blog.cloudera.com/mastering-day-2-operations-with-cloudera/\">Mastering Day 2 Operations with Cloudera</a> appeared first on <a href=\"https://blog.cloudera.com\">Cloudera Blog</a>.</p>\n","PublishedAt":"2024-02-01 18:51:10+00:00","OriginURL":"https://blog.cloudera.com/mastering-day-2-operations-with-cloudera/","SourceName":"Cloudera"}},{"node":{"ID":5580,"Title":"Migrating Our iOS Build System from Buck to Bazel","Description":"","PublishedAt":"2024-02-01 17:20:20+00:00","OriginURL":"https://medium.com/airbnb-engineering/migrating-our-ios-build-system-from-buck-to-bazel-ddd6f3f25aa3?source=rss----53c7c27702d5---4","SourceName":"Airbnb"}},{"node":{"ID":5581,"Title":"How to deploy and manage Elastic on Microsoft Azure","Description":"","PublishedAt":"2024-02-01 08:00:00+00:00","OriginURL":"https://www.elastic.co/blog/getting-started-with-the-azure-integration-enhancement","SourceName":"Elastic"}},{"node":{"ID":5579,"Title":"Introducing the Elastic Trust Center!","Description":"","PublishedAt":"2024-02-01 00:00:00+00:00","OriginURL":"https://www.elastic.co/blog/introducing-the-elastic-trust-center","SourceName":"Elastic"}},{"node":{"ID":5584,"Title":"Streams Replication Manager Prefixless Replication","Description":"<p>Part 1</p>\n<p>The post <a href=\"https://blog.cloudera.com/streams-replication-manager-prefixless-replication-part-1/\">Streams Replication Manager Prefixless Replication</a> appeared first on <a href=\"https://blog.cloudera.com\">Cloudera Blog</a>.</p>\n","PublishedAt":"2024-01-31 17:02:51+00:00","OriginURL":"https://blog.cloudera.com/streams-replication-manager-prefixless-replication-part-1/","SourceName":"Cloudera"}},{"node":{"ID":5578,"Title":"LangChain Support for Workers AI, Vectorize and D1","Description":" During Developer Week, we announced LangChain support for Cloudflare Workers. Since then, we’ve been working with the LangChain team on deeper integration of many tools across Cloudflare’s developer platform and are excited to share what we’ve been up to ","PublishedAt":"2024-01-31 14:00:12+00:00","OriginURL":"https://blog.cloudflare.com/langchain-support-for-workers-ai-vectorize-and-d1","SourceName":"Cloudflare"}},{"node":{"ID":5742,"Title":"Unlocking Efficiency: How Ava Became Our AI Productivity Partner","Description":"","PublishedAt":"2024-01-31 00:24:57+00:00","OriginURL":"https://tech.instacart.com/unlocking-efficiency-how-ava-became-our-ai-productivity-partner-f1a560686361?source=rss----587883b5d2ee---4","SourceName":"Instacart"}},{"node":{"ID":5577,"Title":"Why did Steve Mayzak return to Elastic? The people (and the tech)","Description":"","PublishedAt":"2024-01-31 00:00:00+00:00","OriginURL":"https://www.elastic.co/blog/culture-steve-mayzak-return-to-elastic","SourceName":"Elastic"}},{"node":{"ID":5575,"Title":"Achieving Trusted AI in Manufacturing","Description":"<p>In the dynamic landscape of modern manufacturing, AI has emerged as a transformative differentiator, reshaping the industry for those seeking the competitive advantages of gained efficiency and innovation. As we navigate the fourth and fifth industrial revolution, AI technologies are catalyzing a paradigm shift in how products are designed, produced, and optimized.  With the ability [&#8230;]</p>\n<p>The post <a href=\"https://blog.cloudera.com/achieving-trusted-ai-in-manufacturing/\">Achieving Trusted AI in Manufacturing</a> appeared first on <a href=\"https://blog.cloudera.com\">Cloudera Blog</a>.</p>\n","PublishedAt":"2024-01-30 17:54:17+00:00","OriginURL":"https://blog.cloudera.com/achieving-trusted-ai-in-manufacturing/","SourceName":"Cloudera"}},{"node":{"ID":5574,"Title":"New chat experience for AWS Glue using natural language – Amazon Q data integration in AWS Glue (Preview)","Description":"Today we’re previewing a new chat experience for AWS Glue that will let you use natural language to author and troubleshoot data integration jobs. Amazon Q data integration in AWS Glue will reduce the time and effort you need to learn, build, and run data integration jobs using AWS Glue data integration engines. You can […]","PublishedAt":"2024-01-30 17:05:11+00:00","OriginURL":"https://aws.amazon.com/blogs/aws/new-chat-experience-for-aws-glue-using-natural-language-amazon-q-data-integration-in-aws-glue-preview/","SourceName":"AWS"}},{"node":{"ID":5576,"Title":"Build secure AI applications on Azure with HashiCorp Terraform and Vault","Description":"Learn how to deploy, secure, and enable AI-based applications with Terraform and Vault.","PublishedAt":"2024-01-30 17:00:00+00:00","OriginURL":"https://www.hashicorp.com/blog/build-secure-ai-applications-on-azure-with-hashicorp-terraform-and-vault","SourceName":"HashiCorp"}},{"node":{"ID":5572,"Title":"Working with ChatGPT Functions on Heroku","Description":"<h2 class=\"anchored\">\n  <a name=\"how-to-build-and-deploy-a-node-js-app-that-uses-openai-s-apis\" href=\"#how-to-build-and-deploy-a-node-js-app-that-uses-openai-s-apis\">How to Build and Deploy a Node.js App That Uses OpenAI’s APIs</a>\n</h2>\n\n<p>Near the end of 2023, ChatGPT <a href=\"https://www.linkedin.com/news/story/chatgpt-hits-100m-weekly-users-5808204/\">announced</a> that it had 100M weekly users. That’s a massive base of users who want to take advantage of the convenience and power of intelligent question answering with natural language.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1706564004-chatgpt.png\" alt=\"ChatGPT Interface\"></p>\n\n<p>With this level of popularity for ChatGPT, it’s no wonder that software developers are joining the ChatGPT app gold rush, building tools on top of OpenAI’s APIs. Building and deploying a GenAI-based app is quite easy to do—and we’re going to show you how!</p>\n\n<!-- more -->\n\n<p>In this post, we walk through how to build a Node.js application that works with OpenAI’s <a href=\"https://platform.openai.com/docs/guides/text-generation/chat-completions-api\">Chat Completions API</a> and uses its <a href=\"https://platform.openai.com/docs/guides/function-calling\">function calling</a> feature. We deploy it all to Heroku for quick, secure, and simple hosting. And we’ll have some fun along the way. This project is part of our new <a href=\"https://github.com/heroku-reference-apps\">Heroku Reference Applications</a>, a GitHub organization where we host different projects showcasing architectures to deploy to Heroku.</p>\n\n<p>Ready? Let’s go!</p>\n<h2 class=\"anchored\">\n  <a name=\"meet-the-menu-maker\" href=\"#meet-the-menu-maker\">Meet the Menu Maker</a>\n</h2>\n\n<p>Our web application is called Menu Maker. What does it do? Menu Maker lets users enter a list of ingredients that they have available to them. Menu Maker comes up with a dish using those ingredients. It provides a description of the dish as you’d find it on a fine dining menu, along with a full ingredients list and recipe instructions. </p>\n\n<p>This basic example of using generative AI uses the user-supplied ingredients, additional instructional prompts, and some structured constraints via ChatGPT's functions calling to create new content. The application’s code provides the user experience and the data flow.</p>\n\n<p>Menu Maker is a Node.js application with a React front-end UI that talks to an Express back-end API server. The Node.js application is a monorepo, containing both front-end and back-end code, stored at GitHub. The entire application is deployed on Heroku.</p>\n\n<p>Here’s a preview of Menu Maker in action:</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1706564097-menumaker.gif\" alt=\"Menu Maker in action\"></p>\n\n<p>Let’s briefly break down the application flow:</p>\n\n<ol>\n<li>The back-end server takes the user’s form submission, supplements it with additional information, and then sends a request to OpenAI’s Chat Completions API.</li>\n<li>The back-end server receives the response from OpenAI and passes it up to the front-end.</li>\n<li>The front-end updates the interface to reflect the response received from OpenAI.</li>\n</ol>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1706563894-Heroku%20AI%20Ref%20App%20%231%20-%20Heroku%20Reference%20App%201%20-%20Architecture.jpg\" alt=\"Architecture Diagram\"></p>\n<h2 class=\"anchored\">\n  <a name=\"prerequisites\" href=\"#prerequisites\">Prerequisites</a>\n</h2>\n\n<p><strong>Note</strong>: If you want to try the application first, deploy it using the “Deploy to Heroku” button in the reference application’s <a href=\"https://github.com/heroku-reference-apps/menumaker/blob/main/README.md\">README</a> file.</p>\n\n<p>Before we dive into the code let’s cover the prerequisites. Here’s what you need to get started:</p>\n\n<ol>\n<li>An <a href=\"https://openai.com/\">OpenAI account</a>. You must add a payment method and purchase a small amount of credit to access its APIs. As we built and tested our application, the total cost of all the API calls made was less than $1*. </li>\n<li>After setting up your OpenAI account, create a <a href=\"https://platform.openai.com/api-keys\">secret API key</a> and copy it down. Your application back-end needs this key to authenticate its requests to the OpenAI API.</li>\n<li>A <a href=\"https://signup.heroku.com/\">Heroku account</a>. You must add a payment method to cover your compute costs. For building and testing this application, we recommend using an <a href=\"https://devcenter.heroku.com/articles/eco-dyno-hours\">Eco dyno</a>, which has a $5 monthly flat fee and provides more than enough hours for your initial app.</li>\n<li>A <a href=\"https://github.com/\">GitHub account</a> for your code repository. Heroku hooks into your GitHub repo directly, simplifying deployment to a single click.</li>\n</ol>\n\n<p><strong>Note</strong>: Every menu recipe request incurs costs and the price varies depending on the selected model. For example, using the GPT-3 model, in order to spend $1, you'd have to request more than 30,000 recipes. See the <a href=\"https://openai.com/pricing\">OpenAI API pricing</a> page for more information.</p>\n<h2 class=\"anchored\">\n  <a name=\"initial-steps\" href=\"#initial-steps\">Initial Steps</a>\n</h2>\n\n<p>For our environment, we use Node <code>v20.10.0</code> and <code>yarn</code> as our package manager. Start by cloning the <a href=\"https://github.com/heroku-reference-apps/menumaker\">codebase available in our Heroku Reference Applications GitHub organization</a>. Then, install your dependencies by running:</p>\n\n<pre><code>yarn install\n</code></pre>\n<h2 class=\"anchored\">\n  <a name=\"build-the-back-end\" href=\"#build-the-back-end\">Build the Back-End</a>\n</h2>\n\n<p>Our back-end API server uses <a href=\"https://expressjs.com/\">Express</a> and listens for POST requests to the <code>/ingredients</code> endpoint. We supplement those ingredients with more precise prompt instructions, sending a subsequent request to OpenAI.</p>\n<h3 class=\"anchored\">\n  <a name=\"working-with-openai\" href=\"#working-with-openai\">Working with OpenAI</a>\n</h3>\n\n<p>Although OpenAI’s API supports advanced usage like image generation or speech-to-text, the simplest use case is to work with <a href=\"https://platform.openai.com/docs/guides/text-generation\">text generation</a>. You send a set of messages to let OpenAI know what you’re seeking, and what kind of behavior you expect as it responds to you.</p>\n\n<p>Typically, the first message is a <code>system</code> message, where you specify the desired behavior of ChatGPT. Eventually, you end up with a string of messages, a conversation, between the <code>user</code> (you) and the <code>assistant</code> (ChatGPT).</p>\n<h3 class=\"anchored\">\n  <a name=\"call-functions-with-openai\" href=\"#call-functions-with-openai\">Call Functions with OpenAI</a>\n</h3>\n\n<p>Most users are familiar with the chatbot-style conversation format of ChatGPT. However, developers want structured data, like a JSON object, in their ChatGPT responses. JSON makes it easier to work with responses programmatically.</p>\n\n<p>For example, imagine asking ChatGPT for a list of events in the 2020 Summer Olympics. As a programmer, you want to process the response by inserting each Olympic event into a database. You also want to send follow-up API requests for each event returned. In this case, you don’t want several paragraphs of ChatGPT describing Olympic events in prose. You’d rather have a JSON object with an array of event names.</p>\n\n<p>Use cases like these are where ChatGPT <em><a href=\"https://platform.openai.com/docs/guides/function-calling\">functions</a></em> come in handy. Alongside the set of <code>messages</code> you send to OpenAI, you send <code>functions</code>, which detail how you use the response from OpenAI. You can specify the name of a function to call, along with data types and descriptions of all the parameters to pass to that function.</p>\n\n<p><strong>Note:</strong> ChatGPT <em>doesn’t</em> call functions as part of its response. Instead, it provides a formatted response that you can easily feed directly into a custom function in your code.</p>\n<h3 class=\"anchored\">\n  <a name=\"initialize-prompt-settings-with-function-information\" href=\"#initialize-prompt-settings-with-function-information\">Initialize Prompt Settings with Function Information</a>\n</h3>\n\n<p>Let’s take a look at <code>src/server/ai.js</code>. In our code, we send a <code>settings</code> object to the Chat Completions API. The <code>settings</code> object starts with the following:</p>\n\n<pre><code class=\"language-javascript\">const settings = {\n  functions: [\n    {\n    name: 'updateDish',\n    description: 'Generate a fine dining dish based on a list of ingredients',\n    parameters: {\n        type: 'object',\n        properties: {\n        title: {\n            type: 'string',\n            description: 'Name of the dish, as it would appear on a fine dining menu'\n        },\n        description: {\n            type: 'string',\n            description: 'Description of the dish, in 2-3 sentences, as it would appear on a fine dining menu'\n        },\n        ingredients: {\n            type: 'array',\n            description: 'List of all ingredients--both provided and additional ones in the dish you have conceived--capitalized, along with measurements, that would be needed to make 8 servings of this dish',\n            items: {\n            type: 'object',\n            properties: {\n                ingredient: {\n                type: 'string',\n                description: 'Name of ingredient'\n                },\n                amount: {\n                type: 'string',\n                description: 'Amount of ingredient needed for recipe'\n                }\n            }\n            }\n        },\n        recipe: {\n            type: 'array',\n            description: 'Ordered list of recipe steps, numbered as \"1.\", \"2.\", etc., needed to make this dish',\n            items: {\n            type: 'string',\n            description: 'Recipe step'\n            }\n        }\n        },\n        required: ['title', 'description', 'ingredients', 'recipe']\n    }\n    }\n  ],\n  model: CHATGPT_MODEL,\n  function_call: 'auto'\n}\n</code></pre>\n\n<p>We’re telling OpenAI that we plan to use its response in a function that we call <code>updateDish</code>, a function in our React front-end code. When calling <code>updateDish</code>, we must pass in an object with four parameters:</p>\n\n<ol>\n<li>\n<code>title</code>: the name of our dish</li>\n<li>\n<code>description</code>: a description of our dish</li>\n<li>\n<code>ingredients</code>: an array of objects, each having an <code>ingredient</code> name and <code>amount</code>\n</li>\n<li>\n<code>recipe</code>: an array of recipe steps for making the dish</li>\n</ol>\n<h3 class=\"anchored\">\n  <a name=\"send-settings-with-ingredients-attached\" href=\"#send-settings-with-ingredients-attached\">Send Settings with Ingredients Attached</a>\n</h3>\n\n<p>In addition to the <code>functions</code> specification, we must attach <code>messages</code> in our request <code>settings</code>, to clearly tell ChatGPT what we want it to do. Our module’s <code>send</code> function looks like:</p>\n\n<pre><code class=\"language-javascript\">const PROMPT = 'I am writing descriptions of dishes for a menu. I am going to provide you with a list of ingredients. Based on that list, please come up with a dish that can be created with those ingredients.'\n\nconst send = async (ingredients) =&gt; {\n  const openai = new OpenAI({\n    timeout: 10000,\n    maxRetries: 3\n  })\n  settings.messages = [\n    {\n      role: 'system',\n      content: PROMPT\n    }, {\n      role: 'user',\n      content: `The ingredients that will contribute to my dish are: ${ingredients}.`\n    }\n  ]\n  const completion = await openai.chat.completions.create(settings)\n  return completion.choices[0].message\n}\n</code></pre>\n\n<p>Our Node.js application imports the <code><a href=\"https://www.npmjs.com/package/openai\">openai</a></code> package (not shown), which serves as a handy JavaScript library for OpenAI. It abstracts away the details of sending HTTP requests to the OpenAI API.</p>\n\n<p>We start with a <code>system</code> message that tells ChatGPT what the basic task is and the behavior we expect. Then, we add a <code>user</code> message that includes the ingredients, which gets passed as an argument to the <code>send</code> function. We send these <code>settings</code> to the API, asking it to <code><a href=\"https://platform.openai.com/docs/api-reference/chat/create\">create</a></code> a model response. Then, we return the response <code>message</code>.</p>\n<h3 class=\"anchored\">\n  <a name=\"handle-the-post-request\" href=\"#handle-the-post-request\">Handle the POST Request</a>\n</h3>\n\n<p>In <code>src/server/index.js</code>, we set up our Express server and handle POST requests to <code>/ingredients</code>. Our code looks like:</p>\n\n<pre><code class=\"language-javascript\">import express from 'express'\nimport AI from './ai.js'\n\nconst server = express()\nserver.use(express.json())\n\nserver.post('/ingredients', async (req, res) =&gt; {\n  if (process.env.NODE_ENV !== 'test') {\n    console.log(`Request to /ingredients received: ${req.body.message}`)\n  }\n  if ((typeof req.body.message) === 'undefined' || !req.body.message.length) {\n    res.status(400).json({ error: 'No ingredients provided in \"message\" key of payload.' })\n    return\n  }\n  try {\n    const completionResponse = await AI.send(req.body.message)\n    res.json(completionResponse.function_call)\n  } catch (error) {\n    res.status(500).json({ error: error.message })\n  }\n})\n\nexport default server\n</code></pre>\n\n<p>After removing the error handling and log messages, the most important lines of code are:</p>\n\n<pre><code class=\"language-javascript\">const completionResponse = await AI.send(req.body.message)\nres.json(completionResponse.function_call)\n</code></pre>\n\n<p>Our server passes the request payload <code>message</code> contents to our module’s <code>send</code> method. The response, from OpenAI, and then from our module, is an object that includes a <code>function_call</code> subobject. <code>function_call</code> has a <code>name</code> and <code>arguments</code>, which we use in our custom <code>updateDish</code> function.</p>\n<h3 class=\"anchored\">\n  <a name=\"testing-the-back-end\" href=\"#testing-the-back-end\">Testing the Back-End</a>\n</h3>\n\n<p>We’re ready to test our back-end!</p>\n\n<p>The <code>openai</code> JavaScript package expects an environment variable called <code>OPENAI_API_KEY</code>. We set up our server <a href=\"https://devcenter.heroku.com/articles/heroku-local#run-your-app-locally-with-the-heroku-local-command-line-tool-start-your-app-locally\">to listen on port 3000</a>, and then we start it:</p>\n\n<pre><code>OPENAI_API_KEY=sk-Kie*** node index.js\nServer is running on port 3000\n</code></pre>\n\n<p>In a separate terminal, we send a request with curl:</p>\n\n<pre><code>curl -X POST \\\n  --header \"Content-type:application/json\" \\\n  --data \"{\\\"message\\\":\\\"cauliflower, fresh rosemary, parmesan cheese\\\"}\" \\\n  http://localhost:3000/ingredients\n\n{\"name\":\"updateDish\",\"arguments\":\"{\\\"title\\\":\\\"Crispy Rosemary Parmesan Cauliflower\\\",\\\"description\\\":\\\"Tender cauliflower florets roasted to perfection with aromatic fresh rosemary and savory Parmesan cheese, creating a crispy and flavorful dish.\\\",\\\"ingredients\\\":[{\\\"ingredient\\\":\\\"cauliflower\\\",\\\"amount\\\":\\\"1 large head, cut into florets\\\"},{\\\"ingredient\\\":\\\"fresh rosemary\\\",\\\"amount\\\":\\\"2 tbsp, chopped\\\"},{\\\"ingredient\\\":\\\"parmesan cheese\\\",\\\"amount\\\":\\\"1/2 cup, grated\\\"},{\\\"ingredient\\\":\\\"olive oil\\\",\\\"amount\\\":\\\"3 tbsp\\\"},{\\\"ingredient\\\":\\\"salt\\\",\\\"amount\\\":\\\"to taste\\\"},{\\\"ingredient\\\":\\\"black pepper\\\",\\\"amount\\\":\\\"to taste\\\"}],\\\"recipe\\\":[\\\"1. Preheat the oven to 425°F.\\\",\\\"2. In a large bowl, toss the cauliflower florets with olive oil, chopped rosemary, salt, and black pepper.\\\",\\\"3. Spread the cauliflower on a baking sheet and roast for 25-30 minutes, or until golden brown and crispy.\\\",\\\"4. Sprinkle the roasted cauliflower with grated Parmesan cheese and return to the oven for 5 more minutes, until the cheese is melted and bubbly.\\\",\\\"5. Serve hot and enjoy!\\\"]}\"}\n</code></pre>\n\n<p>It works! We have a JSON response with <code>arguments</code> that our back-end can pass to the front-end’s <code>updateDish</code> function.</p>\n\n<p>Let’s briefly touch on what we did for the front-end UI.</p>\n<h2 class=\"anchored\">\n  <a name=\"build-the-front-end\" href=\"#build-the-front-end\">Build the Front-End</a>\n</h2>\n\n<p>All the OpenAI-related work happened in the back-end, so we won’t spend too much time unpacking the front-end. We built a basic React application that uses <a href=\"https://mui.com/material-ui/getting-started/\">Material UI</a> for styling. You can poke around in <code>src/client</code> to see all the details for our front-end application.</p>\n\n<p>In <code>src/client/App.js</code>, we see how our app handles the user’s web form submission:</p>\n\n<pre><code class=\"language-javascript\">const handleSubmit = async (inputValue) =&gt; {\n  if (inputValue.length === 0) {\n    setErrorMessage('Please provide ingredients before submitting the form.')\n    return\n  }\n  try {\n    setWaiting(true)\n    const response = await fetch('/ingredients', {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json'\n      },\n      body: JSON.stringify({ message: inputValue })\n    })\n    const data = await response.json()\n    if (!response.ok) {\n      setErrorMessage(data.error)\n      return\n    }\n\n    updateDish(JSON.parse(data.arguments))\n  } catch (error) {\n    setErrorMessage(error)\n  }\n}\n</code></pre>\n\n<p>When a user submits the form, the application sends a POST request to <code>/ingredients</code>. The <code>arguments</code> object in the response is JSON-parsed, then sent directly to our <code>updateDish</code> function. Using ChatGPT’s function calling feature significantly simplifies the steps to handle the response programmatically.</p>\n\n<p>Our <code>updateDish</code> function looks like:</p>\n\n<pre><code class=\"language-javascript\">const [title, setTitle] = useState('')\nconst [waiting, setWaiting] = useState(false)\nconst [description, setDescription] = useState('')\nconst [recipeSteps, setRecipeSteps] = useState([])\nconst [ingredients, setIngredients] = useState([])\nconst [errorMessage, setErrorMessage] = useState('')\nconst updateDish = ({ title, description, recipe, ingredients }) =&gt; {\n  setTitle(title)\n  setDescription(description)\n  setRecipeSteps(recipe)\n  setIngredients(ingredients)\n  setWaiting(false)\n  setErrorMessage('')\n}\n</code></pre>\n\n<p>Yes, that’s it. We work with <a href=\"https://react.dev/learn/state-a-components-memory\">React states</a> to keep track of our dish title, description, ingredients, and recipe. When <code>updateDish</code> updates these values, all of our components update accordingly. </p>\n\n<p>Our back-end and front-end pieces are all done. All that’s left to do is deploy.</p>\n\n<p>Not shown in this walkthrough, but which you can find in the code repository, are:</p>\n\n<ul>\n<li>Basic unit tests for back-end and front-end components, using <a href=\"https://jestjs.io/\">Jest</a>\n</li>\n<li>\n<a href=\"https://eslint.org/\">ESLint</a> and <a href=\"https://prettier.io/\">Prettier</a> configurations to keep our code clean and readable</li>\n<li>\n<a href=\"https://babeljs.io/\">Babel</a> and <a href=\"https://webpack.js.org/\">Webpack</a> configurations for working with modules and packaging our front-end code for deployment</li>\n</ul>\n<h2 class=\"anchored\">\n  <a name=\"deploy-to-heroku\" href=\"#deploy-to-heroku\">Deploy to Heroku</a>\n</h2>\n\n<p>With our codebase committed to GitHub, we’re ready to deploy our entire application on Heroku. You can also use the <a href=\"https://www.heroku.com/elements/buttons\">Heroku Button</a> in the reference repository to simplify the deployment.</p>\n<h3 class=\"anchored\">\n  <a name=\"step-1-create-a-new-heroku-app\" href=\"#step-1-create-a-new-heroku-app\">Step 1: Create a New Heroku App</a>\n</h3>\n\n<p>After logging in to Heroku, click “Create new app” in the Heroku Dashboard.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1706564301-create-new-app.png\" alt=\"Create a new Heroku app\"></p>\n\n<p>Next, provide a name for your app and click “Create app”.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1706564335-app-name.png\" alt=\"Application name\"></p>\n<h3 class=\"anchored\">\n  <a name=\"step-2-connect-your-repository\" href=\"#step-2-connect-your-repository\">Step 2: Connect Your Repository</a>\n</h3>\n\n<p>With your Heroku app created, <a href=\"https://devcenter.heroku.com/articles/github-integration#enabling-github-integration\">connect it to the GitHub repository</a> for your project.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1706564362-connect-github.png\" alt=\"Connect to GitHub\"></p>\n<h3 class=\"anchored\">\n  <a name=\"step-3-set-up-config-vars\" href=\"#step-3-set-up-config-vars\">Step 3: Set Up Config Vars</a>\n</h3>\n\n<p>Remember that your application back-end needs an OpenAI API key to authenticate requests. Navigate to your app “Settings”, then look for “Config Vars”. Add a new config var called <code>OPENAI_API_KEY</code>, and paste in the value for your key.</p>\n\n<p>Optionally, you can also set a <code>CHATGPT_MODEL</code> config var, telling <code>src/server/ai.js</code> which <a href=\"https://platform.openai.com/docs/models/overview\">GPT model</a> you want OpenAI to use. Models differ in capabilities, training data cutoff date, speed, and usage cost. If you don’t specify this config var, Menu Maker defaults to <code>gpt-3.5-turbo-1106</code>.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1706564394-config-vars.png\" alt=\"Setup config vars\"></p>\n<h3 class=\"anchored\">\n  <a name=\"step-4-deploy\" href=\"#step-4-deploy\">Step 4: Deploy</a>\n</h3>\n\n<p>Go to the “Deploy” tab for your Heroku app. Click “Deploy Branch”. Heroku takes the latest commit on the main branch, builds the application (<code>yarn build</code>), and then starts it up (<code>yarn start</code>). With just one click, you can deploy and update your application in under a minute.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1706564420-deploy.png\" alt=\"Deploy the app\"></p>\n<h3 class=\"anchored\">\n  <a name=\"step-5-open-your-app\" href=\"#step-5-open-your-app\">Step 5: Open Your App</a>\n</h3>\n\n<p>With the app deployed, click “Open app” at the top of your Heroku app page to get redirected to the unique and secure URL for your app.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1706564446-open-app.png\" alt=\"Open application\"></p>\n\n<p>With that, your shiny, new, ChatGPT-powered web application is up and running!</p>\n<h3 class=\"anchored\">\n  <a name=\"step-6-scale-down-your-app\" href=\"#step-6-scale-down-your-app\">Step 6: Scale Down Your App</a>\n</h3>\n\n<p>When you’re done using the app, remember to <a href=\"https://devcenter.heroku.com/articles/scaling#manual-scaling\">scale your dynos to zero</a> to prevent incurring unwanted costs.</p>\n<h2 class=\"anchored\">\n  <a name=\"conclusion\" href=\"#conclusion\">Conclusion</a>\n</h2>\n\n<p>With all the recent hype surrounding generative AI, many developers are itching to build ChatGPT-powered applications. Working with OpenAI’s API can initially seem daunting, but it’s straightforward. In addition, OpenAI’s function calling feature simplifies your task by accommodating your structured data needs.</p>\n\n<p>When it comes to quick and easy deployment, you can get up and running on Heroku within minutes, for just a few dollars a month. While the demonstration here works specifically with ChatGPT, it’s just as easy to deploy apps that use other foundation models, such as Google Bard, LLaMA from Meta, or other APIs.</p>\n\n<p>Are you ready to take the plunge into building GenAI-based applications? Today is the day. Happy coding!</p>","PublishedAt":"2024-01-30 09:00:00+00:00","OriginURL":"https://blog.heroku.com/working-with-chatgpt-functions-on-heroku","SourceName":"Heroku"}},{"node":{"ID":5573,"Title":"Monitor processes running on AWS Fargate with Datadog","Description":"<img class=\"webfeedsFeaturedVisual rss\" src=\"https://imgix.datadoghq.com/img/blog/monitor-fargate-processes/fargate-processes-hero.png\" width=\"100%\"/>Serverless platforms like AWS Fargate enable teams to focus on delivering value to customers by freeing up time otherwise spent managing infrastructure and operations. However, maintaining a deep level of observability into applications running on these fully managed platforms remains challenging. As a DevOps engineer, SRE, or application developer, understanding the performance of processes running on your serverless infrastructure is critical, as these are the building blocks that power your application and consume system resources.","PublishedAt":"2024-01-30 00:00:00+00:00","OriginURL":"https://www.datadoghq.com/blog/monitor-fargate-processes/","SourceName":"Datadog"}},{"node":{"ID":5570,"Title":"AWS Weekly Roundup —  Amazon API Gateway, AWS Step Functions, Amazon ECS, Amazon EKS, Amazon LightSail, Amazon VPC, and more — January 29, 2024","Description":"This past week our service teams continue to innovate on your behalf, and a lot has happened in the Amazon Web Services (AWS) universe. I’ll also share about all the AWS Community events and initiatives that are happening around the world. Let’s dive in! Last week’s launches Here are some launches that got my attention: […]","PublishedAt":"2024-01-29 20:32:52+00:00","OriginURL":"https://aws.amazon.com/blogs/aws/aws-weekly-roundup-amazon-api-gateway-aws-step-functions-amazon-ecs-amazon-eks-amazon-lightsail-amazon-vpc-and-more-january-29-2024/","SourceName":"AWS"}},{"node":{"ID":5569,"Title":"Improving machine learning iteration speed with faster application build and packaging","Description":"<p>Slow build times and inefficiencies in packaging and distributing execution files were costing our ML/AI engineers a significant amount of time while working on our training stack. By addressing these issues head-on, we were able to reduce this overhead by double-digit percentages.  In the fast-paced world of AI/ML development, it’s crucial to ensure that our [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2024/01/29/ml-applications/improving-machine-learning-iteration-speed-with-faster-application-build-and-packaging/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2024/01/29/ml-applications/improving-machine-learning-iteration-speed-with-faster-application-build-and-packaging/\">Improving machine learning iteration speed with faster application build and packaging</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n","PublishedAt":"2024-01-29 17:00:57+00:00","OriginURL":"https://engineering.fb.com/2024/01/29/ml-applications/improving-machine-learning-iteration-speed-with-faster-application-build-and-packaging/","SourceName":"Facebook"}},{"node":{"ID":5567,"Title":"Using Elastic Agent Performance Presets in 8.12","Description":"","PublishedAt":"2024-01-29 00:00:00+00:00","OriginURL":"https://www.elastic.co/blog/using-elastic-agent-performance-presets-in-8-12","SourceName":"Elastic"}},{"node":{"ID":5568,"Title":"Elastic Observability monitors metrics for Microsoft Azure in just minutes","Description":"","PublishedAt":"2024-01-29 00:00:00+00:00","OriginURL":"https://www.elastic.co/blog/elastic-observability-monitors-metrics-microsoft-azure","SourceName":"Elastic"}},{"node":{"ID":5565,"Title":"Cyber attacks targeting Jewish and Holocaust educational websites surge by 872% in 2023","Description":" In 2023, Cloudflare mitigated 35.7 million malicious requests that targeted Jewish and Holocaust educational websites. Today more than ever, it’s important to ensure these websites are protected and available ","PublishedAt":"2024-01-26 14:00:52+00:00","OriginURL":"https://blog.cloudflare.com/cyber-attacks-targeting-jewish-and-holocaust-educational-websites-surge-by-872-in-2023","SourceName":"Cloudflare"}},{"node":{"ID":5564,"Title":"Metadata Management and Data Governance with Cloudera SDX","Description":"<p>In this article, we will walk you through the process of implementing fine grained access control for the data governance framework within the Cloudera platform. This will allow a data office to implement access policies over metadata management assets like tags or classifications, business glossaries, and data catalog entities, laying the foundation for comprehensive data [&#8230;]</p>\n<p>The post <a href=\"https://blog.cloudera.com/metadata-management-and-data-governance-with-cloudera-sdx/\">Metadata Management and Data Governance with Cloudera SDX</a> appeared first on <a href=\"https://blog.cloudera.com\">Cloudera Blog</a>.</p>\n","PublishedAt":"2024-01-26 12:41:17+00:00","OriginURL":"https://blog.cloudera.com/metadata-management-and-data-governance-with-cloudera-sdx/","SourceName":"Cloudera"}},{"node":{"ID":5566,"Title":"Reflecting on the GDPR to celebrate Privacy Day 2024","Description":" On Privacy Day 2024, we answer the EU Commission’s call for reflection on how the GDPR has been functioning by pointing out two ways in which the GDPR has been applied that actually may harm people’s privacy ","PublishedAt":"2024-01-26 12:22:06+00:00","OriginURL":"https://blog.cloudflare.com/reflecting-on-the-gdpr-to-celebrate-privacy-day-2024","SourceName":"Cloudflare"}},{"node":{"ID":5563,"Title":"Go memory metrics demystified","Description":"<img class=\"webfeedsFeaturedVisual rss\" src=\"https://imgix.datadoghq.com/img/blog/go-memory-metrics/gomemory-00-hero.png\" width=\"100%\"/>For engineers in charge of supporting Go applications, diagnosing and resolving memory issues such as OOM kills or memory leaks can be a daunting task. Practical and easy-to-understand information about Go memory metrics is hard to come by, so it’s often challenging to reconcile your system metrics—such as process resident set size (RSS)—with the metrics provided by the old runtime.MemStats, with the newer runtime/metrics, or with profiling data.If you’ve been looking for this kind of information, then you’re in luck.","PublishedAt":"2024-01-26 00:00:00+00:00","OriginURL":"https://www.datadoghq.com/blog/go-memory-metrics/","SourceName":"Datadog"}},{"node":{"ID":5560,"Title":"New Version of ServiceNow Terraform catalog enables custom workspace naming and tags","Description":"The new ServiceNow Service Catalog for Terraform version 2.4 allows you to customize workspace names and add tags to your Terraform workspaces. ","PublishedAt":"2024-01-25 13:00:00+00:00","OriginURL":"https://www.hashicorp.com/blog/new-version-servicenow-terraform-catalog-custom-workspace-naming-tags","SourceName":"HashiCorp"}}]}},"pageContext":{"limit":30,"skip":180,"numPages":193,"currentPage":7}},"staticQueryHashes":["3649515864"]}