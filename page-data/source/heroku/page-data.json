{"componentChunkName":"component---src-templates-blog-post-list-by-source-tsx","path":"/source/heroku","result":{"data":{"allPost":{"edges":[{"node":{"ID":5572,"Title":"Working with ChatGPT Functions on Heroku","Description":"<h2 class=\"anchored\">\n  <a name=\"how-to-build-and-deploy-a-node-js-app-that-uses-openai-s-apis\" href=\"#how-to-build-and-deploy-a-node-js-app-that-uses-openai-s-apis\">How to Build and Deploy a Node.js App That Uses OpenAI’s APIs</a>\n</h2>\n\n<p>Near the end of 2023, ChatGPT <a href=\"https://www.linkedin.com/news/story/chatgpt-hits-100m-weekly-users-5808204/\">announced</a> that it had 100M weekly users. That’s a massive base of users who want to take advantage of the convenience and power of intelligent question answering with natural language.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1706564004-chatgpt.png\" alt=\"ChatGPT Interface\"></p>\n\n<p>With this level of popularity for ChatGPT, it’s no wonder that software developers are joining the ChatGPT app gold rush, building tools on top of OpenAI’s APIs. Building and deploying a GenAI-based app is quite easy to do—and we’re going to show you how!</p>\n\n<!-- more -->\n\n<p>In this post, we walk through how to build a Node.js application that works with OpenAI’s <a href=\"https://platform.openai.com/docs/guides/text-generation/chat-completions-api\">Chat Completions API</a> and uses its <a href=\"https://platform.openai.com/docs/guides/function-calling\">function calling</a> feature. We deploy it all to Heroku for quick, secure, and simple hosting. And we’ll have some fun along the way. This project is part of our new <a href=\"https://github.com/heroku-reference-apps\">Heroku Reference Applications</a>, a GitHub organization where we host different projects showcasing architectures to deploy to Heroku.</p>\n\n<p>Ready? Let’s go!</p>\n<h2 class=\"anchored\">\n  <a name=\"meet-the-menu-maker\" href=\"#meet-the-menu-maker\">Meet the Menu Maker</a>\n</h2>\n\n<p>Our web application is called Menu Maker. What does it do? Menu Maker lets users enter a list of ingredients that they have available to them. Menu Maker comes up with a dish using those ingredients. It provides a description of the dish as you’d find it on a fine dining menu, along with a full ingredients list and recipe instructions. </p>\n\n<p>This basic example of using generative AI uses the user-supplied ingredients, additional instructional prompts, and some structured constraints via ChatGPT's functions calling to create new content. The application’s code provides the user experience and the data flow.</p>\n\n<p>Menu Maker is a Node.js application with a React front-end UI that talks to an Express back-end API server. The Node.js application is a monorepo, containing both front-end and back-end code, stored at GitHub. The entire application is deployed on Heroku.</p>\n\n<p>Here’s a preview of Menu Maker in action:</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1706564097-menumaker.gif\" alt=\"Menu Maker in action\"></p>\n\n<p>Let’s briefly break down the application flow:</p>\n\n<ol>\n<li>The back-end server takes the user’s form submission, supplements it with additional information, and then sends a request to OpenAI’s Chat Completions API.</li>\n<li>The back-end server receives the response from OpenAI and passes it up to the front-end.</li>\n<li>The front-end updates the interface to reflect the response received from OpenAI.</li>\n</ol>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1706563894-Heroku%20AI%20Ref%20App%20%231%20-%20Heroku%20Reference%20App%201%20-%20Architecture.jpg\" alt=\"Architecture Diagram\"></p>\n<h2 class=\"anchored\">\n  <a name=\"prerequisites\" href=\"#prerequisites\">Prerequisites</a>\n</h2>\n\n<p><strong>Note</strong>: If you want to try the application first, deploy it using the “Deploy to Heroku” button in the reference application’s <a href=\"https://github.com/heroku-reference-apps/menumaker/blob/main/README.md\">README</a> file.</p>\n\n<p>Before we dive into the code let’s cover the prerequisites. Here’s what you need to get started:</p>\n\n<ol>\n<li>An <a href=\"https://openai.com/\">OpenAI account</a>. You must add a payment method and purchase a small amount of credit to access its APIs. As we built and tested our application, the total cost of all the API calls made was less than $1*. </li>\n<li>After setting up your OpenAI account, create a <a href=\"https://platform.openai.com/api-keys\">secret API key</a> and copy it down. Your application back-end needs this key to authenticate its requests to the OpenAI API.</li>\n<li>A <a href=\"https://signup.heroku.com/\">Heroku account</a>. You must add a payment method to cover your compute costs. For building and testing this application, we recommend using an <a href=\"https://devcenter.heroku.com/articles/eco-dyno-hours\">Eco dyno</a>, which has a $5 monthly flat fee and provides more than enough hours for your initial app.</li>\n<li>A <a href=\"https://github.com/\">GitHub account</a> for your code repository. Heroku hooks into your GitHub repo directly, simplifying deployment to a single click.</li>\n</ol>\n\n<p><strong>Note</strong>: Every menu recipe request incurs costs and the price varies depending on the selected model. For example, using the GPT-3 model, in order to spend $1, you'd have to request more than 30,000 recipes. See the <a href=\"https://openai.com/pricing\">OpenAI API pricing</a> page for more information.</p>\n<h2 class=\"anchored\">\n  <a name=\"initial-steps\" href=\"#initial-steps\">Initial Steps</a>\n</h2>\n\n<p>For our environment, we use Node <code>v20.10.0</code> and <code>yarn</code> as our package manager. Start by cloning the <a href=\"https://github.com/heroku-reference-apps/menumaker\">codebase available in our Heroku Reference Applications GitHub organization</a>. Then, install your dependencies by running:</p>\n\n<pre><code>yarn install\n</code></pre>\n<h2 class=\"anchored\">\n  <a name=\"build-the-back-end\" href=\"#build-the-back-end\">Build the Back-End</a>\n</h2>\n\n<p>Our back-end API server uses <a href=\"https://expressjs.com/\">Express</a> and listens for POST requests to the <code>/ingredients</code> endpoint. We supplement those ingredients with more precise prompt instructions, sending a subsequent request to OpenAI.</p>\n<h3 class=\"anchored\">\n  <a name=\"working-with-openai\" href=\"#working-with-openai\">Working with OpenAI</a>\n</h3>\n\n<p>Although OpenAI’s API supports advanced usage like image generation or speech-to-text, the simplest use case is to work with <a href=\"https://platform.openai.com/docs/guides/text-generation\">text generation</a>. You send a set of messages to let OpenAI know what you’re seeking, and what kind of behavior you expect as it responds to you.</p>\n\n<p>Typically, the first message is a <code>system</code> message, where you specify the desired behavior of ChatGPT. Eventually, you end up with a string of messages, a conversation, between the <code>user</code> (you) and the <code>assistant</code> (ChatGPT).</p>\n<h3 class=\"anchored\">\n  <a name=\"call-functions-with-openai\" href=\"#call-functions-with-openai\">Call Functions with OpenAI</a>\n</h3>\n\n<p>Most users are familiar with the chatbot-style conversation format of ChatGPT. However, developers want structured data, like a JSON object, in their ChatGPT responses. JSON makes it easier to work with responses programmatically.</p>\n\n<p>For example, imagine asking ChatGPT for a list of events in the 2020 Summer Olympics. As a programmer, you want to process the response by inserting each Olympic event into a database. You also want to send follow-up API requests for each event returned. In this case, you don’t want several paragraphs of ChatGPT describing Olympic events in prose. You’d rather have a JSON object with an array of event names.</p>\n\n<p>Use cases like these are where ChatGPT <em><a href=\"https://platform.openai.com/docs/guides/function-calling\">functions</a></em> come in handy. Alongside the set of <code>messages</code> you send to OpenAI, you send <code>functions</code>, which detail how you use the response from OpenAI. You can specify the name of a function to call, along with data types and descriptions of all the parameters to pass to that function.</p>\n\n<p><strong>Note:</strong> ChatGPT <em>doesn’t</em> call functions as part of its response. Instead, it provides a formatted response that you can easily feed directly into a custom function in your code.</p>\n<h3 class=\"anchored\">\n  <a name=\"initialize-prompt-settings-with-function-information\" href=\"#initialize-prompt-settings-with-function-information\">Initialize Prompt Settings with Function Information</a>\n</h3>\n\n<p>Let’s take a look at <code>src/server/ai.js</code>. In our code, we send a <code>settings</code> object to the Chat Completions API. The <code>settings</code> object starts with the following:</p>\n\n<pre><code class=\"language-javascript\">const settings = {\n  functions: [\n    {\n    name: 'updateDish',\n    description: 'Generate a fine dining dish based on a list of ingredients',\n    parameters: {\n        type: 'object',\n        properties: {\n        title: {\n            type: 'string',\n            description: 'Name of the dish, as it would appear on a fine dining menu'\n        },\n        description: {\n            type: 'string',\n            description: 'Description of the dish, in 2-3 sentences, as it would appear on a fine dining menu'\n        },\n        ingredients: {\n            type: 'array',\n            description: 'List of all ingredients--both provided and additional ones in the dish you have conceived--capitalized, along with measurements, that would be needed to make 8 servings of this dish',\n            items: {\n            type: 'object',\n            properties: {\n                ingredient: {\n                type: 'string',\n                description: 'Name of ingredient'\n                },\n                amount: {\n                type: 'string',\n                description: 'Amount of ingredient needed for recipe'\n                }\n            }\n            }\n        },\n        recipe: {\n            type: 'array',\n            description: 'Ordered list of recipe steps, numbered as \"1.\", \"2.\", etc., needed to make this dish',\n            items: {\n            type: 'string',\n            description: 'Recipe step'\n            }\n        }\n        },\n        required: ['title', 'description', 'ingredients', 'recipe']\n    }\n    }\n  ],\n  model: CHATGPT_MODEL,\n  function_call: 'auto'\n}\n</code></pre>\n\n<p>We’re telling OpenAI that we plan to use its response in a function that we call <code>updateDish</code>, a function in our React front-end code. When calling <code>updateDish</code>, we must pass in an object with four parameters:</p>\n\n<ol>\n<li>\n<code>title</code>: the name of our dish</li>\n<li>\n<code>description</code>: a description of our dish</li>\n<li>\n<code>ingredients</code>: an array of objects, each having an <code>ingredient</code> name and <code>amount</code>\n</li>\n<li>\n<code>recipe</code>: an array of recipe steps for making the dish</li>\n</ol>\n<h3 class=\"anchored\">\n  <a name=\"send-settings-with-ingredients-attached\" href=\"#send-settings-with-ingredients-attached\">Send Settings with Ingredients Attached</a>\n</h3>\n\n<p>In addition to the <code>functions</code> specification, we must attach <code>messages</code> in our request <code>settings</code>, to clearly tell ChatGPT what we want it to do. Our module’s <code>send</code> function looks like:</p>\n\n<pre><code class=\"language-javascript\">const PROMPT = 'I am writing descriptions of dishes for a menu. I am going to provide you with a list of ingredients. Based on that list, please come up with a dish that can be created with those ingredients.'\n\nconst send = async (ingredients) =&gt; {\n  const openai = new OpenAI({\n    timeout: 10000,\n    maxRetries: 3\n  })\n  settings.messages = [\n    {\n      role: 'system',\n      content: PROMPT\n    }, {\n      role: 'user',\n      content: `The ingredients that will contribute to my dish are: ${ingredients}.`\n    }\n  ]\n  const completion = await openai.chat.completions.create(settings)\n  return completion.choices[0].message\n}\n</code></pre>\n\n<p>Our Node.js application imports the <code><a href=\"https://www.npmjs.com/package/openai\">openai</a></code> package (not shown), which serves as a handy JavaScript library for OpenAI. It abstracts away the details of sending HTTP requests to the OpenAI API.</p>\n\n<p>We start with a <code>system</code> message that tells ChatGPT what the basic task is and the behavior we expect. Then, we add a <code>user</code> message that includes the ingredients, which gets passed as an argument to the <code>send</code> function. We send these <code>settings</code> to the API, asking it to <code><a href=\"https://platform.openai.com/docs/api-reference/chat/create\">create</a></code> a model response. Then, we return the response <code>message</code>.</p>\n<h3 class=\"anchored\">\n  <a name=\"handle-the-post-request\" href=\"#handle-the-post-request\">Handle the POST Request</a>\n</h3>\n\n<p>In <code>src/server/index.js</code>, we set up our Express server and handle POST requests to <code>/ingredients</code>. Our code looks like:</p>\n\n<pre><code class=\"language-javascript\">import express from 'express'\nimport AI from './ai.js'\n\nconst server = express()\nserver.use(express.json())\n\nserver.post('/ingredients', async (req, res) =&gt; {\n  if (process.env.NODE_ENV !== 'test') {\n    console.log(`Request to /ingredients received: ${req.body.message}`)\n  }\n  if ((typeof req.body.message) === 'undefined' || !req.body.message.length) {\n    res.status(400).json({ error: 'No ingredients provided in \"message\" key of payload.' })\n    return\n  }\n  try {\n    const completionResponse = await AI.send(req.body.message)\n    res.json(completionResponse.function_call)\n  } catch (error) {\n    res.status(500).json({ error: error.message })\n  }\n})\n\nexport default server\n</code></pre>\n\n<p>After removing the error handling and log messages, the most important lines of code are:</p>\n\n<pre><code class=\"language-javascript\">const completionResponse = await AI.send(req.body.message)\nres.json(completionResponse.function_call)\n</code></pre>\n\n<p>Our server passes the request payload <code>message</code> contents to our module’s <code>send</code> method. The response, from OpenAI, and then from our module, is an object that includes a <code>function_call</code> subobject. <code>function_call</code> has a <code>name</code> and <code>arguments</code>, which we use in our custom <code>updateDish</code> function.</p>\n<h3 class=\"anchored\">\n  <a name=\"testing-the-back-end\" href=\"#testing-the-back-end\">Testing the Back-End</a>\n</h3>\n\n<p>We’re ready to test our back-end!</p>\n\n<p>The <code>openai</code> JavaScript package expects an environment variable called <code>OPENAI_API_KEY</code>. We set up our server <a href=\"https://devcenter.heroku.com/articles/heroku-local#run-your-app-locally-with-the-heroku-local-command-line-tool-start-your-app-locally\">to listen on port 3000</a>, and then we start it:</p>\n\n<pre><code>OPENAI_API_KEY=sk-Kie*** node index.js\nServer is running on port 3000\n</code></pre>\n\n<p>In a separate terminal, we send a request with curl:</p>\n\n<pre><code>curl -X POST \\\n  --header \"Content-type:application/json\" \\\n  --data \"{\\\"message\\\":\\\"cauliflower, fresh rosemary, parmesan cheese\\\"}\" \\\n  http://localhost:3000/ingredients\n\n{\"name\":\"updateDish\",\"arguments\":\"{\\\"title\\\":\\\"Crispy Rosemary Parmesan Cauliflower\\\",\\\"description\\\":\\\"Tender cauliflower florets roasted to perfection with aromatic fresh rosemary and savory Parmesan cheese, creating a crispy and flavorful dish.\\\",\\\"ingredients\\\":[{\\\"ingredient\\\":\\\"cauliflower\\\",\\\"amount\\\":\\\"1 large head, cut into florets\\\"},{\\\"ingredient\\\":\\\"fresh rosemary\\\",\\\"amount\\\":\\\"2 tbsp, chopped\\\"},{\\\"ingredient\\\":\\\"parmesan cheese\\\",\\\"amount\\\":\\\"1/2 cup, grated\\\"},{\\\"ingredient\\\":\\\"olive oil\\\",\\\"amount\\\":\\\"3 tbsp\\\"},{\\\"ingredient\\\":\\\"salt\\\",\\\"amount\\\":\\\"to taste\\\"},{\\\"ingredient\\\":\\\"black pepper\\\",\\\"amount\\\":\\\"to taste\\\"}],\\\"recipe\\\":[\\\"1. Preheat the oven to 425°F.\\\",\\\"2. In a large bowl, toss the cauliflower florets with olive oil, chopped rosemary, salt, and black pepper.\\\",\\\"3. Spread the cauliflower on a baking sheet and roast for 25-30 minutes, or until golden brown and crispy.\\\",\\\"4. Sprinkle the roasted cauliflower with grated Parmesan cheese and return to the oven for 5 more minutes, until the cheese is melted and bubbly.\\\",\\\"5. Serve hot and enjoy!\\\"]}\"}\n</code></pre>\n\n<p>It works! We have a JSON response with <code>arguments</code> that our back-end can pass to the front-end’s <code>updateDish</code> function.</p>\n\n<p>Let’s briefly touch on what we did for the front-end UI.</p>\n<h2 class=\"anchored\">\n  <a name=\"build-the-front-end\" href=\"#build-the-front-end\">Build the Front-End</a>\n</h2>\n\n<p>All the OpenAI-related work happened in the back-end, so we won’t spend too much time unpacking the front-end. We built a basic React application that uses <a href=\"https://mui.com/material-ui/getting-started/\">Material UI</a> for styling. You can poke around in <code>src/client</code> to see all the details for our front-end application.</p>\n\n<p>In <code>src/client/App.js</code>, we see how our app handles the user’s web form submission:</p>\n\n<pre><code class=\"language-javascript\">const handleSubmit = async (inputValue) =&gt; {\n  if (inputValue.length === 0) {\n    setErrorMessage('Please provide ingredients before submitting the form.')\n    return\n  }\n  try {\n    setWaiting(true)\n    const response = await fetch('/ingredients', {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json'\n      },\n      body: JSON.stringify({ message: inputValue })\n    })\n    const data = await response.json()\n    if (!response.ok) {\n      setErrorMessage(data.error)\n      return\n    }\n\n    updateDish(JSON.parse(data.arguments))\n  } catch (error) {\n    setErrorMessage(error)\n  }\n}\n</code></pre>\n\n<p>When a user submits the form, the application sends a POST request to <code>/ingredients</code>. The <code>arguments</code> object in the response is JSON-parsed, then sent directly to our <code>updateDish</code> function. Using ChatGPT’s function calling feature significantly simplifies the steps to handle the response programmatically.</p>\n\n<p>Our <code>updateDish</code> function looks like:</p>\n\n<pre><code class=\"language-javascript\">const [title, setTitle] = useState('')\nconst [waiting, setWaiting] = useState(false)\nconst [description, setDescription] = useState('')\nconst [recipeSteps, setRecipeSteps] = useState([])\nconst [ingredients, setIngredients] = useState([])\nconst [errorMessage, setErrorMessage] = useState('')\nconst updateDish = ({ title, description, recipe, ingredients }) =&gt; {\n  setTitle(title)\n  setDescription(description)\n  setRecipeSteps(recipe)\n  setIngredients(ingredients)\n  setWaiting(false)\n  setErrorMessage('')\n}\n</code></pre>\n\n<p>Yes, that’s it. We work with <a href=\"https://react.dev/learn/state-a-components-memory\">React states</a> to keep track of our dish title, description, ingredients, and recipe. When <code>updateDish</code> updates these values, all of our components update accordingly. </p>\n\n<p>Our back-end and front-end pieces are all done. All that’s left to do is deploy.</p>\n\n<p>Not shown in this walkthrough, but which you can find in the code repository, are:</p>\n\n<ul>\n<li>Basic unit tests for back-end and front-end components, using <a href=\"https://jestjs.io/\">Jest</a>\n</li>\n<li>\n<a href=\"https://eslint.org/\">ESLint</a> and <a href=\"https://prettier.io/\">Prettier</a> configurations to keep our code clean and readable</li>\n<li>\n<a href=\"https://babeljs.io/\">Babel</a> and <a href=\"https://webpack.js.org/\">Webpack</a> configurations for working with modules and packaging our front-end code for deployment</li>\n</ul>\n<h2 class=\"anchored\">\n  <a name=\"deploy-to-heroku\" href=\"#deploy-to-heroku\">Deploy to Heroku</a>\n</h2>\n\n<p>With our codebase committed to GitHub, we’re ready to deploy our entire application on Heroku. You can also use the <a href=\"https://www.heroku.com/elements/buttons\">Heroku Button</a> in the reference repository to simplify the deployment.</p>\n<h3 class=\"anchored\">\n  <a name=\"step-1-create-a-new-heroku-app\" href=\"#step-1-create-a-new-heroku-app\">Step 1: Create a New Heroku App</a>\n</h3>\n\n<p>After logging in to Heroku, click “Create new app” in the Heroku Dashboard.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1706564301-create-new-app.png\" alt=\"Create a new Heroku app\"></p>\n\n<p>Next, provide a name for your app and click “Create app”.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1706564335-app-name.png\" alt=\"Application name\"></p>\n<h3 class=\"anchored\">\n  <a name=\"step-2-connect-your-repository\" href=\"#step-2-connect-your-repository\">Step 2: Connect Your Repository</a>\n</h3>\n\n<p>With your Heroku app created, <a href=\"https://devcenter.heroku.com/articles/github-integration#enabling-github-integration\">connect it to the GitHub repository</a> for your project.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1706564362-connect-github.png\" alt=\"Connect to GitHub\"></p>\n<h3 class=\"anchored\">\n  <a name=\"step-3-set-up-config-vars\" href=\"#step-3-set-up-config-vars\">Step 3: Set Up Config Vars</a>\n</h3>\n\n<p>Remember that your application back-end needs an OpenAI API key to authenticate requests. Navigate to your app “Settings”, then look for “Config Vars”. Add a new config var called <code>OPENAI_API_KEY</code>, and paste in the value for your key.</p>\n\n<p>Optionally, you can also set a <code>CHATGPT_MODEL</code> config var, telling <code>src/server/ai.js</code> which <a href=\"https://platform.openai.com/docs/models/overview\">GPT model</a> you want OpenAI to use. Models differ in capabilities, training data cutoff date, speed, and usage cost. If you don’t specify this config var, Menu Maker defaults to <code>gpt-3.5-turbo-1106</code>.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1706564394-config-vars.png\" alt=\"Setup config vars\"></p>\n<h3 class=\"anchored\">\n  <a name=\"step-4-deploy\" href=\"#step-4-deploy\">Step 4: Deploy</a>\n</h3>\n\n<p>Go to the “Deploy” tab for your Heroku app. Click “Deploy Branch”. Heroku takes the latest commit on the main branch, builds the application (<code>yarn build</code>), and then starts it up (<code>yarn start</code>). With just one click, you can deploy and update your application in under a minute.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1706564420-deploy.png\" alt=\"Deploy the app\"></p>\n<h3 class=\"anchored\">\n  <a name=\"step-5-open-your-app\" href=\"#step-5-open-your-app\">Step 5: Open Your App</a>\n</h3>\n\n<p>With the app deployed, click “Open app” at the top of your Heroku app page to get redirected to the unique and secure URL for your app.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1706564446-open-app.png\" alt=\"Open application\"></p>\n\n<p>With that, your shiny, new, ChatGPT-powered web application is up and running!</p>\n<h3 class=\"anchored\">\n  <a name=\"step-6-scale-down-your-app\" href=\"#step-6-scale-down-your-app\">Step 6: Scale Down Your App</a>\n</h3>\n\n<p>When you’re done using the app, remember to <a href=\"https://devcenter.heroku.com/articles/scaling#manual-scaling\">scale your dynos to zero</a> to prevent incurring unwanted costs.</p>\n<h2 class=\"anchored\">\n  <a name=\"conclusion\" href=\"#conclusion\">Conclusion</a>\n</h2>\n\n<p>With all the recent hype surrounding generative AI, many developers are itching to build ChatGPT-powered applications. Working with OpenAI’s API can initially seem daunting, but it’s straightforward. In addition, OpenAI’s function calling feature simplifies your task by accommodating your structured data needs.</p>\n\n<p>When it comes to quick and easy deployment, you can get up and running on Heroku within minutes, for just a few dollars a month. While the demonstration here works specifically with ChatGPT, it’s just as easy to deploy apps that use other foundation models, such as Google Bard, LLaMA from Meta, or other APIs.</p>\n\n<p>Are you ready to take the plunge into building GenAI-based applications? Today is the day. Happy coding!</p>","PublishedAt":"2024-01-30 09:00:00+00:00","OriginURL":"https://blog.heroku.com/working-with-chatgpt-functions-on-heroku","SourceName":"Heroku"}},{"node":{"ID":5055,"Title":"How to Use pgvector for Similarity Search on Heroku Postgres","Description":"<h2 class=\"anchored\">\n  <a name=\"introducing-pgvector-for-heroku-postgres\" href=\"#introducing-pgvector-for-heroku-postgres\">Introducing pgvector for Heroku Postgres</a>\n</h2>\n\n<p>Over the past few weeks, we worked on adding <a href=\"https://github.com/pgvector/pgvector\">pgvector</a> as an extension on Heroku Postgres. We're excited to release this feature, and based on the feedback on <a href=\"https://github.com/heroku/roadmap/issues/156\">our public roadmap</a>, many of you are too. We want to share a bit more about how you can use it and how it may be helpful to you. </p>\n\n<p>All <a href=\"https://devcenter.heroku.com/articles/heroku-postgres-plans#plan-tiers\">Standard-tier or higher</a> databases running Postgres 15 now support the <a href=\"https://devcenter.heroku.com/articles/heroku-postgres-extensions-postgis-full-text-search#pgvector\"><code>pgvector</code> extension</a>. You can get started by running <code>CREATE EXTENSION vector;</code> in a client session. Postgres 15 has been the default version on Heroku Postgres since March 2023.  If you're on an older version and want to use pgvector, <a href=\"https://devcenter.heroku.com/articles/upgrading-heroku-postgres-databases\">upgrade</a> to Postgres 15.</p>\n\n<p>The extension adds the vector data type to Heroku Postgres along with additional functions to work with it. Vectors are important for working with large language models and other machine learning applications, as the <a href=\"https://huggingface.co/blog/getting-started-with-embeddings#understanding-embeddings\">embeddings</a> generated by these models are often output in vector format. Working with vectors lets you implement things like similarity search across these embeddings. See our <a href=\"https://blog.heroku.com/pgvector-launch#understanding-pgvector-and-its-significance\">launch blog</a> for more background into what pgvector is, its significance, and ideas for how to use this new data type.</p>\n<h2 class=\"anchored\">\n  <a name=\"an-example-word-vector-similarity-search\" href=\"#an-example-word-vector-similarity-search\">An Example: Word Vector Similarity Search</a>\n</h2>\n\n<p>To show a simple example of how to generate and save vector data to your Heroku database, I'm using the <a href=\"https://wikipedia2vec.github.io/wikipedia2vec/\">Wikipedia2Vec</a> pretrained embeddings. However, you can train your own embeddings or use other models providing embeddings via API, like <a href=\"https://huggingface.co/blog/getting-started-with-embeddings\">HuggingFace</a> or <a href=\"https://openai.com/\">OpenAI</a>. The model you want to use depends on the type of data you're working with. There are models for tasks like computing sentence similarities, searching large texts, or performing image classification. Wikipedia2Vec uses a <a href=\"https://en.wikipedia.org/wiki/Word2vec\">Word2vec</a> algorithm to generate vectors for individual words, which maps similar words close to each other in a continuous vector space. </p>\n\n<p>I like animals, so I want to use Wikipedia2Vec to group similar animals. I’m using the vector embeddings of each animal and the distance between them to find animals that are alike.</p>\n\n<p>If I want to get the embedding for a word from Wikipedia2Vec, I need to use a model. I downloaded one from the <a href=\"https://wikipedia2vec.github.io/wikipedia2vec/pretrained/\">pretrained embeddings</a> on their website. Then I can use their Python module and the function <code>get_word_vector</code> as follows:</p>\n\n<pre><code>from wikipedia2vec import Wikipedia2Vec\nwiki2vec = Wikipedia2Vec.load('enwiki_20180420_100d.pkl')\nwiki2vec.get_word_vector('llama')\n</code></pre>\n\n<p>The output of the vector looks like this:</p>\n\n<pre><code>memmap([-0.15647224,  0.04055957,  0.48439676, -0.22689971, -0.04544162,\n        -0.06538601,  0.22609918, -0.26075622, -0.7195759 , -0.24022003,\n         0.1050799 , -0.5550985 ,  0.4054564 ,  0.14180332,  0.19856507,\n         0.09962048,  0.38372937, -1.1912689 , -0.93939453, -0.28067762,\n         0.04410955,  0.43394643, -0.3429818 ,  0.22209083, -0.46317756,\n        -0.18109794,  0.2775289 , -0.21939017, -0.27015808,  0.72002393,\n        -0.01586861, -0.23480305,  0.365697  ,  0.61743397, -0.07460125,\n        -0.10441436, -0.6537417 ,  0.01339269,  0.06189647, -0.17747395,\n         0.2669941 , -0.03428648, -0.8533792 , -0.09588563, -0.7616592 ,\n        -0.11528812, -0.07127796,  0.28456485, -0.12986512, -0.8063386 ,\n        -0.04875885, -0.27353695, -0.32921   , -0.03807172,  0.10544889,\n         0.49989182, -0.03783042, -0.37752548, -0.19257008,  0.06255971,\n         0.25994852, -0.81092316, -0.15077794,  0.00658835,  0.02033841,\n        -0.32411653, -0.03033727, -0.64633304, -0.43443972, -0.30764043,\n        -0.11036412,  0.04134453, -0.26934972, -0.0289086 , -0.50319433,\n        -0.0204528 , -0.00278326,  0.36589545,  0.5446438 , -0.10852882,\n         0.09699931, -0.01168614,  0.08618425, -0.28925297, -0.25445923,\n         0.63120073,  0.52186656,  0.3439454 ,  0.6686451 ,  0.1076297 ,\n        -0.34688494,  0.05976971, -0.3720558 ,  0.20328045, -0.485623  ,\n        -0.2222396 , -0.22480975,  0.4386788 , -0.7506131 ,  0.14270408],\n       dtype=float32)\n</code></pre>\n\n<p>To get your vector data into your database:</p>\n\n<ol>\n<li>Generate the embeddings.</li>\n<li>Add a column to your database to store your embeddings.</li>\n<li>Save the embeddings to the database.</li>\n</ol>\n\n<p>I already have the embeddings from Wikipedia2Vec, so let’s walk through preparing my database and saving them. When creating a vector column, it's necessary to declare a length for it, so check and see the length of the embedding the model outputs. In my case, the embeddings are 100 numbers long, so I add that column to my table.</p>\n\n<pre><code>CREATE TABLE animals(id serial PRIMARY KEY, name VARCHAR(100), embedding VECTOR(100));\n</code></pre>\n\n<p>From there, save the items you're interested in to your database. You can do it directly in SQL:</p>\n\n<pre><code>INSERT INTO animals(name, embedding) VALUES ('llama', '[-0.15647223591804504, \n…\n-0.7506130933761597, 0.1427040845155716]');\n</code></pre>\n\n<p>But you can also use your <a href=\"https://devcenter.heroku.com/articles/connecting-heroku-postgres\">favorite programming language</a> along with a Postgres client and a <a href=\"https://github.com/pgvector/pgvector#languages\">pgvector library</a>. For this example, I used Python, <a href=\"https://github.com/psycopg/psycopg\">psycopg</a>, and <a href=\"https://github.com/pgvector/pgvector-python\">pgvector-python</a>. Here I'm using the pretrained embedding file to generate embeddings for a list of animals I made, <code>valeries-animals.txt</code>,  and save them to my database.</p>\n\n<pre><code>import psycopg\nfrom pathlib import Path\nfrom pgvector.psycopg import register_vector\nfrom wikipedia2vec import Wikipedia2Vec\n\nwiki2vec = Wikipedia2Vec.load('enwiki_20180420_100d.pkl')\nanimals = Path('valeries-animals.txt').read_text().split('\\n')\n\nwith psycopg.connect(DATABASE_URL, sslmode='require', autocommit=True) as conn:\n    register_vector(conn)\n    cur = conn.cursor()\n    for animal in animals:\n        cur.execute(\"INSERT INTO animals(name, embedding) VALUES (%s, %s)\", (animal, wiki2vec.get_word_vector(animal)))\n</code></pre>\n\n<p>Now that I have the embeddings in my database, I can use pgvector's functions to query them. The extension includes functions to calculate Euclidean distance (<code>&lt;-&gt;</code>), cosine distance (<code>&lt;=&gt;</code>), and inner product (<code>&lt;#&gt;</code>). You can use all three for <a href=\"https://developers.google.com/machine-learning/clustering/similarity/measuring-similarity\">calculating similarity</a> between vectors. Which one you use depends on <a href=\"https://cmry.github.io/notes/euclidean-v-cosine\">your data as well as your use case</a>.</p>\n\n<p>Here I'm using Euclidean distance to find the five animals closest to a shark:</p>\n\n<pre><code>=&gt; SELECT name FROM animals WHERE name != 'shark' ORDER BY embedding &lt;-&gt; (SELECT embedding FROM animals WHERE name = 'shark') LIMIT 5;\n name \n-----------\n crocodile\n dolphin\n whale\n turtle\n alligator\n(5 rows)\n</code></pre>\n\n<p>It works! It's worth noting that the model that we used is based on words appearing together in Wikipedia articles, and different models or source corpuses likely yield different results. The results here are also limited to the hundred or so animals that I added to my database.</p>\n<h2 class=\"anchored\">\n  <a name=\"pgvector-optimization-and-performance-considerations\" href=\"#pgvector-optimization-and-performance-considerations\">pgvector Optimization and Performance Considerations</a>\n</h2>\n\n<p>As you add more vector data to your database, you may notice performance issues or slowness in performing queries. You can index vector data like other columns in Postgres, and pgvector provides a few ways to do so, but there are some important considerations to keep in mind:</p>\n\n<ul>\n<li>Adding an index causes pgvector to switch to using approximate nearest neighbor search instead of exact nearest neighbor search, possibly causing a difference in query results.</li>\n<li>Indexing functions are based on distance calculations, so create one based on the calculation you plan to rely on the most in your application.</li>\n<li>There are two index types supported, IVFFlat and HNSW. Before you add an IVFFlat index, make sure you have some data in your table for better recall.</li>\n</ul>\n\n<p>Check out the <a href=\"https://github.com/pgvector/pgvector#indexing\">pgvector documentation</a> for more information on indexing and other performance considerations.</p>\n<h2 class=\"anchored\">\n  <a name=\"collaborate-and-share-your-pgvector-projects\" href=\"#collaborate-and-share-your-pgvector-projects\">Collaborate and Share Your pgvector Projects</a>\n</h2>\n\n<p>Now that pgvector for Heroku Postgres is out in the world, we're really excited to hear what you do with it! One of pgvector's great advantages is that it lets vector data live alongside all the other data you might already have in Postgres. You can add an embedding column to your existing tables and start experimenting. Our <a href=\"https://blog.heroku.com/pgvector-launch\">launch blog</a> for this feature includes a lot of ideas and possible use cases for how to use this new tool, and I'm sure you can come up with many more. If you have questions, our <a href=\"https://help.heroku.com/\">Support team</a> is available to assist. Don't forget you can share your solutions using the <a href=\"https://devcenter.heroku.com/articles/heroku-button\">Heroku Button</a> on your repo. If you feel like blogging on your success, tag us on social media and we would love to read about it!</p>","PublishedAt":"2023-11-15 18:42:51+00:00","OriginURL":"https://blog.heroku.com/pgvector-for-similarity-search-on-heroku-postgres","SourceName":"Heroku"}},{"node":{"ID":4934,"Title":"Router 2.0: The Road to Beta","Description":"<p>Last month, Heroku announced the <a href=\"https://devcenter.heroku.com/changelog-items/2682\">beta release of Router 2.0</a>, the new Common Runtime router! </p>\n\n<p>As part of our commitment to infrastructure modernization, Heroku is making upgrades to the <a href=\"https://devcenter.heroku.com/articles/dyno-runtime#common-runtime\">Common Runtime</a> routing layer. The <a href=\"https://devcenter.heroku.com/changelog-items/2682\">beta release of Router 2.0</a> is an important step along this journey. We’re excited to give you an inside look at all we’ve been doing to get here. </p>\n\n<p>In both the Common Runtime and <a href=\"https://devcenter.heroku.com/articles/dyno-runtime#private-spaces-runtime\">Private Spaces</a>, the <a href=\"https://devcenter.heroku.com/articles/how-heroku-works#http-routing\">Heroku router</a> is responsible for serving requests to customers’ web dynos. In 2024, Router 2.0 will replace the existing Common Runtime router. We’re being transparent about this project so that you, our customers, are motivated to try out Router 2.0 now, while it’s in beta. As an early adopter, you can help us validate that things are working as they should, particularly for <em>your</em> apps and <em>your</em> use cases. You’ll also be first in line to try out the new features we’re planning to add, like <a href=\"https://github.com/heroku/roadmap/issues/34\">HTTP/2</a>.</p>\n<h2 class=\"anchored\">\n  <a name=\"why-a-new-router\" href=\"#why-a-new-router\">Why a New Router?</a>\n</h2>\n\n<p>Now, you may be asking, why build a new router instead of improving the existing one? Our primary motivator has been faster and safer delivery of new routing features for our customers. For a couple of reasons, this has been difficult to achieve with the Common Runtime’s legacy routing layer.</p>\n\n<p>The current Common Runtime router is written in Erlang. It’s built around a <a href=\"https://blog.heroku.com/vegur-free-software\">custom HTTP server library</a> that supports Heroku-specific features, such as <a href=\"https://devcenter.heroku.com/articles/error-codes\">H-codes</a>, <a href=\"https://devcenter.heroku.com/articles/eco-dyno-hours#dyno-sleeping\">dyno sleeping</a>, and <a href=\"https://devcenter.heroku.com/articles/http-routing#heroku-router-log-format\">router logs</a>. For over 10 years, this router, dubbed “Hermes” internally, has served all requests to Heroku’s Common Runtime. At the time of Hermes’ launch, Erlang was an appropriate choice since the language places emphasis on concurrency, scalability, and fault tolerance. In addition, Erlang offers a powerful process introspection toolchain that has served our networking engineers well when <a href=\"https://blog.heroku.com/erlang-in-anger\">debugging in-memory state issues</a>. Our engineers embraced the language fully, also choosing to write the previous version of our logging system, <a href=\"https://blog.heroku.com/logging-on-heroku\">Logplex</a>, in Erlang. </p>\n\n<p>However, as the years passed, development on the Hermes codebase proved difficult. The popularity of Erlang within Heroku began to taper off. The open-source and internal libraries that Hermes depends on stopped receiving the volume of contributions they once had. Productivity declined due to these factors, making significant router upgrades risky. After a few failed upgrade attempts, our team decided to pin the software versions of relevant Erlang components. This action wasn’t without trade-offs. Being pinned to an old version of Erlang became a blocker to delivering now common-place features like <a href=\"https://github.com/heroku/roadmap/issues/34\">HTTP/2</a>. Thus, we decided to put Hermes into maintenance mode and focus on its replacement.</p>\n<h2 class=\"anchored\">\n  <a name=\"choosing-a-language\" href=\"#choosing-a-language\">Choosing a Language</a>\n</h2>\n\n<p>Before kicking off design sessions, our team discussed what broader goals we had for the replacement. In establishing our priorities, the team came to a consensus around three main goals:</p>\n\n<ul>\n<li>\n<strong>Write the router in a language everyone on our team knows well</strong>. With Erlang knowledge limited to just a couple of engineers on the team, we wanted to rewrite the router in a different language. That language had to be something our team already knew well.</li>\n<li>\n<strong>Write the router in a language with a strong open-source community.</strong> A robust community unlocks the ability to quickly adopt new specs, write features, fix bugs, and respond to CVEs. It also expands the candidate pool when it comes time to hire new engineers.</li>\n<li>\n<strong>Share as much code as possible between the Common Runtime and Private Spaces routers.</strong> Since the Common Runtime and Private Spaces routers share most of the same features, there’s no reason for the codebases to differ much. Additionally, it’s faster and easier to deliver a feature if we only have to write it once.</li>\n</ul>\n\n<p>With these goals in mind, the language to choose for Router 2.0 was clear — Go.</p>\n\n<p>Not only is the Private Spaces router already written in Go, but the language has become our standard choice for developing new components of Heroku’s runtime. This story isn’t at all unique. Many others in the DevOps and cloud hosting world today have chosen Go for its performance, built-in concurrency handling, automatic garbage collection — the list goes on. Simply put, it’s a language designed specifically for building big dynamic distributed systems. Because of these factors, the Go community outside and within Heroku has flourished, with Go expertise in abundance across our runtime engineering teams.</p>\n\n<p>Today, by writing Router 2.0 in Go, we’re creating a piece of software to which everyone on our team can contribute. Furthermore, by doubling down on the language of the Private Spaces router, we unify the source code and routing behavior of these two products. Historically, these codebases have been entirely distinct, meaning that any implementation our engineers introduce must be written twice. To combat this, we’ve extracted the common functionality of the two routers into an internal HTTP library. With a unified codebase, the delivery of features and fixes becomes faster and simpler, reducing the cognitive burden on our engineers who operate and maintain the routers.</p>\n\n<p>Developing the router is only half the story, though. The other half is about introducing this service to the world as safely and seamlessly as possible.</p>\n<h2 class=\"anchored\">\n  <a name=\"architecture\" href=\"#architecture\">Architecture</a>\n</h2>\n\n<p>You may recall that back in 2021, Heroku announced the completion of an <a href=\"https://blog.heroku.com/faster-dynos-for-all\">infrastructure upgrade</a> to the Common Runtime that brought customers better performing dynos and lower request latencies. This upgrade involved an extensive migration from our old, “classic” cloud environment to our more performant and secure “sharded” environment. We wanted to complete this migration without disrupting any active traffic or asking customers to change their DNS setups. To do this, our engineers put an <a href=\"https://www.geeksforgeeks.org/open-systems-interconnection-model-osi/\">L4</a> reverse proxy in front of Hermes, straddling the classic and sharded environments. The idea was to slowly shift traffic over to the sharded environments, with the L4 proxy splitting connections to both the classic and the new “in-shard” Hermes instances.</p>\n\n<p>Also a part of this migration, TLS termination on custom domains was transitioned from Hermes to the L4 proxy.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1698271166-IMG_2180.jpeg\" alt=\"IMG_2180\">\nThis L4 proxy is the component that has formed the basis for Router 2.0. Over the past year, our networking team has been developing an L7 router to sit in-memory behind the L4 proxy. Today, the L4 proxy + Router 2.0 process runs alongside Hermes, communicating over the <code>localhost</code> network on our router instances. Putting these two processes side by side, instead of on separate hosts, means we limit the number of network hops between clients and backend dynos.</p>\n<h3 class=\"anchored\">\n  <a name=\"the-strangler-pattern\" href=\"#the-strangler-pattern\">The Strangler Pattern</a>\n</h3>\n\n<p>For apps still on the default routing path, client connections are established with the L4 proxy, which directs traffic through Hermes.\n<img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1698271115-IMG_2488.jpeg\" alt=\"IMG_2488\">\nWhen an <a href=\"https://devcenter.heroku.com/articles/heroku-runtime-router-2-0#enable-router-2-0\">app has Router 2.0 enabled</a>, the L4 proxy instead funnels traffic over an in-memory listener to Router 2.0, then out to the app’s web dynos. Hermes is cut out of the network path.\n<img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1698339992-IMG_5679.jpeg\" alt=\"IMG_5679\">\nThis sort of architecture has a particular name — the “<a href=\"https://www.redhat.com/architect/pros-and-cons-strangler-architecture-pattern\">Strangler pattern</a>” — and it involves inserting a form of middleman between clients and the old system you want to replace. The middleman directs traffic, dividing it between the old system and a new system that is built out incrementally. The major advantage of such a setup is that “big bang” changes or “all-at-once” cut-overs are completely avoided. However, both the old and the new systems live on the same production hot path while the development of the new system is in progress. What has this meant for Router 2.0? Well, we had to lay a complete production-ready foundation early on.</p>\n<h2 class=\"anchored\">\n  <a name=\"living-on-the-hot-path\" href=\"#living-on-the-hot-path\">Living on the Hot Path</a>\n</h2>\n\n<p>Heroku has always been an opinionated hosting and deployment platform that caters to general use cases. In our products, we optimize for stability while delivering innovation. Within the framing of Router 2.0, this commitment to stability meant we had to do a few things <em>before</em> releasing beta.</p>\n<h3 class=\"anchored\">\n  <a name=\"automate-router-deployments\" href=\"#automate-router-deployments\">Automate Router Deployments</a>\n</h3>\n\n<p>Up until recently, deploying Router 2.0 meant creating a new release and manually triggering router fleet cycles across all our production clouds. This process wasn’t only tedious and time-consuming, but it was also really error prone. We fixed this by building out an automation pipeline, outfitted with gates on availability metrics, performance metrics, and smoke tests. Anytime a router release fails on just one of these health indicators, it doesn’t advance to the next stage of deployment.</p>\n<h3 class=\"anchored\">\n  <a name=\"load-test-continuously\" href=\"#load-test-continuously\">Load Test Continuously</a>\n</h3>\n\n<p>An important aspect of vetting the new sharded environments in 2021 was load testing the L4 proxy/Hermes combo. At the time, this was a significant manual undertaking. After manually running these tests, it became obvious that we would need a more practical load testing story while developing Router 2.0. In response, we built a load testing system to continuously push our staging routers to their limits and trigger scaling policies, so that we can also validate our autoscaling setup. This framework has been immensely valuable for Router 2.0 development, catching bugs and regressions before they ever hit production. The results of these load tests feed right back into our deployment pipeline, blocking any deploys that don’t live up to our internal service level objectives.</p>\n<h3 class=\"anchored\">\n  <a name=\"introduce-network-error-logging\" href=\"#introduce-network-error-logging\">Introduce Network Error Logging</a>\n</h3>\n\n<p>Traditionally, routing health has been measured through the use of “checkee” apps. These are web-server applications that we deploy across our production Common Runtime clouds and constantly probe from corresponding ”checker“ apps that run in Private Spaces. The checker-checkee duo allows us to mimic and measure our customers’ routing experience. In recent years, the gaps in this model have become more apparent. Namely, our checkees only represent the tiniest fraction of traffic pumping through the router at any given time. In addition, we can’t within our checkers possibly account for all the various client types and configurations that may be used to connect to the platform.</p>\n\n<p>To address the gap, we introduced <a href=\"https://devcenter.heroku.com/changelog-items/2678\">Network Error Logging</a> (NEL) to both Hermes and Router 2.0. It’s an experimental <a href=\"https://www.w3.org/\">W3C</a> standard that enables the measurement of routing layer performance by collecting real-time data about network failures from web browsers. Google Chrome, Microsoft Edge, and certain mobile clients already <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/NEL#browser_compatibility\">support</a> the spec. NEL ensures our engineers maintain a more holistic understanding of the routing experience actually felt by clients.</p>\n<h2 class=\"anchored\">\n  <a name=\"the-future\" href=\"#the-future\">The Future</a>\n</h2>\n\n<p>Completely retiring Hermes will take time. We’re only at the end of the beginning of that journey. As detailed in the <a href=\"https://devcenter.heroku.com/articles/heroku-runtime-router-2-0\">Dev Center article</a>, Router 2.0 isn’t complete yet because it doesn’t support the full list of features on our <a href=\"https://devcenter.heroku.com/articles/http-routing\">HTTP Routing</a> page. We’re working on it. We’ll soon be adding <a href=\"https://github.com/heroku/roadmap/issues/34\">HTTP/2 support</a>, one of the most requested features, to both the Common Runtime and Private Spaces. However, in the Common Runtime, HTTP/2 will only be available when your app is using Router 2.0.</p>\n\n<p>Our aim is to achieve feature parity with Hermes, plus a little more, over the next few months. Once we’re there, we’ll focus on a migration plan that involves flagging apps into Router 2.0 automatically. Much like in the migration from classic environments to sharded environments, we’ll break the process out into phases based on small batches of apps in similar <a href=\"https://devcenter.heroku.com/articles/dyno-types#dyno-tiers-and-mixing-dyno-types\">dyno tiers</a>. This approach gives us time to pause between phases and assess the performance of the new system.</p>\n<h2 class=\"anchored\">\n  <a name=\"participating\" href=\"#participating\">Participating</a>\n</h2>\n\n<p>We hope that you, our customers, can help us validate the new router well before it becomes the default. You can enable Router 2.0 for a Common Runtime app, by running:</p>\n\n<p><code>heroku labs:enable http-routing-2-dot-0 -a &lt;app&gt;</code></p>\n\n<p>If you choose to enroll, you can submit feedback by commenting on the <a href=\"https://github.com/heroku/roadmap/issues/219\">Heroku Public Roadmap item</a> or <a href=\"https://help.heroku.com/tickets/new?id=4\">creating a support ticket</a>.</p>\n<h2 class=\"anchored\">\n  <a name=\"conclusion\" href=\"#conclusion\">Conclusion</a>\n</h2>\n\n<p>Delivering new features to a platform like Heroku is never as simple as flipping an on/off switch. When we deliver something to our customers, there’s always a mountain of behind-the-scenes effort put into it. Simply stated, we write a lot of software to ensure the software that you see works the way it should.</p>\n\n<p>We’re proud of the work we’ve done so far on Router 2.0, and we’re excited for what’s coming next. If you enroll your applications in the beta, keep an eye on the <a href=\"https://devcenter.heroku.com/articles/heroku-runtime-router-2-0\">Router 2.0 Dev Center</a> page and the <a href=\"https://devcenter.heroku.com/changelog\">Heroku Changelog</a>. We’ll be posting updates about new features as they become available.</p>\n\n<p>Thanks for reading and happy coding!</p>","PublishedAt":"2023-10-30 17:00:00+00:00","OriginURL":"https://blog.heroku.com/router-2dot0-the-road-to-beta","SourceName":"Heroku"}},{"node":{"ID":3384,"Title":"More Predictable Shared Dyno Performance","Description":"<p>In this post, we’d like to share an example of the kind of behind-the-scenes work that the Heroku team does to continuously improve the platform based on customer feedback. </p>\n\n<p>The Heroku Common Runtime is one of the best parts of Heroku. It’s the modern embodiment of the principle of computing resource <a href=\"https://en.wikipedia.org/wiki/Time-sharing\">time-sharing</a> pioneered by John McCarthy and later by UNIX, which evolved into the underpinnings of much of modern-day cloud computing. Because Common Runtime resources are safely shared between customers, we can offer dynos very efficiently, participate in the <a href=\"https://blog.heroku.com/github-student-developer-program\">GitHub Student Program</a>, and run the <a href=\"https://www.heroku.com/open-source-credit-program\">Heroku Open Source Credit Program</a>.</p>\n\n<p>We previously allowed individual dynos to burst their CPU use relatively freely as long as capacity was available. This is in the spirit of time-sharing and improves overall resource utilization by allowing some dynos to burst while others are dormant or waiting on I/O.</p>\n\n<p>Liberal bursting has worked well over the years and most customers got excellent CPU performance at a fair price. Some customers using shared dynos occasionally reported degraded performance, however, typically due to “noisy neighbors”: other dynos on the same instance that, because of misconfiguration or malice, used much more than their fair share of the shared resources. This would manifest as random spikes in request response times or even <a href=\"https://devcenter.heroku.com/articles/addressing-h12-errors-request-timeouts\">H12 timeouts</a>.</p>\n\n<p>To help address the problem of noisy neighbors, over the past year Heroku has quietly rolled out improved resource isolation for shared dyno types to ensure more stable and predictable access to CPU resources. Dynos can still burst CPU use, but not as much as before. While less flexible, this will mean fairer and more predictable access to the shared resources backing <code>eco</code>, <code>basic</code>, <code>standard-1X</code>, and <code>standard-2X</code> Dynos. We’re not changing how many dynos run on each instance, we’re only ensuring more predictable and fair access to resources. Also note that Performance, Private, and Shield type Dynos are not affected because they run on dedicated instances.</p>\n\n<p><i>Want to see what we’re working on next or suggest improvements for Heroku? Check out our <a href=\"https://github.com/heroku/roadmap\">roadmap on GitHub</a>! Curious to learn about all the other recent enhancements we’ve made to Heroku? Check out the <a href=\"https://blog.heroku.com/heroku-2022-roundup\">‘22 roundup</a> and  <a href=\"https://blog.heroku.com/heroku-feedback-news-2023-q1\">Q1 ’23 News</a> blog posts.</i></p>","PublishedAt":"2023-04-11 18:00:05+00:00","OriginURL":"https://blog.heroku.com/more-predictable-shared-dyno-performance","SourceName":"Heroku"}},{"node":{"ID":350,"Title":"The Adventures of Rendezvous in Heroku’s New Architecture","Description":"<p><em>This article was originally authored by Srinath Ananthakrishnan, an engineer on the Heroku Runtime Networking Team</em> </p>\n<h2 class=\"anchored\">\n  <a name=\"summary\" href=\"#summary\">Summary</a>\n</h2>\n\n<p>This following story outlines a recent issue we saw with migrating one of our internal systems over to a new EC2 substrate and in the process breaking one of our customer’s use cases. We also outline how we went about discovering the root of the issue, how we fixed it, and how we enjoyed solving a complex problem that helped keep the Heroku customer experience as simple and straightforward as possible!</p>\n<h2 class=\"anchored\">\n  <a name=\"history\" href=\"#history\">History 📖</a>\n</h2>\n\n<p>Heroku has been leveraging AWS and EC2 since the very early days. All these years, the <a href=\"https://devcenter.heroku.com/articles/dyno-runtime#common-runtime\">Common Runtime</a> has been running on <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-classic-platform.html\">EC2 Classic</a> and while there have always been talks about moving to the more performant and feature rich VPC architecture that AWS offers, we hadn’t had the time and personnel investment to make it a reality until very recently. The results of that effort were captured in a previous blog post titled <a href=\"https://blog.heroku.com/faster-dynos-for-all\">Faster Dynos for All</a></p>\n\n<p>While our Common Runtime contains many critical components, including our instance fleet to run app containers, our routers and several other control plane components, one of the often overlooked but yet critical components is Rendezvous, our bidirectional proxy server that enables <a href=\"https://devcenter.heroku.com/articles/one-off-dynos\">Heroku Run</a> sessions to containers. This is the component that lets customers run what are called one-off dynos that are used for a wide range of use-cases ranging from a simple prompt to execute/test a piece of code to complex CI scenarios.</p>\n<h2 class=\"anchored\">\n  <a name=\"architecture-of-rendezvous\" href=\"#architecture-of-rendezvous\">Architecture of Rendezvous 💪</a>\n</h2>\n\n<p>Rendezvous has been a single-instance server from time immemorial. It is a sub-200 line Ruby script that runs on an EC2 instance with an EIP attached to it. The ruby process receives TLS connections directly, performs TLS termination and proxies bidirectional connections that match a given hash.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1642548509-Screen%20Shot%202021-12-07%20at%2011.16.39%20AM.png\" alt=\"Screen Shot 2021-12-07 at 11\"></p>\n\n<p>Every Heroku Run/One-off dyno invocation involves two parties - the client which is usually the Heroku CLI or custom implementations that use the <a href=\"https://devcenter.heroku.com/articles/platform-api-reference\">Heroku API</a> and the dyno on one of Heroku’s instances deep in the cloud. The existence of Rendezvous is necessitated by one of the painful yet essential warts of the Internet - NATs.</p>\n\n<p>Both the client and the dyno are behind NATs and there’s no means for them to talk to each other through these pesky devices. To combat this, the Heroku API returns an <code>attach_url</code> as part of the <code>create_dyno</code> <a href=\"https://devcenter.heroku.com/articles/platform-api-reference#dyno-create\">request</a> which lets the client reach the dyno. The <code>attach_url</code> also contains a 64 bit hash to identify this specific session in Rendezvous. The same <code>attach_url</code> with the exact hash is passed on by our dyno management system to an agent that runs on our EC2 instance fleet which is responsible for the lifecycle of dynos.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1642548531-Screen%20Shot%202021-12-07%20at%2011.57.32%20AM.png\" alt=\"Screen Shot 2021-12-07 at 11\"></p>\n\n<p>Once both the systems receive the <code>attach_url</code> with the same hash, they make a TLS request to the host, which is a specific instance of Rendezvous. Once the TLS session is established, both sides send the hash as the first message which lets Rendezvous identify which session the connection belongs to. Once the two sides of the session are established, Rendezvous splices them together, thus creating a bi-directional session between the CLI/user and the dyno.</p>\n<h2 class=\"anchored\">\n  <a name=\"a-unique-use-case-of-rendezvous\" href=\"#a-unique-use-case-of-rendezvous\">A unique use-case of Rendezvous 💡</a>\n</h2>\n\n<p>While the majority of customers use Rendezvous via <code>heroku run</code> commands executed via the CLI, some clients have more sophisticated ways of needing containers to be started arbitrarily via the <a href=\"https://devcenter.heroku.com/articles/platform-api-reference#dyno-create\">Heroku API</a>. These clients programmatically create a dyno via the API and also establish a session to the <code>attach_url</code>.</p>\n\n<p>One of our customers utilized Rendezvous in a very unique way by running an app in a Private Space that received client HTTP requests and within the context of a request, issued another request to the Heroku API and to Rendezvous. They had a requirement to support requests across multiple customers and to ensure isolation between them, they opted to run each of their individual customer’s requests inside <a href=\"https://devcenter.heroku.com/articles/one-off-dynos#one-off-dynos\">one-off dynos</a>. The tasks in the one-off dyno runs are expected to take a few seconds and were usually well within the expected maximum response time limit of <a href=\"https://devcenter.heroku.com/articles/limits#http-timeouts\">30s by the Heroku router</a>.</p>\n<h2 class=\"anchored\">\n  <a name=\"oh-something-s-broken\" href=\"#oh-something-s-broken\">Oh! Something’s broken!</a>\n</h2>\n\n<p>In July 2021, we moved Rendezvous into AWS VPCs as part of our effort to evacuate EC2 classic. We chose similar generation instances for this as our instance in classic. As part of this effort, we also wanted to remove a few of the architectural shortcomings of rendezvous - having a single EIP ingress and also manual certificate management for terminating TLS.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1642548541-Screen%20Shot%202022-01-07%20at%201.36.18%20PM.png\" alt=\"Screen Shot 2022-01-07 at 1\"></p>\n\n<p>Based on experience with other routing projects, we decided to leverage <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\">Network Load Balancers</a> that AWS offers. From a performance perspective, these were also significantly better - our internal tests revealed that NLBs offered 5-7x more throughput in comparison to the EIP approach. We also decided to leverage the NLB’s TLS termination capabilities which allowed us to stop managing our own certificate and private key manually and rely on AWS ACM to take care of renewals in the future.</p>\n\n<p>While the move was largely a success and most customers didn’t notice this and their <code>heroku run</code> sessions continued to work after the transition, our unique customer immediately hit <a href=\"https://devcenter.heroku.com/articles/error-codes#h12-request-timeout\">H12s</a> on their app that spawns one-off dynos. Almost immediately, we identified this issue to Rendezvous sessions taking longer than the 30s limit imposed by the Heroku Router. We temporarily switched their app to use the classic path and sat down to investigate.</p>\n<h2 class=\"anchored\">\n  <a name=\"where-s-the-problem\" href=\"#where-s-the-problem\">Where’s the problem! 😱</a>\n</h2>\n\n<p>Our first hunch was that the TLS termination on the NLB wasn’t happening as expected but our investigations revealed that TLS was appropriately terminated and the client was able to make progress following that. The next line of investigation was in Rendezvous itself. The new VPC-based instances were supposed to be faster, so the slowdown was something of a mystery. We even tried out an instance type that supported 100Gbps networking but the issue persisted. As part of this effort, we also had upgraded the Ruby version that Rendezvous was running on - and you guessed it right - we attempted a downgrade as well. This proved to be inconclusive as well.</p>\n\n<p>All along we also suspected this could possibly be a problem in the language runtime of the recipient of the connection, where the bytes were available in the userspace buffer of the runtime but the API call was not notified or there is a race condition. We attempted to mimic the data pattern between the client and the process in the one-off dyno by writing our own sample applications. We actually built sample applications in two different languages with very different runtimes. Both these ended up having the same issues in the new environment as well. 🤔</p>\n\n<p>We even briefly considered altering the Heroku Router’s timeout from 30s, but it largely felt like spinning a roulette wheel since we weren’t absolutely sure where the problem was.</p>\n<h2 class=\"anchored\">\n  <a name=\"nailing-it-down\" href=\"#nailing-it-down\">Nailing it down! 🔍</a>\n</h2>\n\n<p>As part of the troubleshooting effort, we also added some more logging on the agent that runs on every EC2 instance that is responsible for maintaining a connection with Rendezvous and the dyno. This agent negotiates TLS with Rendezvous and establishes a connection and sets up a <code>pty</code> terminal connection on the dyno side and sets up stdin/stdout/stderr channels with the same. The client would send requests in a set-size byte chunks which would be streamed by this agent to the dyno. The same agent would also receive bytes from the dyno and stream it back to Rendezvous to send it back to the client. Through the logs on the agent, we determined that there were logs back and forth indicating traffic between the dyno and Rendezvous when connections worked. However, for the abnormal case, there were no logs indicating traffic coming from the dyno after a while and the last log was bytes being sent to the dyno.</p>\n\n<p>Digging more, we identified two issues with this piece of code:</p>\n\n<ol>\n<li>This piece of code was single threaded - i.e. a single thread was performing an <code>IO.select</code> on the TCP socket on the Rendezvous side and the terminal reader on the dyno.</li>\n<li>While #1 itself is not a terrible problem, it became a problem with the use of NLBs which are more performant and have different TLS frame characteristics.</li>\n</ol>\n\n<p>#2 meant that the NLB could potentially send much larger TLS frames than the classic setup where the Rendezvous ruby process would have performed TLS.</p>\n\n<p>The snippet of code that had the bug was as follows.</p>\n\n<pre><code># tcp_socket can be used with IO.select\n# ssl_socket is after openssl has its say\n# pty_reader and pty_writer are towards the dyno\ndef rendezvous_channel(tcp_socket, ssl_socket, pty_reader, pty_writer)\n    if o = IO.select([tcp_socket, pty_reader], nil, nil, IDLE_TIMEOUT)\n        if o.first.first == pty_reader\n\n            # read from the pty_reader and write to ssl_socket\n\n        elsif o.first.first == tcp_socket\n\n            # read from the ssl_socket and write to pty_writer\n\n        end\n    end\nend\n</code></pre>\n\n<p>Since the majority of the bytes were from the client, this thread would have read from the <code>ssl_socket</code> and written them to the <code>pty_writer</code>. With classic, these would have been small TLS frames which would mean that an <code>IO.select</code> readability notification would correspond to a single read from the SSL socket which would in-turn read from the TCP socket.</p>\n\n<p>However, with the shards, the TLS frames from the NLB end up being larger, and a previous read from the <code>ssl_socket</code> could end up reading more bytes off of the <code>tcp_socket</code> which could potentially block <code>IO.select</code> till the <code>IDLE_TIMEOUT</code> has passed. It’s not a problem if the <code>IDLE_TIMEOUT</code> is a relatively smaller number but since this was larger than the 30s limit imposed by the Heroku Router, <code>IO.select</code> blocking here resulted in that timer elapsing resulting in H12s.</p>\n\n<p>In fact, the <a href=\"https://www.rubydoc.info/stdlib/core/IO.select\">Ruby docs</a> for <code>IO.select</code> specifically talk about this <a href=\"https://www.rubydoc.info/stdlib/core/IO.select#:%7E:text=The%20most%20likely%20situation%20is%20that%20OpenSSL%3A%3ASSL%3A%3ASSLSocket%20buffers%20some%20data.%20IO.select%20doesn%27t%20see%20the%20buffer.%20So%20IO.select%20can%20block%20when%20OpenSSL%3A%3ASSL%3A%3ASSLSocket%23readpartial%20doesn%27t%20block.\">issue</a>.</p>\n\n<blockquote>\n<p>The most likely situation is that OpenSSL::SSL::SSLSocket buffers some data. IO.select doesn't see the buffer. So IO.select can block when OpenSSL::SSL::SSLSocket#readpartial doesn't block.</p>\n</blockquote>\n\n<p>According to the Linux kernel on the instance, there were no bytes to be read from the <code>tcp_socket</code> while there were still bytes being left to read from the buffers in openssl since we only read partially the last time around.</p>\n<h2 class=\"anchored\">\n  <a name=\"the-fix\" href=\"#the-fix\">The fix 💜</a>\n</h2>\n\n<p>Once we had identified the issue, it was rather straightforward for us to fix this. We made the code dual threaded - one each for one side of the connection and also fixed the way we read from the sockets and did an <code>IO.select</code>. With this code change, we ensured that we wouldn’t perennially block where there are bytes lying around to be read.</p>\n\n<p>We deployed this fix to our staging environments and after thorough testing we moved the customer over to the VPC-based rendezvous. The customer subsequently confirmed that the issue was resolved and all our remote offices erupted in roars of cheer after that. It was 🍰 time.</p>\n<h2 class=\"anchored\">\n  <a name=\"conclusion\" href=\"#conclusion\">Conclusion</a>\n</h2>\n\n<blockquote>\n<p>Computers are fun, computers are hard!</p>\n\n<p>Try to run a platform and you’ll often say, oh my god!</p>\n\n<p>Gratifying and inspiring it is, to run our stack</p>\n\n<p>For if you lose their trust, it’s hard to get it back ...</p>\n</blockquote>\n\n<p>Running a platform makes you appreciate more of <a href=\"https://www.hyrumslaw.com/\">Hyrum’s Law</a>, every day. Customers find interesting ways to use your platform and they sure do keep you on your toes to ensure you provide the best in class service. At Heroku we have always taken pride in our mission to make life easy for customers and we are grateful to have got the opportunity to demonstrate that yet again as part of this endeavor.</p>\n\n<p>Thanks are in order for all the folks who tirelessly worked on identifying this issue and fixing it. In alphabetical order - David Murray, Elizabeth Cox, Marcus Blankenship, Srinath Ananthakrishnan, Thomas Holmes, Tilman Holschuh and Will Farrington.</p>","PublishedAt":"2022-01-19 17:00:00+00:00","OriginURL":"https://blog.heroku.com/adventures-of-rendevous","SourceName":"Heroku"}},{"node":{"ID":351,"Title":"Building a Monorepo with Yarn 2","Description":"<p>In true JavaScript fashion, there was no shortage of releases in the JavaScript ecosystem this year. This includes the Yarn project’s release of Yarn 2 with a compressed cache of JavaScript dependencies, including a Yarn binary to reference,  that can be used for a zero-install deployment. </p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1608668413-yarn.png\" alt=\"Ball of yarn and knitting needles illustration\"></p>\n\n<p>Yarn is a package manager that also provides developers a project management toolset. Now, Yarn 2 is now officially supported by Heroku, and Heroku developers are able to take advantage of leveraging zero-installs during their Node.js builds. We’ll go over a popular use case for Yarn that is enhanced by Yarn 2: using workspaces to manage dependencies for your monorepo.</p>\n\n<p>We will cover taking advantage of Yarn 2’s cache to manage monorepo dependencies. Prerequisites for this include a development environment with Node installed. To follow these guides, set up an existing Node project that makes use of a <code>package.json</code> too. If you don’t have one, use the <a href=\"https://github.com/heroku/node-js-getting-started\">Heroku Getting Started with Node.js Project</a>.</p>\n<h2 class=\"anchored\">\n  <a name=\"workspaces\" href=\"#workspaces\">Workspaces</a>\n</h2>\n\n<p>First off, what are workspaces? Workspaces is Yarn’s solution to a monorepo structure for a JavaScript app or Node.js project. A monorepo refers to a project, in this case, a JavaScript project, that has more than one section of the code base. For example, you may have the following set up:</p>\n\n<pre><code class=\"language-javascript\">/app\n - package.json\n - /server\n   - package.json\n - /ui\n   - package.json\n</code></pre>\n\n<p>Your JavaScript server has source code, but there’s an additional front end application that will be built and made available to users separately. This is a popular pattern for setting up a separation of concerns with a custom API client, a build or testing tool, or something else that may not have a place in the application logic. Each of the subdirectory’s <code>package.json</code> will have their own dependencies. How can we manage them? How do we optimize caching? This is where Yarn workspaces comes in.</p>\n\n<p>In the root <code>package.json</code>, set up the subdirectories under the <code>workspaces</code> key. You should add this to your <code>package.json</code>:</p>\n\n<pre><code class=\"language-javascript\">\"workspaces\": [\n    \"server\",\n    \"ui\"\n]\n</code></pre>\n\n<p>For more on workspaces, visit here: <a href=\"https://yarnpkg.com/features/workspaces\">https://yarnpkg.com/features/workspaces</a></p>\n\n<p>Additionally, add the <code>workspaces-tools</code> plugin. This will be useful when running workspace scripts that you’ll use later. You can do this by running:</p>\n\n<pre><code class=\"language-javascript\">yarn plugin import workspace-tools\n</code></pre>\n<h2 class=\"anchored\">\n  <a name=\"setting-up-yarn\" href=\"#setting-up-yarn\">Setting up Yarn</a>\n</h2>\n\n<p>If you’re already using Yarn, you have a <code>yarn.lock</code> file already checked into your code base’s git repository. There’s other files and directories that you’ll need up to set up the cache. If you aren’t already using Yarn, install it globally.</p>\n\n<pre><code class=\"language-javascript\">npm install -g yarn\n</code></pre>\n\n<p><em>Note: If you don’t have Yarn &gt;=1.22.10 installed on your computer, update it with the same install command.</em></p>\n\n<p>Next, set up your Yarn version for this code base. One of the benefits of using Yarn 2 is that you’ll have a checked in Yarn binary that will be used by anyone that works on this code base and eliminates version conflicts between environments.</p>\n\n<pre><code class=\"language-javascript\">yarn set version berry\n</code></pre>\n\n<p>A <code>.yarn</code> directory and <code>.yarnrc.yml</code> file will both be created that need to be checked into git. These are the files that will set up your project’s local Yarn instance.</p>\n<h2 class=\"anchored\">\n  <a name=\"setting-up-the-dependency-cache\" href=\"#setting-up-the-dependency-cache\">Setting Up the Dependency Cache</a>\n</h2>\n\n<p>Once Yarn is set up, you can set up your cache. Run yarn install:</p>\n\n<pre><code class=\"language-javascript\">yarn\n</code></pre>\n\n<p>Before anything else, make sure to add the following to the <code>.gitignore</code>:</p>\n\n<pre><code class=\"language-javascript\"># Yarn\n.yarn/*\n!.yarn/cache\n!.yarn/releases\n!.yarn/plugins\n!.yarn/sdks\n!.yarn/versions\n</code></pre>\n\n<p>The files that are ignored will be machine specific, and the remaining files you’ll want to check in. If you run <code>git status</code>, you’ll see the following:</p>\n\n<pre><code class=\"language-javascript\">Untracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n    .gitignore\n    .pnp.js\n    .yarn/cache/\n    yarn.lock\n</code></pre>\n\n<p>You’ve created new files that will speed up your install process:</p>\n\n<ul>\n<li>\n<code>.pnp.js</code> - This is the Plug’n’Play (PnP) file. The PnP file tells your Node app or build how to find the dependencies that are stored in <code>.yarn/cache</code>.</li>\n<li>\n<code>.yarn/cache</code> - This directory will have the dependencies that are needed to run and build your app.</li>\n<li>\n<code>yarn.lock</code> - The lock file still is used to lock the versions that are resolved from the <code>package.json</code>.</li>\n</ul>\n\n<p>Check all of this in to git, and you’re set. For more information about Yarn 2’s zero-install philosophy, read here: <a href=\"https://yarnpkg.com/features/zero-installs\">https://yarnpkg.com/features/zero-installs</a></p>\n<h2 class=\"anchored\">\n  <a name=\"adding-dependencies-to-subdirectories\" href=\"#adding-dependencies-to-subdirectories\">Adding Dependencies to Subdirectories</a>\n</h2>\n\n<p>Now that Yarn and the cache are set up, we can start adding dependencies. As initially shown, we have a <code>server</code> directory and a <code>ui</code> directory. We can assume that each of these will be built and hosted differently. For example, my server is written in TypeScript, using Express.js for routing, and running on a Heroku web dyno. For the front end app, it is using Next.js. The build will be run during the app’s build process.</p>\n\n<p>Add <code>express</code> to the server <code>dependencies</code>. </p>\n\n<pre><code class=\"language-javascript\">yarn workspace server add express\n</code></pre>\n\n<p>Additionally, add <code>@types/express</code> and <code>typescript</code> to the <code>devDependencies</code>. You can use the <code>-D</code> flag to indicate that you’re adding <code>devDependencies</code>. </p>\n\n<pre><code class=\"language-javascript\">yarn workspace server add @types/express typescript -D\n</code></pre>\n\n<p>We now have our dependencies in our <code>server</code> workspace. We just need to create our <code>ui</code> workspace. Next, build a Next.js app with the <code>yarn create</code> command.</p>\n\n<pre><code class=\"language-javascript\">yarn create next-app ui\n</code></pre>\n\n<p>Finally, run <code>yarn</code> again to update the cache and check these changes into git.</p>\n<h2 class=\"anchored\">\n  <a name=\"running-scripts-with-workspaces\" href=\"#running-scripts-with-workspaces\">Running Scripts with Workspaces</a>\n</h2>\n\n<p>The last piece is to run scripts within the workspaces. If you look through your source code, you’ll see that there’s one global cache for all dependencies under your app’s root directory. Run the following to see all the compressed dependencies:</p>\n\n<pre><code class=\"language-javascript\">ls .yarn/cache\n</code></pre>\n\n<p>Now, lets run build scripts with workspaces. First, set up the workspace. For server, use <code>tsc</code> to build the TypeScript app. You’ll need to set up a TypeScript config and a <code>.ts</code> file first:</p>\n\n<pre><code class=\"language-javascript\">cd server\nyarn dlx --package typescript tsc --init\ntouch index.ts\n</code></pre>\n\n<p><code>yarn dlx</code> will run a command from a package so that it doesn’t need to be installed globally. It’s useful for one-off initializing commands, like initializing a TypeScript app.</p>\n\n<p>Next, add the build step to the <code>server/package.json</code>.</p>\n\n<pre><code class=\"language-javascript\">\"scripts\": {\n    \"build\": \"tsc\",\n    \"start\": \"node index.js\"\n},\n</code></pre>\n\n<p>Change directories back to the application level, and run the build.</p>\n\n<pre><code class=\"language-javascript\">cd ..\nyarn workspace server build\n</code></pre>\n\n<p>You’ll see that a <code>server/index.js</code> file is created. Add <code>server/*.js</code> to the <code>.gitignore</code>.</p>\n\n<p>Since we already have <code>build</code> and <code>start</code> scripts in our Next.js app (created by the <code>yarn create</code> command), add a build script at the root level <code>package.json</code>.</p>\n\n<pre><code class=\"language-javascript\">\"scripts\": {\n    \"build\": \"yarn workspaces foreach run build\"\n},\n</code></pre>\n\n<p>This is when the <code>workspaces-tool</code> plugin is used. Run <code>yarn build</code> from your app’s root, and both of your workspaces will build. Open a second terminal, and you’ll be able to run <code>yarn workspace server start</code> and <code>yarn workspace ui start</code> in each terminal and run the Express and Next servers in parallel.</p>\n<h2 class=\"anchored\">\n  <a name=\"deploy-to-heroku\" href=\"#deploy-to-heroku\">Deploy to Heroku</a>\n</h2>\n\n<p>Finally, we can deploy our code to Heroku. Since Heroku will run the script is in the <code>package.json</code> under <code>start</code>, add a script to the <code>package.json</code>.</p>\n\n<pre><code class=\"language-javascript\">\"scripts\": {\n    \"build\": \"yarn workspaces foreach run build\",\n    \"start\": \"yarn workspaces server start\"\n},\n</code></pre>\n\n<p><a href=\"https://devcenter.heroku.com/articles/deploying-nodejs#specifying-a-start-script\">Heroku will use the <code>start</code> script</a> from the <code>package.json</code> to start the <code>web</code> process on your app.</p>\n<h2 class=\"anchored\">\n  <a name=\"conclusion\" href=\"#conclusion\">Conclusion</a>\n</h2>\n\n<p>There are <a href=\"https://yarnpkg.com/features\">plenty more features</a> that Yarn, and specifically Yarn 2, offers that are useful for Heroku developers. Check out the Yarn docs to see if there are additional workspace features that may work nicely with Heroku integration. As always, if you have any feedback or issues, please <a href=\"https://github.com/heroku/heroku-buildpack-nodejs/issues/new/choose\">open an Issue on GitHub</a>.</p>","PublishedAt":"2020-12-22 20:53:08+00:00","OriginURL":"https://blog.heroku.com/building-a-monorepo-with-yarn-2","SourceName":"Heroku"}},{"node":{"ID":352,"Title":"Extend Flows with Heroku Compute: An Event-Driven Pattern","Description":"<p><em>This post <a href=\"https://medium.com/salesforce-architects/extend-flows-with-heroku-compute-an-event-driven-pattern-a9840a91ce5b\">previously appeared</a> on the Salesforce Architects blog.</em></p>\n\n<p>Event-driven application architectures have proven to be effective for implementing enterprise solutions using loosely coupled services that interact by exchanging asynchronous events. Salesforce enables event-driven architectures (EDAs) with Platform Events and Change Data Capture (CDC) events as well as triggers and Apex callouts, which makes the Salesforce Platform a great way to build all of your <a href=\"https://www.salesforce.com/products/platform/best-practices/understanding-digital-customer-experience/\">digital customer experiences</a>. This post is the first in a series that covers various EDA patterns, considerations for using them, and examples deployed on the Salesforce Platform.</p>\n<h2 class=\"anchored\">\n  <a name=\"expanding-the-event-driven-architecture-of-the-salesforce-platform\" href=\"#expanding-the-event-driven-architecture-of-the-salesforce-platform\">Expanding the event-driven architecture of the Salesforce Platform</a>\n</h2>\n\n<p>Back in April, Frank Caron wrote a <a href=\"https://developer.salesforce.com/blogs/2020/04/event-driven-app-architecture-on-the-customer-360-platform.html\">blog post</a> describing the power of EDAs. In it, he covered the event-driven approach and the benefits of loosely coupled service interactions. He focused mainly on use cases where events triggered actions across platform services as well as how incorporating third-party external services can greatly expand the power of applications developed using declarative low-code tools like Salesforce Flow.</p>\n\n<p>As powerful as flows can be for accessing third-party services, even greater power comes when your own custom applications, running your own business logic on the Salesforce Platform, are part of flows. </p>\n\n<p>API-first, event-driven <a href=\"https://medium.com/adobetech/three-principles-of-api-first-design-fa6666d9f694\">design</a> is the kind of development that frequently requires collaboration across different members of you team. Low-code builders with domain expertise who are familiar with the business requirements can build the flows. Programmers are typically necessary to develop the back-end services that implement the business logic. An enterprise architect may get involved as well to design the service APIs.</p>\n\n<p>However you are organized, you will need to expose your services with APIs and enable them to produce and consume events. The Salesforce Platform enables this with the <a href=\"https://www.salesforce.com/video/1771211/\">Salesforce Event Bus</a>, <a href=\"https://developer.salesforce.com/blogs/2019/11/introducing-salesforce-evergreen.html\">Salesforce Functions</a>, and <a href=\"https://developer.salesforce.com/docs/atlas.en-us.228.0.api_streaming.meta/api_streaming/intro_stream.htm\">Streaming API</a> as well as support for OpenAPI specification for external services. </p>\n\n<p>Heroku capabilities on the Salesforce Platform include event streaming, relational data stores, and key-value caches seamlessly integrated with elastic compute. These capabilities, combined with deployment automation and hands-off operational excellence, lets your developers focus entirely on delivering your unique business requirements. Seamless integration with the rest of Salesforce makes your apps deployed on Heroku the foundation for complete, compelling, economical, secure, and successful solutions.</p>\n\n<p>This post focuses on expanding flows with Heroku compute. Specifically, how to expose Heroku apps as external services and securely access them via flows using Flow Builder as the low-code development environment. Subsequent posts will expand this idea to include event-driven interactions between Heroku apps and the rest of the Salesforce Platform as well as other examples of how Salesforce Platform based EDAs address common challenges we see across many of our customers including:</p>\n\n<ul>\n<li>Multi-organization visibility and reporting</li>\n<li>Shared event bus designs</li>\n<li>B2C apps with Lightning Web Components</li>\n</ul>\n<h2 class=\"anchored\">\n  <a name=\"building-salesforce-flows-with-your-own-business-logic\" href=\"#building-salesforce-flows-with-your-own-business-logic\">Building Salesforce flows with your own business logic</a>\n</h2>\n\n<p>Salesforce external services are a great way to access third-party services from a flow. All you need are the services’ OpenAPI spec schema (OAS schema), and you’re set to go. There are some great examples of how to register your external services <a href=\"https://andyinthecloud.com/2017/07/23/simplified-api-integrations-with-external-services/\">here</a>, with a more detailed example of how to generate an Apex client and explore your schema <a href=\"https://andyinthecloud.com/2017/09/30/swagger-open-api-salesforce-like/\">here</a>. </p>\n\n<p>But what if you want to incorporate custom business logic into your flow app? What if you wanted to extend and complement the declarative programming model of flows with an imperative model with full programming semantics? What if you wanted to make your app available to flow developers in other organizations, or possibly accessed as a stand-alone service behind a Lightning Web Components based app?</p>\n\n<p>This kind of service deployment typically requires off-platform development, bringing with it all the complexity and operational overhead that goes with meeting the scalability, availability, and reliability requirements of your business critical apps. </p>\n\n<p>The following steps show you how you can deploy your own apps using Heroku on the Salesforce Platform without any of this operational overhead. We’re going to walk through an example of how to build and deploy custom business logic into your own service and access it in a flow. Deployment will be via a Heroku app, which brings the power and flexibility to write your own code, without having to worry about the operational burden of production app deployment or DevOps toolchains. </p>\n\n<p>This approach works well in scenarios where you have programmers and low-code builders working together to deploy a new app. The team first collaborates on what the app needs to do and defines the API that a flow can access. Once the API is designed, this specification then becomes the <a href=\"https://swagger.io/blog/api-strategy/benefits-of-openapi-api-development/\">contract</a> between the two teams. As progress is made on each side, they iterate, perfect their design, and ultimately deliver the app. All the code used for this example is available on <a href=\"https://github.com/chrismarino/flowapp\">GitHub</a>, so that you can try it out for yourself.</p>\n\n<p><em>Note: Apex is a great way to customize Salesforce, but there are times when a standalone app might be the better way to go. If your team prefers Python, Node, or some other programming language, or perhaps you already have an app running on premises or in the cloud, and you want to run it all within the secure perimeter of the Salesforce Platform, a standalone Heroku app is the way to go.</em></p>\n<h2 class=\"anchored\">\n  <a name=\"api-spec-defines-the-interface\" href=\"#api-spec-defines-the-interface\">API spec defines the interface</a>\n</h2>\n\n<p>The example application an on-line shopping site that lets users login, browse products, and make a purchase. We’ll describe the process of building out this app in a number of posts, but for this first part we’ll simply build a flow and an external service that lists products and updates inventory in Salesforce. For the API, we’re using a sample API available on <a href=\"https://swagger.io/tools/swaggerhub/\">Swagger Hub</a>. There are a variety of tools and systems that can do this, including the MuleSoft <a href=\"https://www.mulesoft.com/platform/api-design\">Anypoint Platform API Designer</a>. For this example, however, we’re using this simple shopping cart spec to bootstrap the API design and provide the initial application stub for development. </p>\n\n<p>From the API spec, API Portals can produce server side application stubs to jumpstart application development. In this example, we’ve downloaded the node.js API stub as the starting point for API and app development. We’ve also modified the code so that it can run on Heroku by adding a <a href=\"https://devcenter.heroku.com/articles/procfile\">Procfile</a> and changing the port configuration. If you want to try it yourself, you can <a href=\"https://heroku.com/deploy?template=https://github.com/chrismarino/flowapp\">Deploy to Heroku</a>, or click the Deploy to Heroku button on the <a href=\"https://github.com/chrismarino/flowapp/blob/master/README.md\">GitHub</a> page. </p>\n\n<p>Let’s begin by looking at the initial <a href=\"http://swagger-shop.herokuapp.com/docs/#/Internal_calls\">API spec</a> for the application. These API docs are being served from a deployment of the app stub on Heroku. The actual YAML spec (which we will modify later in this post) is included in the <a href=\"https://github.com/chrismarino/flowapp/blob/master/api/swagger.yaml\">repo as well</a>.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1607639439-1%2AYhqGS0YRXgD3lAYVqyuhQQ.png\" alt=\"\"></p>\n\n<p>As you can see in the spec, there are definitions for each of the methods that specify which parameters are required and what the response payload will look like. Since this is a valid OpenAPI spec, we can register this API as an external service as described in <a href=\"https://trailhead.salesforce.com/en/content/learn/modules/external-services/get-started-with-external-services\">Get Started with External Services</a>.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1607639550-1%2A5NKtci6bYvqVp1DHex7VfQ.png\" alt=\"\"></p>\n<h2 class=\"anchored\">\n  <a name=\"external-service-authorization\" href=\"#external-service-authorization\">External service authorization</a>\n</h2>\n\n<p>The flow needs a Named Credential in Salesforce to access the external service. Salesforce offers many alternatives for how the app can use the Named Credential including per-user credentials that can help you track and control access. For this example, though, we’re going to use a single login for all flow access using basic HTTP authentication.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1607639589-1%2AU2DxWullRClXrHinOJ7_6g.png\" alt=\"\"></p>\n\n<p>You can find the code that implements this method is in the app included in the <a href=\"https://github.com/chrismarino/flowapp/blob/master/controllers/InternalCalls.js#L6\">repo</a>. </p>\n\n<p>App access to the organization is authorized via a Salesforce JWT account token and implemented in the app in <a href=\"https://github.com/chrismarino/flowapp/blob/master/service/SFAuthService.js\">SFAuthService.js</a>:</p>\n\n<pre><code class=\"lang-javascript\">'use strict';\n\nconst jwt = require('salesforce-jwt-bearer-token-flow');\nconst jsforce = require('jsforce');\n\nrequire('dotenv').config();\n\nconst { SF_CONSUMER_KEY, SF_USERNAME, SF_LOGIN_URL } = process.env;\n\nlet SF_PRIVATE_KEY = process.env.SF_PRIVATE_KEY;\nif (!SF_PRIVATE_KEY) {\n    SF_PRIVATE_KEY = require('fs').readFileSync('private.pem').toString('utf8');\n}\n\nexports.getSalesforceConnection = function () {\n    return new Promise(function (resolve, reject) {\n        jwt.getToken(\n            {\n                iss: SF_CONSUMER_KEY,\n                sub: SF_USERNAME,\n                aud: SF_LOGIN_URL,\n                privateKey: SF_PRIVATE_KEY\n            },\n            (err, tokenResponse) =&gt; {\n                if (tokenResponse) {\n                    let conn = new jsforce.Connection({\n                        instanceUrl: tokenResponse.instance_url,\n                        accessToken: tokenResponse.access_token\n                    });\n                    resolve(conn);\n                } else {\n                    reject('Authentication to Salesforce failed');\n                }\n            }\n        );\n    });\n};\n</code></pre>\n\n<p>The private key is configured in Heroku as a configuration variable and is installed when the app is deployed.</p>\n<h2 class=\"anchored\">\n  <a name=\"register-the-external-service-methods\" href=\"#register-the-external-service-methods\">Register the external service methods</a>\n</h2>\n\n<p>Individual methods for the ShoppingService external service are easily added to a flow just as they would be for any external service. Here we’ve added the Get Products and Get Order methods, as shown in Flow Builder below. But since Flow Builder can register an external service method using only the API spec, they are just references to the stub methods that we still need to build out. We’ll program something for them later in this post.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1607639692-1%2A5uLV0XtFHdqwubLxGn74UA.png\" alt=\"\"></p>\n\n<p>These are familiar steps to anyone that has registered an external service for a low, but if you want more detail on how to do this, check out the <a href=\"https://trailhead.salesforce.com/en/content/learn/modules/external-services/get-started-with-external-services\">Get Started with External Services Trailhead</a>.</p>\n<h2 class=\"anchored\">\n  <a name=\"define-the-api-and-build-out-the-methods\" href=\"#define-the-api-and-build-out-the-methods\">Define the API and build out the methods</a>\n</h2>\n\n<p>With the authorizations in place and the methods defined, we are now ready to build out the external service in a way that meets our company’s specific needs. For this, we need to implement each of the API methods. </p>\n\n<p>To illustrate this, here is the Node function that has been stubbed out by the API for the Get Order method. It is here that your business logic is implemented.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1607639765-1%2AM8e6Ge18JmmIMe0hHTUBKw.png\" alt=\"\"></p>\n\n<p>For each of the API methods, we’ve implemented some simple logic that we will use to test interactions with the flow. For example, here’s the code for getting a list of all orders:</p>\n\n<pre><code class=\"lang-javascript\">/**\n * Get list of all orders for the user\n *\n * type String json or xml\n * pOSTDATA List Creates a new employee in DB (optional)\n * returns List\n **/\nexports.typePost_orderPOST = function(type,pOSTDATA) {\n  return new Promise(function(resolve, reject) {\n    var examples = {};\n    examples['application/json'] = [ {\n  \"Item Total Price\" : 1998.0,\n  \"Order Item ID\" : 643,\n  \"Order ID\" : 298,\n  \"Total Order Price\" : 3996.0\n}, {\n  \"Item Total Price\" : 1998.0,\n  \"Order Item ID\" : 643,\n  \"Order ID\" : 298,\n  \"Total Order Price\" : 3996.0\n} ];\n    if (Object.keys(examples).length &gt; 0) {\n      resolve(examples[Object.keys(examples)[0]]);\n    } else {\n      resolve();\n    }\n  });\n}\n</code></pre>\n\n<p>You can examine the code that implements each of the methods in the repo:</p>\n\n<ul>\n<li>Get Products Method</li>\n<li>Post Order Method</li>\n<li>Get Order Method</li>\n<li>Get Orders Method</li>\n</ul>\n\n<p>Now that we have some simple logic executing in each of these methods, we can build a simple flow that logs in using the Named Credential, accesses the external service, and returns product data.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1607639841-1%2AnmANPe2Z75KWm1M76YLjCg.png\" alt=\"\"></p>\n\n<p>Running this flow shows the product data from the stub app. The successful display of product data here indicates that the flow has been able to successfully log in to the app, call the Get Product method, and get the proper response.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1607639862-1%2AMPICEeMp8XjIMsyNRkTL7Q.png\" alt=\"\"></p>\n<h2 class=\"anchored\">\n  <a name=\"update-the-api\" href=\"#update-the-api\">Update the API</a>\n</h2>\n\n<p>So now that we have our basic flow defined and accessing the app, we can complete the API with new methods necessary for the app to do what it needs to do. </p>\n\n<p>Let’s imagine that the app has up-to-date product inventory data and we want to use that data to update the Product object in Salesforce with the current quantity in stock. For this, the app would need to be able to access Salesforce and update the Product object. </p>\n\n<p>To do this, the flow needs to make a request to a Get Inventory method. But that method does not yet exist. However, we can modify the API to include any new methods we need. Here our teams work together to determine what the flow needs and what methods are necessary in the app. </p>\n\n<p>After discussion, we determine that a single Get Inventory method will satisfy the requirements. So, now we update the API spec to include a new <a href=\"https://github.com/chrismarino/flowapp/blob/ExternalService/api/swagger.yaml#L174-L205\">method</a>:</p>\n\n<pre><code class=\"lang-yaml\">/{type}/get_inventory/:\n    get:\n      tags:\n      - \"Internal calls\"\n      description: \"Get Inventory\"\n      operationId: \"typeGet_inventoryGET\"\n      consumes:\n      - \"application/json\"\n      produces:\n      - \"application/json\"\n      - \"application/xml\"\n      parameters:\n      - name: \"type\"\n        in: \"path\"\n        description: \"json or xml\"\n        required: true\n        type: \"string\"\n      - name: \"product_id\"\n        in: \"query\"\n        description: \"Product Id\"\n        required: true\n        type: \"integer\"\n      responses:\n        \"200\":\n          description: \"OK\"\n          schema:\n            type: \"array\"\n            items:\n              $ref: \"#/definitions/Inventory\"\n      security:\n      - basic: []\n</code></pre>\n\n<p>With this updated API, we can update the external service so that we can use it in a flow.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1607639924-1%2A8I3LMY2IrvvZXItY219arg.png\" alt=\"\"></p>\n\n<p>And with the updated API spec, we can automatically generate a stub method as well. From the empty stub method we can <a href=\"https://github.com/chrismarino/flowapp/blob/ExternalService/service/InternalCallsService.js#L4-L25\">complete the function</a> with the necessary logic to access Salesforce directly and update the Product object. Note that it uses the SFAuthService.js code from above and an API token to access the organization data.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1607639965-1%2AVALT9oPUv4gQEocwMuxhgA.png\" alt=\"\"></p>\n<h2 class=\"anchored\">\n  <a name=\"platform-events-and-eda\" href=\"#platform-events-and-eda\">Platform events and EDA</a>\n</h2>\n\n<p>Now that this inventory method is available, we can check the operation with a simple flow that triggers on a Platform Event and updates the Product object. When we run this test flow, it updates the iPhone Product object in the organization.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1607639993-1%2A9w82sZ_SIfNW1Ve_S-A0TA.png\" alt=\"\"></p>\n\n<p>How and when the flow might need to update the product inventory would be up to the actual business needs. However, triggering the flow to update the object can be done using Platform Events. The flow can respond to any Platform Event with a call to the Get Inventory method.</p>\n<h2 class=\"anchored\">\n  <a name=\"deploy-the-business-logic\" href=\"#deploy-the-business-logic\">Deploy the business logic</a>\n</h2>\n\n<p>The process described in this post can go on until the flow and app API converge on a stable design. Once stable, our programmer and low-code builder can complete their work independently to complete the app. The flow designer can build in all the decision logic that surrounds the flow and build out screens for the user to interact with.</p>\n\n<p>Separately, and independently from the flow designer, the programmer can code each of the methods in the stub app to implement the business logic.</p>\n<h2 class=\"anchored\">\n  <a name=\"summary\" href=\"#summary\">Summary</a>\n</h2>\n\n<p>We’ve just started building out this app and running it as an external service. In this post, however, you’ve already seen the the basic steps that would be part of every development cycle: defining an API, registering methods that a flow can call, building out the stub app, and authorizing access for the flow, app, and Platform Event triggers. </p>\n\n<p>Future posts in this series will take these basic elements and methodology to expand the flow to execute the business logic contained in the app via user interface elements for a complete process automation solution running entirely on the Salesforce Platform.</p>\n\n<p>To learn more, see the <a href=\"https://trailhead.salesforce.com/en/content/learn/modules/heroku_enterprise_baiscs?trail_id=heroku_enterprise\">Heroku Enterprise Basics Trailhead module</a> and the <a href=\"https://trailhead.salesforce.com/en/content/learn/modules/flow-basics\">Flow Basics Trailhead module</a>. Please share your feedback with <a href=\"https://twitter.com/SalesforceArchs\">@SalesforceArchs</a> on Twitter.</p>","PublishedAt":"2020-12-11 16:30:00+00:00","OriginURL":"https://blog.heroku.com/extend-flows-heroku-event-driven","SourceName":"Heroku"}},{"node":{"ID":353,"Title":"Incident Response at Heroku","Description":"<p><em>This post is an update on a <a href=\"https://blog.heroku.com/incident-response-at-heroku\">previous post</a> about how Heroku handles incident response.</em></p>\n\n<p>As a service provider, when things go wrong, you try to get them fixed as quickly as possible. In addition to technical troubleshooting, there’s a lot of coordination and communication that needs to happen in resolving issues with systems like Heroku’s.</p>\n\n<p>At Heroku we’ve codified our practices around these aspects into an incident response framework. Whether you’re just interested in how incident response works at Heroku, or looking to adopt and apply some of these practices for yourself, we hope you find this inside look helpful.</p>\n<h2 class=\"anchored\">\n  <a name=\"incident-response-and-the-incident-commander-role\" href=\"#incident-response-and-the-incident-commander-role\">Incident Response and the Incident Commander Role</a>\n</h2>\n\n<p>We describe Heroku’s incident response framework below. It’s based on the <a href=\"https://en.wikipedia.org/wiki/Incident_Command_System\">Incident Command System</a> used in natural disaster response and other emergency response fields. Our response framework and the Incident Commander role in particular help us to successfully respond to a variety of incidents.</p>\n\n<p>When an incident occurs, we follow these steps:</p>\n<h3 class=\"anchored\">\n  <a name=\"page-an-incident-commander\" href=\"#page-an-incident-commander\">Page an Incident Commander</a>\n</h3>\n\n<p>They will assess the issue, and decide if it’s worth investigating further</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1601548057-Slack%20Page.png\" alt=\"Slack Page\"></p>\n<h3 class=\"anchored\">\n  <a name=\"move-to-a-dedicated-chat-room\" href=\"#move-to-a-dedicated-chat-room\">Move to a dedicated chat room</a>\n</h3>\n\n<p>The Incident Commander creates a new room in Slack, to centralize all the information for this specific incident</p>\n<h3 class=\"anchored\">\n  <a name=\"update-public-status-site\" href=\"#update-public-status-site\">Update public status site</a>\n</h3>\n\n<p>Our customers want information about incidents as quickly as possible, even if it is preliminary. As soon as possible, the IC designates someone to take on the communications role (“comms”) with a first responsibility of updating the <a href=\"https://status.heroku.com/\">status site</a> with our current understanding of the incident and how it’s affecting customers. The admin section of Heroku’s status site helps the comms operator to get this update out quickly:</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1601548128-public-post.png\" alt=\"public-post\"></p>\n\n<p>The status update then appears on <a href=\"https://status.heroku.com/\">status.heroku.com</a> and is sent to customers and internal communication channels via SMS, email, and Slack bot. It also shows on twitter:</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1601548203-twitter-post.png\" alt=\"twitter-post\"></p>\n<h3 class=\"anchored\">\n  <a name=\"send-out-internal-situation-report\" href=\"#send-out-internal-situation-report\">Send out internal Situation Report</a>\n</h3>\n\n<p>Next the IC compiles and sends out the first situation report (“sitrep”) to the internal team describing the incident. It includes what we know about the problem, who is working on it and in what roles, and open issues. As the incident evolves, the sitrep acts as a concise description of the current state of the incident and our response to it. A good sitrep provides information to active incident responders, helps new responders get quickly up to date about the situation, and gives context to other observers like customer support staff.</p>\n\n<p>The Heroku status site has a form for the sitrep, so that the IC can update it and the public-facing status details at the same time. When a sitrep is created or updated, it’s automatically distributed internally via email and Slack bot. A versioned log of sitreps is also maintained for later review:</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1601548277-sitrep2.png\" alt=\"sitrep2\">\n<img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1601548298-Sitrep-emails.png\" alt=\"Sitrep-emails\"></p>\n<h3 class=\"anchored\">\n  <a name=\"assess-problem\" href=\"#assess-problem\">Assess problem</a>\n</h3>\n\n<p>The next step is to assess the problem in more detail. The goals here are to gain better information for the public status communication (e.g. what users are affected and how, what they can do to work around the problem) and more detail that will help engineers fix the problem (e.g. what internal components are affected, the underlying technical cause). The IC collects this information and reflects it in the sitrep so that everyone involved can see it. It includes the severity, going from SEV0 (critical disruption), to SEV4 (minor feature impacted)</p>\n<h3 class=\"anchored\">\n  <a name=\"mitigate-problem\" href=\"#mitigate-problem\">Mitigate problem</a>\n</h3>\n\n<p>Once the response team has some sense of the problem, it will try to mitigate customer-facing effects if possible. For example, we may put the Platform API in maintenance mode to reduce load on infrastructure systems, or boot additional instances in our fleet to temporarily compensate for capacity issues. A successful mitigation will reduce the impact of the incident on customer apps and actions, or at least prevent the customer-facing issues from getting worse.</p>\n<h3 class=\"anchored\">\n  <a name=\"coordinate-response\" href=\"#coordinate-response\">Coordinate response</a>\n</h3>\n\n<p>In coordinating the response, the IC focuses on bringing in the right people to solve the problem and making sure that they have the information they need. The IC can use a Slack bot to page in additional teams as needed (the page will route to the on-call person for that team), or page teams directly.</p>\n<h3 class=\"anchored\">\n  <a name=\"manage-ongoing-response\" href=\"#manage-ongoing-response\">Manage ongoing response</a>\n</h3>\n\n<p>As the response evolves, the IC acts as an information radiator to keep the team informed about what’s going on. The IC will keep track of who’s active on the response, what problems have been solved and are still open, the current resolution methods being attempted, when we last communicated with customers, and reflect this back to the team regularly with the sitrep mechanism. Finally, the IC is making sure that nothing falls through the cracks: that no problems go unaddressed and that decisions are made in a timely manner.</p>\n<h3 class=\"anchored\">\n  <a name=\"post-incident-cleanup\" href=\"#post-incident-cleanup\">Post-incident cleanup</a>\n</h3>\n\n<p>Once the immediate incident has been resolved, the IC calls for the team to unwind any temporary changes made during the response. For example, alerts may have been silenced and need to be turned back on. The team double-checks that all monitors are green and that all incidents in PagerDuty have been resolved.</p>\n<h3 class=\"anchored\">\n  <a name=\"post-incident-follow-up\" href=\"#post-incident-follow-up\">Post-incident follow-up</a>\n</h3>\n\n<p>Finally, the Production Engineering Department will tee up a post-incident follow up. Depending on the severity of the incident, this could be a quick discussion in the normal weekly operational review or a dedicated internal post-mortem with associated public post-mortem post. The post-mortem process often informs changes that we should make to our infrastructure, testing, and process; these are tracked over time within engineering as incident remediation items.</p>\n<h2 class=\"anchored\">\n  <a name=\"when-everything-goes-south\" href=\"#when-everything-goes-south\">When everything goes south</a>\n</h2>\n\n<p>As Heroku is part of the Salesforce Platform, we leverage Salesforce Incident Response, and Crisis communication center when things gets really bad.</p>\n\n<p>If the severity decided by the IC is SEV1 or worse, Salesforce’s Critical Incident Center (CIC) gets involved. Their role is to assist the Heroku Incident Commander with support around customer communication, and keep the executives informed of the situation. They also can engage the legal teams if needed, mostly for customer communication.</p>\n\n<p>In the case where the incident is believed to be a SEV0 ( major disruption for example ), the Heroku Incident Commander can also request assistance from the Universal Command (UC) Leadership. They will help to assess the issue, and determine if the incident really rises to the level of Sev 0. </p>\n\n<p>Once it is determined to be the case, the UC will spin up a conference call ( called  bridge ) involving executives, in order for them to have a single source of truth to follow-up on the incident’s evolution. One of the goals is that executives don’t first learn failures from outsides sources. This may seem obvious, but amidst the stress of a significant incident when we're solely focused on fixing a problem impacting customers, it's easy to overlook communicating status to those not directly involved with solving the problem. They are also much better suited to answer to customers requests, and keep them informed of the incident response.</p>\n<h2 class=\"anchored\">\n  <a name=\"incident-response-in-other-fields\" href=\"#incident-response-in-other-fields\">Incident Response in Other Fields</a>\n</h2>\n\n<p>The incident response framework described above draws from decades of related work in emergency response: natural disaster response, firefighting, aviation, and other fields that need to manage response to critical incidents. We try to learn from this body of work where possible to avoid inventing our incident response policy from first principles.\nTwo areas of previous work particularly influenced how we approach incident response:</p>\n<h3 class=\"anchored\">\n  <a name=\"incident-command-system\" href=\"#incident-command-system\">Incident Command System</a>\n</h3>\n\n<p>Our framework draws most directly from the <a href=\"http://en.wikipedia.org/wiki/Incident_Command_System\">Incident Command System</a> used to manage natural disaster and other large-scale incident responses. This prior art informs our Incident Commander role and our explicit focus on facilitating incident response in addition to directly addressing the technical issues.</p>\n<h3 class=\"anchored\">\n  <a name=\"crew-resource-management\" href=\"#crew-resource-management\">Crew Resource Management</a>\n</h3>\n\n<p>The ideas of <a href=\"http://en.wikipedia.org/wiki/Crew_resource_management\">Crew Resource Management</a> (a different “CRM”) originated in aviation but have since been successfully applied to other fields such as medicine and firefighting. We draw lessons on communication, leadership, and decision-making from CRM into our incident response thinking.\nWe believe that learning from fields outside of software engineering is a valuable practice, both for operations and other aspects of our business.</p>\n<h2 class=\"anchored\">\n  <a name=\"summary\" href=\"#summary\">Summary</a>\n</h2>\n\n<p>Heroku’s incident response framework helps us quickly resolve issues while keeping customers informed about what’s happening. We hope you’ve found these details about our incident response framework interesting and that they may even inspire changes in how you think about incident response at your own company.\nAt Heroku we’re continuing to learn from our own experiences and the work of others in related fields. Over time this will mean even better incident response for our platform and better experiences for our customers.</p>","PublishedAt":"2020-10-08 12:53:43+00:00","OriginURL":"https://blog.heroku.com/incident-response-at-heroku-2020","SourceName":"Heroku"}},{"node":{"ID":354,"Title":"How I Broke `git push heroku main`","Description":"<p>Incidents are inevitable. Any platform, large or small will have them. While resiliency work will definitely be an important factor in reducing the number of incidents, hoping to remove all of them (and therefore reach 100% uptime) is not an achievable goal.</p>\n\n<p>We should, however, learn as much as we can from incidents, so we can avoid repeating them.</p>\n\n<p>In this post, we will look at one of those incidents, <a href=\"https://status.heroku.com/incidents/2105\">#2105</a>, see how it happened (spoiler: I messed up), and what we’re doing to avoid it from happening again (spoiler: I’m not fired).</p>\n\n<!-- more -->\n<h2 class=\"anchored\">\n  <a name=\"git-push-inception\" href=\"#git-push-inception\">Git push inception</a>\n</h2>\n\n<p>Our Git server is a component written in Go which can listen for HTTP and SSH connections to process a Git command.\nWhile we try to run all our components as Heroku apps on our platform just like Heroku customers, this component is different, as it has several constraints which make it unsuitable for running on the Heroku platform. Indeed, Heroku currently only provides HTTP routing, so it can’t handle incoming SSH connections.</p>\n\n<p>This component is therefore hosted as a “kernel app” using an internal framework which mimics the behavior of Heroku, but runs directly on virtual servers.</p>\n\n<p>Whenever we deploy new code for this component, we will mark instances running the previous version of the code as poisoned. They won’t be able to receive new requests but will have the time they need to finish processing any ongoing requests (every Git push is one request, and those can take up to one hour).\nOnce they don’t have any active requests open, the process will stop and restart using the new code.</p>\n\n<p>When all selected instances have been deployed to, we can move to another batch, and repeat until all instances are running the new code.</p>\n<h2 class=\"anchored\">\n  <a name=\"it-was-such-a-nice-morning\" href=\"#it-was-such-a-nice-morning\">It was such a nice morning</a>\n</h2>\n\n<p>On September 3, I had to deploy a change to switch from calling one internal API endpoint to another. It included a new authentication method between components.</p>\n\n<p>This deploy was unusual because it required setting a new configuration variable, which includes the following manual actions:</p>\n\n<ol>\n<li>Set the new config variable with the framework handling our instances</li>\n<li>Run a command to have the new config variable transmitted to every instance</li>\n<li>Trigger the deploy so the config variables starts being used</li>\n</ol>\n\n<p>So, on that morning, I started deploying our staging instance. I set the new configuration variable on both staging and production.\nThen, I had the config variables transmitted to every instance, but only in staging as I figured I’d avoid touching production right now.\nFinally, I kicked off the staging deployment, and started monitoring that everything went smoothly, which it did.</p>\n\n<p>A few hours later, I went on to production.</p>\n<h2 class=\"anchored\">\n  <a name=\"houston-we-have-a-problem\" href=\"#houston-we-have-a-problem\">Houston, we have a problem</a>\n</h2>\n\n<p>I started my production deployment. Since I had set the configuration variable earlier, I went straight to deploying the new code.</p>\n\n<p>You may see what I did wrong now.</p>\n\n<p>So my code change went to a batch of instances. I didn’t move to another batch though, as I was about to go to lunch. There was no rush to move forward right away, especially since deploying every instance can take several hours.</p>\n\n<p>So I went to lunch, but came back a few minutes later as an alert had gone off.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1601509354-Screenshot%202020-09-09%20at%2010.24.13.png\" alt=\"Screenshot 2020-09-09 at 10\"></p>\n\n<p>The spike you can see on this graph is HTTP 401 responses.</p>\n\n<p>If you read carefully the previous section, you may have noticed that I set the new configuration variable in production, but didn’t apply it to the instances.\nSo my deploy to a batch of servers didn’t have the new configuration variable, meaning we were making unauthenticated calls to a private API, which gave us 401 responses. Hence the 401s being sent back publicly.</p>\n\n<p>Once I realized that, I ran the script to transmit the configuration variables to the instances, killed the impacted processes, which restarted using the updated configuration variables, and the problem was resolved.</p>\n<h2 class=\"anchored\">\n  <a name=\"did-i-mess-up\" href=\"#did-i-mess-up\">Did I mess up?</a>\n</h2>\n\n<p>An untrained eye could say “wow, you messed up bad. Why didn’t you run that command?”, and they would be right. Except they actually wouldn’t.</p>\n\n<p>The problem isn’t that I forgot to run one command. It’s that the system has allowed me to go forward with the deployment when it could have helped me avoid the issue.</p>\n\n<p>Before figuring out any solution, the real fix is to do a truly blameless retrospective. If we had been blaming me for forgetting to run a command instead of focusing on why the system still permitted the deployment, I would probably have felt unsafe reporting this issue, and we would not have been able to improve our systems so that this doesn’t happen again.</p>\n\n<p>Then we can focus on solutions. In this specific case, we are going to merge the two steps of updating configuration variables and deploying code into a single step.\nThat way there isn’t an additional step to remember to run from time to time.</p>\n\n<p>If we didn’t want or were unable to merge the two steps, we could also have added a safeguard in the form of a confirmation warning if we’re trying to deploy the application’s code while configuration variables aren’t synchronized.</p>\n<h2 class=\"anchored\">\n  <a name=\"computers-are-dumb-but-they-don-t-make-mistakes\" href=\"#computers-are-dumb-but-they-don-t-make-mistakes\">Computers are dumb, but they don’t make mistakes</a>\n</h2>\n\n<p>Relying on humans to perform multiple manual actions,  especially when some of them are only required rarely (we don’t change configuration variables often) is a recipe for incidents.</p>\n\n<p>Our job as engineers is to  build systems that avoid those human flaws, so we can do our human job of thinking about new things, and computers can do theirs: performing laborious and repetitive tasks.</p>\n\n<p>This incident shows how a blameless culture benefits everyone in a company (and customers!). Yes, I messed up. But the fix is to improve the process, not to assign blame. We can’t expect folks to be robots who never make mistakes. Instead, we need to build a system that’s safe enough so those mistakes can’t happen.</p>","PublishedAt":"2020-10-01 15:30:00+00:00","OriginURL":"https://blog.heroku.com/how-i-broke-git-push-heroku-main","SourceName":"Heroku"}},{"node":{"ID":355,"Title":"The Life-Changing Magic of Tidying Ruby Object Allocations","Description":"<p>Your app is slow. It does not spark joy. This post will use memory allocation profiling tools to discover performance hotspots, even when they're coming from inside a library. We will use this technique with a real-world application to identify a piece of optimizable code in Active Record that ultimately leads to a patch with a substantial impact on page speed.</p>\n\n<p>In addition to the talk, I've gone back and written a full technical recap of each section to revisit it any time you want without going through the video.</p>\n\n<p>I make heavy use of theatrics here, including a Japanese voiceover artist, animoji, and some edited clips of Marie Kondo's Netflix TV show. This recording was done at EuRuKo on a boat. If you've got the time, here's the talk:</p>\n\n<div class=\"embedded-video-wrapper\">\n<iframe src=\"https://www.youtube-nocookie.com/embed/Aczy01drwkg?start=287\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n<ol>\n<li><a href=\"#intro-to-tidying-object-allocations\">Intro to Tidying Object Allocations</a></li>\n<li><a href=\"#tidying-example-1-active-record-respond_to-logic\">Tidying Example 1: Active Record respond_to? logic</a></li>\n<li><a href=\"#performance-and-statistical-significance\">Performance and Statistical Significance</a></li>\n<li><a href=\"#tidying-example-2-converting-strings-to-time-takes-time\">Tidying example 2: Converting strings to time takes time</a></li>\n<li>\n<a href=\"tidying-example-3-lightning-fast-cache-keys\">Tidying Example 3: Lightning fast cache keys</a> </li>\n</ol>\n<h2 class=\"anchored\">\n  <a name=\"intro-to-tidying-object-allocations\" href=\"#intro-to-tidying-object-allocations\">Intro to Tidying Object Allocations</a>\n</h2>\n\n<p>The core premise of this talk is that we all want faster applications. Here I'm making the pitch that you can get significant speedups by focusing on your object allocations. To do that, I'll eventually show you a few real-world cases of PRs I made to Rails along with a \"how-to\" that shows how I used profiling and benchmarking to find and fix the hotspots.</p>\n\n<p>At a high level, the \"tidying\" technique looks like this:</p>\n\n<ol>\n<li>Take all your object allocations and put them in a pile where you can see them</li>\n<li>Consider each one: Does it spark joy?</li>\n<li>Keep only the objects that spark joy</li>\n</ol>\n\n<p>An object sparks joy if it is useful, keeps your code clean, and does not cause performance problems. If an object is absolutely necessary, and removing it causes your code to crash, it sparks joy.</p>\n\n<p>To put object allocations in front of us we'll use:</p>\n\n<ul>\n<li><a href=\"https://github.com/SamSaffron/memory_profiler\">memory_profiler</a></li>\n<li><a href=\"https://github.com/schneems/derailed_benchmarks\">derailed_benchmarks</a></li>\n</ul>\n\n<p>To get a sense of the cost of object allocation, we can benchmark two different ways to perform the same logic. One of these allocates an array while the other does not.</p>\n\n<pre><code class=\"lang-ruby\">require 'benchmark/ips'\n\ndef compare_max(a, b)\n  return a if a &gt; b\n  b\nend\n\ndef allocate_max(a, b)\n  array = [a, b] # &lt;===== Array allocation here\n  array.max\nend\n\nBenchmark.ips do |x|\n  x.report(\"allocate_max\") {\n    allocate_max(1, 2)\n  }\n  x.report(\"compare_max \") {\n    compare_max(1, 2)\n  }\n  x.compare!\nend\n</code></pre>\n\n<p>This gives us the results:</p>\n\n<pre><code class=\"lang-term\">Warming up --------------------------------------\n        allocate_max   258.788k i/100ms\n        compare_max    307.196k i/100ms\nCalculating -------------------------------------\n        allocate_max      6.665M (±14.6%) i/s -     32.090M in   5.033786s\n        compare_max      13.597M (± 6.0%) i/s -     67.890M in   5.011819s\n\nComparison:\n        compare_max : 13596747.2 i/s\n        allocate_max:  6664605.5 i/s - 2.04x  slower\n</code></pre>\n\n<p>In this example, allocating an array is 2x slower than making a direct comparison. It's a truism in most languages that allocating memory or creating objects is slow. In the <code>C</code> programming language, it's a truism that \"malloc is slow.\"</p>\n\n<p>Since we know that allocating in Ruby is slow, we can make our programs faster by removing allocations. As a simplifying assumption, I've found that a decrease in bytes allocated roughly corresponds to performance improvement. For example, if I can reduce the number of bytes allocated by 1% in a request, then on average, the request will have been sped up by about 1%. This assumption helps us benchmark faster as it's much easier to measure memory allocated than it is to repeatedly run hundreds or thousands of timing benchmarks.</p>\n<h2 class=\"anchored\">\n  <a name=\"tidying-example-1-active-record-code-respond_to-code-logic\" href=\"#tidying-example-1-active-record-code-respond_to-code-logic\">Tidying Example 1: Active Record <code>respond_to?</code> logic</a>\n</h2>\n\n<p>Using the target application <a href=\"https://www.codetriage.com\">CodeTriage.com</a> and derailed benchmarks, we get a \"pile\" of memory allocations:</p>\n\n<pre><code class=\"lang-term\">$ bundle exec derailed exec perf:objects\n\nallocated memory by gem\n-----------------------------------\n    227058  activesupport/lib\n    134366  codetriage/app\n    # ...\n\n\nallocated memory by file\n-----------------------------------\n    126489  …/code/rails/activesupport/lib/active_support/core_ext/string/output_safety.rb\n     49448  …/code/codetriage/app/views/layouts/_app.html.slim\n     49328  …/code/codetriage/app/views/layouts/application.html.slim\n     36097  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb\n     25096  …/code/codetriage/app/views/pages/_repos_with_pagination.html.slim\n     24432  …/code/rails/activesupport/lib/active_support/core_ext/object/to_query.rb\n     23526  …/code/codetriage/.gem/ruby/2.5.3/gems/rack-mini-profiler-1.0.0/lib/patches/db/pg.rb\n     21912  …/code/rails/activerecord/lib/active_record/connection_adapters/postgresql_adapter.rb\n     18000  …/code/rails/activemodel/lib/active_model/attribute_set/builder.rb\n     15888  …/code/rails/activerecord/lib/active_record/result.rb\n     14610  …/code/rails/activesupport/lib/active_support/cache.rb\n     11109  …/code/codetriage/.gem/ruby/2.5.3/gems/rack-mini-profiler-1.0.0/lib/mini_profiler/storage/file_store.rb\n      9824  …/code/rails/actionpack/lib/abstract_controller/caching/fragments.rb\n      9360  …/.rubies/ruby-2.5.3/lib/ruby/2.5.0/logger.rb\n      8440  …/code/rails/activerecord/lib/active_record/attribute_methods.rb\n      8304  …/code/rails/activemodel/lib/active_model/attribute.rb\n      8160  …/code/rails/actionview/lib/action_view/renderer/partial_renderer.rb\n      8000  …/code/rails/activerecord/lib/active_record/integration.rb\n      7880  …/code/rails/actionview/lib/action_view/log_subscriber.rb\n      7478  …/code/rails/actionview/lib/action_view/helpers/tag_helper.rb\n      7096  …/code/rails/actionview/lib/action_view/renderer/partial_renderer/collection_caching.rb\n      # ...\n</code></pre>\n\n<p>The <a href=\"https://gist.github.com/schneems/5ed597c85a0a49659413456652a1befc\">full output is massive</a>, so I've truncated it here.</p>\n\n<p>Once you've got your memory in a pile. I like to look at the \"allocated memory\" by file. I start at the top and look at each in turn. In this case, we'll look at this file:</p>\n\n<pre><code class=\"lang-term\">      8440  …/code/rails/activerecord/lib/active_record/attribute_methods.rb\n</code></pre>\n\n<p>Once you have a file you want to look at, you can focus on it in derailed like this:</p>\n\n<pre><code class=\"lang-term\">$ ALLOW_FILES=active_record/attribute_methods.rb \\\n  bundle exec derailed exec perf:objects\n\nallocated memory by file\n-----------------------------------\n      8440  …/code/rails/activerecord/lib/active_record/attribute_methods.rb\n\nallocated memory by location\n-----------------------------------\n      8000  …/code/rails/activerecord/lib/active_record/attribute_methods.rb:270\n       320  …/code/rails/activerecord/lib/active_record/attribute_methods.rb:221\n        80  …/code/rails/activerecord/lib/active_record/attribute_methods.rb:189\n        40  …/code/rails/activerecord/lib/active_record/attribute_methods.rb:187\n</code></pre>\n\n<p>Now we can see exactly where the memory is being allocated in this file. Starting at the top of the locations, I'll work my way down to understand how memory is allocated and used. Looking first at this line:</p>\n\n<pre><code class=\"lang-term\">      8000  …/code/rails/activerecord/lib/active_record/attribute_methods.rb:270\n</code></pre>\n\n<p>We can open this in an editor and navigate to that location:</p>\n\n<pre><code class=\"lang-term\">$ bundle open activerecord\n</code></pre>\n\n<p>In that file, here's the line allocating the most memory:</p>\n\n<pre><code class=\"lang-ruby\">def respond_to?(name, include_private = false)\n  return false unless super\n\n  case name\n  when :to_partial_path\n    name = \"to_partial_path\"\n  when :to_model\n    name = \"to_model\"\n  else\n    name = name.to_s # &lt;=== Line 270 here\n  end\n\n  # If the result is true then check for the select case.\n  # For queries selecting a subset of columns, return false for unselected columns.\n  # We check defined?(@attributes) not to issue warnings if called on objects that\n  # have been allocated but not yet initialized.\n  if defined?(@attributes) &amp;&amp; self.class.column_names.include?(name)\n    return has_attribute?(name)\n  end\n\n  true\nend\n</code></pre>\n\n<p>Here we can see on line 270 that it's allocating a string. But why? To answer that question, we need more context. We need to understand how this code is used. When we call <code>respond_to</code> on an object, we want to know if a method by that name exists. Because Active Record is backed by a database, it needs to see if a column exists with that name.</p>\n\n<p>Typically when you call <code>respond_to</code> you pass in a symbol, for example, <code>user.respond_to?(:email)</code>. But in Active Record, columns are stored as strings. On line 270, we're ensuring that the <code>name</code> value is always a string.</p>\n\n<p>This is the code where name is used:</p>\n\n<pre><code class=\"lang-ruby\">  if defined?(@attributes) &amp;&amp; self.class.column_names.include?(name)\n</code></pre>\n\n<p>Here <code>column_names</code> returns an array of column names, and the <code>include?</code> method will iterate over each until it finds the column with that name, or its nothing (<code>nil</code>).</p>\n\n<p>To determine if we can get rid of this allocation, we have to figure out if there's a way to replace it without allocating memory. We need to refactor this code while maintaining correctness. I decided to add a method that converted the array of column names into a hash with symbol keys and string values:</p>\n\n<pre><code class=\"lang-ruby\"># lib/activerecord/model_schema.rb\ndef symbol_column_to_string(name_symbol) # :nodoc:\n  @symbol_column_to_string_name_hash ||= column_names.index_by(&amp;:to_sym)\n  @symbol_column_to_string_name_hash[name_symbol]\nend\n</code></pre>\n\n<p>This is how you would use it:</p>\n\n<pre><code class=\"lang-ruby\">User.symbol_column_to_string(:email) #=&gt; \"email\"\nUser.symbol_column_to_string(:foo)   #=&gt; nil\n</code></pre>\n\n<p>Since the value that is being returned every time by this method is from the same hash, we can re-use the same string and not have to allocate. The refactored <code>respond_to</code> code ends up looking like this:</p>\n\n<pre><code class=\"lang-ruby\">def respond_to?(name, include_private = false)\n  return false unless super\n\n  # If the result is true then check for the select case.\n  # For queries selecting a subset of columns, return false for unselected columns.\n  # We check defined?(@attributes) not to issue warnings if called on objects that\n  # have been allocated but not yet initialized.\n  if defined?(@attributes)\n    if name = self.class.symbol_column_to_string(name.to_sym)\n      return has_attribute?(name)\n    end\n  end\n\n  true\nend\n</code></pre>\n\n<p>Running our benchmarks, this patch yielded a reduction in memory of 1%. Using code that eventually became <code>derailed exec perf:library</code>, I verified that the patch made end-to-end request/response page speed on CodeTriage 1% faster.</p>\n<h2 class=\"anchored\">\n  <a name=\"performance-and-statistical-significance\" href=\"#performance-and-statistical-significance\">Performance and Statistical Significance</a>\n</h2>\n\n<p>When talking about benchmarks, it's important to talk about statistics and their impact. I talk a bit about this in <a href=\"https://schneems.com/2020/03/17/lies-damned-lies-and-averages-perc50-perc95-ex%0Aplained-for-programmers/\">Lies, Damned Lies, and Averages: Perc50, Perc95 explained for Programmers</a>. Essentially any time you measure a value, there's a chance that it could result from randomness. If you run a benchmark 3 times, it will give you 3 different results. If it shows that it was faster twice and slower once, how can you be certain that the results are because of the change and not random chance?</p>\n\n<p>That's precisely the question that \"statistical significance\" tries to answer. While we can never know, we can make an informed decision. How? Well, if you took a measurement of the same code many times, you would know any variation was the result of randomness. This would give you a distribution of randomness. Then you could use this distribution to understand how likely it is that your change was caused by randomness.</p>\n\n<p>In the talk, I go into detail on the origins of \"Student's T-Test.\" In derailed, I've switched to using Kolmogorov-Smirnov instead. When I ran benchmarks on CodeTriage, I wanted to be sure that my results were valid, so I ran them multiple times and ran Kolmogorov Smirnov on them. This gives me a confidence interval. If my results are in that interval, then I can say with 95% certainty that my results are not the result of random chance i.e., that they're valid and are statistically significant.</p>\n\n<p>If it's not significant, it could mean that the change is too small to detect, that you need more samples, or that there is no difference.</p>\n\n<p>In addition to running a significance check on your change, it's useful to see the distribution. Derailed benchmarks does this for you by default now. Here is a result from <code>derailed exec perf:library</code> used to compare the performance difference of two different commits in a library dependency:</p>\n\n<pre><code class=\"lang-term\">                  Histogram - [winner] \"I am the new commit.\"\n                           ┌                                        ┐\n            [11.2 , 11.28) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 12\n            [11.28, 11.36) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 22\n            [11.35, 11.43) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 30\n            [11.43, 11.51) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 17\n   Time (s) [11.5 , 11.58) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 13\n            [11.58, 11.66) ┤▇▇▇▇▇▇▇ 6\n            [11.65, 11.73) ┤ 0\n            [11.73, 11.81) ┤ 0\n            [11.8 , 11.88) ┤ 0\n                           └                                        ┘\n                                      # of runs in range\n\n\n\n                  Histogram - [loser] \"Old commit\"\n                           ┌                                        ┐\n            [11.2 , 11.28) ┤▇▇▇▇ 3\n            [11.28, 11.36) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 19\n            [11.35, 11.43) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 17\n            [11.43, 11.51) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 25\n   Time (s) [11.5 , 11.58) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 15\n            [11.58, 11.66) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 13\n            [11.65, 11.73) ┤▇▇▇▇ 3\n            [11.73, 11.81) ┤▇▇▇▇ 3\n            [11.8 , 11.88) ┤▇▇▇ 2\n                           └                                        ┘\n                                      # of runs in range\n</code></pre>\n\n<p>The TLDR of this whole section is that in addition to showing my change as being faster, I was also able to show that the improvement was statistically significant.</p>\n<h2 class=\"anchored\">\n  <a name=\"tidying-example-2-converting-strings-to-time-takes-time\" href=\"#tidying-example-2-converting-strings-to-time-takes-time\">Tidying example 2: Converting strings to time takes time</a>\n</h2>\n\n<p>One percent faster is good, but it could be better. Let's do it again. First, get a pile of objects:</p>\n\n<pre><code class=\"lang-term\">$ bundle exec derailed exec perf:objects\n\n# ...\n\nallocated memory by file\n-----------------------------------\n    126489  …/code/rails/activesupport/lib/active_support/core_ext/string/output_safety.rb\n     49448  …/code/codetriage/app/views/layouts/_app.html.slim\n     49328  …/code/codetriage/app/views/layouts/application.html.slim\n     36097  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb\n     25096  …/code/codetriage/app/views/pages/_repos_with_pagination.html.slim\n     24432  …/code/rails/activesupport/lib/active_support/core_ext/object/to_query.rb\n     23526  …/code/codetriage/.gem/ruby/2.5.3/gems/rack-mini-profiler-1.0.0/lib/patches/db/pg.rb\n     21912  …/code/rails/activerecord/lib/active_record/connection_adapters/postgresql_adapter.rb\n     18000  …/code/rails/activemodel/lib/active_model/attribute_set/builder.rb\n     15888  …/code/rails/activerecord/lib/active_record/result.rb\n     14610  …/code/rails/activesupport/lib/active_support/cache.rb\n     11148  …/code/codetriage/.gem/ruby/2.5.3/gems/rack-mini-profiler-1.0.0/lib/mini_profiler/storage/file_store.rb\n      9824  …/code/rails/actionpack/lib/abstract_controller/caching/fragments.rb\n      9360  …/.rubies/ruby-2.5.3/lib/ruby/2.5.0/logger.rb\n      8304  …/code/rails/activemodel/lib/active_model/attribute.rb\n</code></pre>\n\n<p>Zoom in on a file:</p>\n\n<pre><code class=\"lang-term\">     36097  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb\n</code></pre>\n\n<p>Isolate the file:</p>\n\n<pre><code class=\"lang-term\">$ ALLOW_FILE=active_model/type/helpers/time_value.rb \\\n  bundle exec derailed exec perf:objects\n\nTotal allocated: 39617 bytes (600 objects)\nTotal retained:  0 bytes (0 objects)\n\nallocated memory by gem\n-----------------------------------\n     39617  activemodel/lib\n\nallocated memory by file\n-----------------------------------\n     39617  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb\n\nallocated memory by location\n-----------------------------------\n     17317  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb:72\n     12000  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb:74\n      6000  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb:73\n      4300  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb:64\n</code></pre>\n\n<p>We're going to do the same thing by starting to look at the top location:</p>\n\n<pre><code class=\"lang-term\">     17317  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb:72\n</code></pre>\n\n<p>Here's the code:</p>\n\n<pre><code class=\"lang-ruby\">def fast_string_to_time(string)\n if string =~ ISO_DATETIME # &lt;=== line 72 Here\n   microsec = ($7.to_r * 1_000_000).to_i\n   new_time $1.to_i, $2.to_i, $3.to_i, $4.to_i, $5.to_i, $6.to_i, microsec\n end\nend\n</code></pre>\n\n<p>On line 72, we are matching the input string with a regular expression constant. This allocates a lot of memory because each grouped match of the regular expression allocates a new string. To understand if we can make this faster, we have to understand how it's used.</p>\n\n<p>This method takes in a string, then uses a regex to split it into parts, and then sends those parts to the <code>new_time</code> method.</p>\n\n<p>There's not much going on that can be sped up there, but what's happening on this line:</p>\n\n<pre><code class=\"lang-ruby\">   microsec = ($7.to_r * 1_000_000).to_i\n</code></pre>\n\n<p>Here's the regex:</p>\n\n<pre><code class=\"lang-ruby\">ISO_DATETIME = /\\A(\\d{4})-(\\d\\d)-(\\d\\d) (\\d\\d):(\\d\\d):(\\d\\d)(\\.\\d+)?\\z/\n</code></pre>\n\n<p>When I ran the code and output $7 from the regex match, I found that it would contain a string that starts with a dot and then has numbers, for example:</p>\n\n<pre><code class=\"lang-ruby\">puts $7 # =&gt; \".1234567\"\n</code></pre>\n\n<p>This code wants microseconds as an integer, so it turns it into a \"rational\" and then multiplies it by a million and turns it into an integer.</p>\n\n<pre><code class=\"lang-ruby\">($7.to_r * 1_000_000).to_i # =&gt; 1234567\n</code></pre>\n\n<p>You might notice that it looks like we're basically dropping the period and then turning it into an integer. So why not do that directly?</p>\n\n<p>Here's what it looks like:</p>\n\n<pre><code class=\"lang-ruby\">def fast_string_to_time(string)\n  if string =~ ISO_DATETIME\n    microsec_part = $7\n    if microsec_part &amp;&amp; microsec_part.start_with?(\".\") &amp;&amp; microsec_part.length == 7\n      microsec_part[0] = \"\"         # &lt;=== HERE\n      microsec = microsec_part.to_i # &lt;=== HERE\n    else\n      microsec = (microsec_part.to_r * 1_000_000).to_i\n    end\n    new_time $1.to_i, $2.to_i, $3.to_i, $4.to_i, $5.to_i, $6.to_i, microsec\n  end\n</code></pre>\n\n<p>We've got to guard this case by checking for the conditions of our optimization. Now the question is: is this faster?</p>\n\n<p>Here's a microbenchmark:</p>\n\n<pre><code class=\"lang-ruby\">original_string = \".443959\"\n\nrequire 'benchmark/ips'\n\nBenchmark.ips do |x|\n  x.report(\"multiply\") {\n    string = original_string.dup\n    (string.to_r * 1_000_000).to_i\n  }\n  x.report(\"new     \") {\n    string = original_string.dup\n    if string &amp;&amp; string.start_with?(\".\".freeze) &amp;&amp; string.length == 7\n      string[0] = ''.freeze\n      string.to_i\n    end\n  }\n  x.compare!\nend\n\n# Warming up --------------------------------------\n#             multiply   125.783k i/100ms\n#             new        146.543k i/100ms\n# Calculating -------------------------------------\n#             multiply      1.751M (± 3.3%) i/s -      8.805M in   5.033779s\n#             new           2.225M (± 2.1%) i/s -     11.137M in   5.007110s\n\n# Comparison:\n#             new     :  2225289.7 i/s\n#             multiply:  1751254.2 i/s - 1.27x  slower\n</code></pre>\n\n<p>The original code is 1.27x slower. YAY!</p>\n<h3 class=\"anchored\">\n  <a name=\"tidying-example-3-lightning-fast-cache-keys\" href=\"#tidying-example-3-lightning-fast-cache-keys\">Tidying Example 3: Lightning fast cache keys</a>\n</h3>\n\n<p>The last speedup is kind of underwhelming, so you might wonder why I added it. If you remember our first example of optimizing <code>respond_to</code>, it helped to understand the broader context of how it's used. Since this is such an expensive object allocation location, is there an opportunity to call it less or not call it at all?</p>\n\n<p>To find out, I added a <code>puts caller</code> in the code and re-ran it. Here's part of a backtrace:</p>\n\n<pre><code class=\"lang-term\">====================================================================================================\n…/code/rails/activemodel/lib/active_model/type/date_time.rb:25:in `cast_value'\n…/code/rails/activerecord/lib/active_record/connection_adapters/postgresql/oid/date_time.rb:16:in `cast_value'\n…/code/rails/activemodel/lib/active_model/type/value.rb:38:in `cast'\n…/code/rails/activemodel/lib/active_model/type/helpers/accepts_multiparameter_time.rb:12:in `block in initialize'\n…/code/rails/activemodel/lib/active_model/type/value.rb:24:in `deserialize'\n…/.rubies/ruby-2.5.3/lib/ruby/2.5.0/delegate.rb:349:in `block in delegating_block'\n…/code/rails/activerecord/lib/active_record/attribute_methods/time_zone_conversion.rb:8:in `deserialize'\n…/code/rails/activemodel/lib/active_model/attribute.rb:164:in `type_cast'\n…/code/rails/activemodel/lib/active_model/attribute.rb:42:in `value'\n…/code/rails/activemodel/lib/active_model/attribute_set.rb:48:in `fetch_value'\n…/code/rails/activerecord/lib/active_record/attribute_methods/read.rb:77:in `_read_attribute'\n…/code/rails/activerecord/lib/active_record/attribute_methods/read.rb:40:in `__temp__57074616475646f51647'\n…/code/rails/activesupport/lib/active_support/core_ext/object/try.rb:16:in `public_send'\n…/code/rails/activesupport/lib/active_support/core_ext/object/try.rb:16:in `try'\n…/code/rails/activerecord/lib/active_record/integration.rb:99:in `cache_version'\n…/code/rails/activerecord/lib/active_record/integration.rb:68:in `cache_key'\n…/code/rails/activesupport/lib/active_support/cache.rb:639:in `expanded_key'\n…/code/rails/activesupport/lib/active_support/cache.rb:644:in `block in expanded_key'\n…/code/rails/activesupport/lib/active_support/cache.rb:644:in `collect'\n…/code/rails/activesupport/lib/active_support/cache.rb:644:in `expanded_key'\n…/code/rails/activesupport/lib/active_support/cache.rb:608:in `normalize_key'\n…/code/rails/activesupport/lib/active_support/cache.rb:565:in `block in read_multi_entries'\n…/code/rails/activesupport/lib/active_support/cache.rb:564:in `each'\n…/code/rails/activesupport/lib/active_support/cache.rb:564:in `read_multi_entries'\n…/code/rails/activesupport/lib/active_support/cache.rb:387:in `block in read_multi'\n</code></pre>\n\n<p>I followed it backwards until I hit these two places:</p>\n\n<pre><code class=\"lang-term\">…/code/rails/activerecord/lib/active_record/integration.rb:99:in `cache_version'\n…/code/rails/activerecord/lib/active_record/integration.rb:68:in `cache_key'\n</code></pre>\n\n<p>It looks like this expensive code is being called while generating a cache key.</p>\n\n<pre><code class=\"lang-ruby\">def cache_key(*timestamp_names)\n  if new_record?\n    \"#{model_name.cache_key}/new\"\n  else\n    if cache_version &amp;&amp; timestamp_names.none? # &lt;== line 68 here\n      \"#{model_name.cache_key}/#{id}\"\n    else\n      timestamp = if timestamp_names.any?\n        ActiveSupport::Deprecation.warn(&lt;&lt;-MSG.squish)\n          Specifying a timestamp name for #cache_key has been deprecated in favor of\n          the explicit #cache_version method that can be overwritten.\n        MSG\n\n        max_updated_column_timestamp(timestamp_names)\n      else\n        max_updated_column_timestamp\n      end\n\n      if timestamp\n        timestamp = timestamp.utc.to_s(cache_timestamp_format)\n        \"#{model_name.cache_key}/#{id}-#{timestamp}\"\n      else\n        \"#{model_name.cache_key}/#{id}\"\n      end\n    end\n  end\nend\n</code></pre>\n\n<p>On line 68 in the <code>cache_key</code> code it calls <code>cache_version</code>. Here's the code for <code>cache_version</code>:</p>\n\n<pre><code class=\"lang-ruby\">def cache_version # &lt;== line 99 here\n  if cache_versioning &amp;&amp; timestamp = try(:updated_at)\n    timestamp.utc.to_s(:usec)\n  end\nend\n</code></pre>\n\n<p>Here is our culprit:</p>\n\n<pre><code class=\"lang-ruby\">timestamp = try(:updated_at)\n</code></pre>\n\n<p>What is happening is that some database adapters, such as the one for Postgres, returned their values from the database driver as strings. Then active record will lazily cast them into Ruby objects when they are needed. In this case, our time value method is being called to convert the updated timestamp into a time object so we can use it to generate a cache version string.</p>\n\n<p>Here's the value before it's converted:</p>\n\n<pre><code class=\"lang-ruby\">User.first.updated_at_before_type_cast # =&gt; \"2019-04-24 21:21:09.232249\"\n</code></pre>\n\n<p>And here's the value after it's converted:</p>\n\n<pre><code class=\"lang-ruby\">User.first.updated_at.to_s(:usec)      # =&gt; \"20190424212109232249\"\n</code></pre>\n\n<p>Basically, all the code is doing is trimming out the non-integer characters. Like before, we need a guard that our optimization can be applied:</p>\n\n<pre><code class=\"lang-ruby\"># Detects if the value before type cast\n# can be used to generate a cache_version.\n#\n# The fast cache version only works with a\n# string value directly from the database.\n#\n# We also must check if the timestamp format has been changed\n# or if the timezone is not set to UTC then\n# we cannot apply our transformations correctly.\ndef can_use_fast_cache_version?(timestamp)\n  timestamp.is_a?(String) &amp;&amp;\n    cache_timestamp_format == :usec &amp;&amp;\n    default_timezone == :utc &amp;&amp;\n    !updated_at_came_from_user?\nend\n</code></pre>\n\n<p>Then once we're in that state, we can modify the string directly:</p>\n\n<pre><code class=\"lang-ruby\"># Converts a raw database string to `:usec`\n# format.\n#\n# Example:\n#\n#   timestamp = \"2018-10-15 20:02:15.266505\"\n#   raw_timestamp_to_cache_version(timestamp)\n#   # =&gt; \"20181015200215266505\"\n#\n# PostgreSQL truncates trailing zeros,\n# https://github.com/postgres/postgres/commit/3e1beda2cde3495f41290e1ece5d544525810214\n# to account for this we pad the output with zeros\ndef raw_timestamp_to_cache_version(timestamp)\n  key = timestamp.delete(\"- :.\")\n  if key.length &lt; 20\n    key.ljust(20, \"0\")\n  else\n    key\n  end\nend\n</code></pre>\n\n<p>There's some extra logic due to the Postgres truncation behavior linked above. The resulting code to <code>cache_version</code> becomes:</p>\n\n<pre><code class=\"lang-ruby\">def cache_version\n  return unless cache_versioning\n\n  if has_attribute?(\"updated_at\")\n    timestamp = updated_at_before_type_cast\n    if can_use_fast_cache_version?(timestamp)\n      raw_timestamp_to_cache_version(timestamp)\n    elsif timestamp = updated_at\n      timestamp.utc.to_s(cache_timestamp_format)\n    end\n  end\nend\n</code></pre>\n\n<p>That's the opportunity. What's the impact?</p>\n\n<pre><code class=\"lang-term\">Before: Total allocated: 743842 bytes (6626 objects)\nAfter:  Total allocated: 702955 bytes (6063 objects)\n</code></pre>\n\n<p>The bytes reduced is 5% fewer allocations. Which is pretty good. How does it translate to speed?</p>\n\n<p>It turns out that time conversion is very CPU intensive and changing this code makes the target application up to 1.12x faster. This means that if your app previously required 100 servers to run, it can now run with about 88 servers.</p>\n<h2 class=\"anchored\">\n  <a name=\"wrap-up\" href=\"#wrap-up\">Wrap up</a>\n</h2>\n\n<p>Adding together these optimizations and others brings the overall performance improvement to 1.23x or a net reduction of 19 servers. Basically, it's like buying 4 servers and getting 1 for free.</p>\n\n<p>These examples were picked from my changes to the Rails codebase, but you can use them to optimize your applications. The general framework looks like this:</p>\n\n<ul>\n<li>Get a list of all your memory</li>\n<li>Zoom in on a hotspot</li>\n<li>Figure out what is causing that memory to be allocated inside of your code</li>\n<li>Ask if you can refactor your code to not depend on those allocations</li>\n</ul>\n\n<p>If you want to learn more about memory, here are my recommendations:</p>\n\n<ul>\n<li>\n<a href=\"https://www.schneems.com/2019/11/07/why-does-my-apps-memory-usage-grow-asymptotically-over-time/\">Why does my App's Memory Use Grow Over Time?</a>  - Start here, an excellent high-level overview of what causes a system's memory to grow that will help you develop an understanding of how Ruby allocates and uses memory at the application level.</li>\n<li>\n<a href=\"https://www.railsspeed.com\">Complete Guide to Rails Performance (Book)</a> - This book is by Nate Berkopec and is excellent. I recommend it to someone at least once a week.</li>\n<li>\n<a href=\"https://www.sitepoint.com/ruby-uses-memory/\">How Ruby uses memory</a> - This is a lower level look at precisely what \"retained\" and \"allocated\" memory means. It uses small scripts to demonstrate Ruby memory behavior. It also explains why the \"total max\" memory of our system rarely goes down.</li>\n<li>\n<a href=\"https://www.schneems.com/2015/05/11/how-ruby-uses-memory.html\">How Ruby uses memory (Video)</a> - If you're new to the concepts of object allocation, this might be an excellent place to start (you can skip the first story in the video, the rest are about memory). Memory stuff starts at 13 minutes</li>\n<li>\n<a href=\"https://www.schneems.com/2017/04/12/jumping-off-the-memory-cliff/\">Jumping off the Ruby Memory Cliff</a> - Sometimes you might see a 'cliff' in your memory metrics or a saw-tooth pattern. This article explores why that behavior exists and what it means.</li>\n<li>\n<a href=\"https://devcenter.heroku.com/articles/ruby-memory-use\">Ruby Memory Use (Heroku Devcenter article I maintain)</a> - This article focuses on alleviating the symptoms of high memory use.</li>\n<li>\n<a href=\"https://blog.codeship.com/debugging-a-memory-leak-on-heroku/\">Debugging a memory leak on Heroku</a> - TLDR; It's probably not a leak. Still worth reading to see how you can come to the same conclusions yourself. Content is valid for other environments than Heroku. Lots of examples of using the tool <code>derailed_benchmarks</code> (that I wrote).</li>\n<li>\n<a href=\"https://www.youtube.com/watch?v=CS11WIalmPM&amp;feature=emb_title\">The Life-Changing Magic of Tidying Active Record Allocations (Video)</a> - This talk shows how I used tools to track down and eliminate memory allocations in real life. All of the examples are from patches I submitted to Rails, but the process works the same for finding allocations caused by your application logic.</li>\n</ul>\n\n<p><em>Get ahold of Richard and stay up-to-date with Ruby, Rails, and other programming related content through a <a href=\"https://www.schneems.com/mailinglist\">subscription to his mailing list</a>.</em></p>","PublishedAt":"2020-09-16 14:58:00+00:00","OriginURL":"https://blog.heroku.com/tidying-ruby-object-allocations","SourceName":"Heroku"}},{"node":{"ID":356,"Title":"Let's Debug a Node.js Application","Description":"<p>There are always challenges when it comes to debugging applications. Node.js' asynchronous workflows add an extra layer of complexity to this arduous process. Although there have been some updates made to the V8 engine in order to easily access asynchronous stack traces, most of the time, we just get errors on the main thread of our applications, which makes debugging a little bit difficult. As well, when our Node.js applications crash, we usually need to <a href=\"https://www.ibm.com/developerworks/library/wa-ibm-node-enterprise-dump-debug-sdk-nodejs-trs/index.html\">rely on some complicated CLI tooling to analyze the core dumps</a>.</p>\n\n<!-- more -->\n\n<p>In this article, we'll take a look at some easier ways to debug your Node.js applications.</p>\n<h2 class=\"anchored\">\n  <a name=\"logging\" href=\"#logging\">Logging</a>\n</h2>\n\n<p>Of course, no developer toolkit is complete without logging. We tend to place <code>console.log</code> statements all over our code in local development, but this is not a really scalable strategy in production. You would likely need to do some filtering and cleanup, or implement a consistent logging strategy, in order to identify important information from genuine errors.</p>\n\n<p>Instead, to implement a proper log-oriented debugging strategy, use a logging tool like <a href=\"https://github.com/pinojs/pino\">Pino</a> or <a href=\"https://github.com/winstonjs/winston\">Winston</a>. These will allow you to set log levels (<code>INFO</code>, <code>WARN</code>, <code>ERROR</code>), allowing you to print verbose log messages locally and only severe ones for production. You can also stream these logs to aggregators, or other endpoints, like LogStash, Papertrail, or even Slack.</p>\n<h2 class=\"anchored\">\n  <a name=\"working-with-node-inspect-and-chrome-devtools\" href=\"#working-with-node-inspect-and-chrome-devtools\">Working with Node Inspect and Chrome DevTools</a>\n</h2>\n\n<p>Logging can only take us so far in understanding why an application is not working the way we would expect. For sophisticated debugging sessions, we will want to use breakpoints to inspect how our code behaves at the moment it is being executed.</p>\n\n<p>To do this, we can use Node Inspect. Node Inspect is a debugging tool which comes with Node.js. It's actually just an implementation of <a href=\"https://developers.google.com/web/tools/chrome-devtools/\">Chrome DevTools</a> for your program, letting you add breakpoints, control step-by-step execution, view variables, and follow the call stack.</p>\n\n<p>There are a couple of ways to launch Node Inspect, but the easiest is perhaps to just call your Node.js application with the <code>--inspect-brk</code> flag:</p>\n\n<pre><code class=\"bash\">$ node --inspect-brk $your_script_name\n</code></pre>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1595873126-image1.png\" alt=\"Node inspector\"></p>\n\n<p>After launching your program, head to the <code>chrome://inspect</code> URL in your Chrome browser to get to the Chrome DevTools. With Chrome DevTools, you have all of the capabilities you'd normally expect when debugging JavaScript in the browser. One of the nicer tools is <a href=\"https://developers.google.com/web/tools/chrome-devtools/memory-problems\">the ability to inspect memory</a>. You can <a href=\"https://developers.google.com/web/tools/chrome-devtools/memory-problems/heap-snapshots\">take heap snapshots</a> and profile memory usage to understand how memory is being allocated, and potentially, plug any memory leaks.</p>\n<h3 class=\"anchored\">\n  <a name=\"using-a-supported-ide\" href=\"#using-a-supported-ide\">Using a supported IDE</a>\n</h3>\n\n<p>Rather than launching your program in a certain way, many modern IDEs also support debugging Node applications. In addition to having many of the features found in Chrome DevTools, they bring their own features, such as <a href=\"https://code.visualstudio.com/blogs/2018/07/12/introducing-logpoints-and-auto-attach\">creating logpoints</a> and allowing you to create multiple debugging profiles. Check out <a href=\"https://nodejs.org/en/docs/guides/debugging-getting-started/#inspector-clients\">the Node.js' guide on inspector clients</a> for more information on these IDEs.</p>\n<h3 class=\"anchored\">\n  <a name=\"using-ndb\" href=\"#using-ndb\">Using NDB</a>\n</h3>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1595873374-image4.png\" alt=\"NDB\"></p>\n\n<p>Another option is to install <a href=\"https://github.com/GoogleChromeLabs/ndb\">ndb</a>, a standalone debugger for Node.js. It makes use of the same DevTools that are available in the browser, just as an isolated, local debugger. It also has some extra features that aren't available in DevTools. It supports edit-in-place, which means you can make changes to your code and have the updated logic supported directly by the debugger platform. This is very useful for doing quick iterations.</p>\n<h2 class=\"anchored\">\n  <a name=\"post-mortem-debugging\" href=\"#post-mortem-debugging\">Post-Mortem Debugging</a>\n</h2>\n\n<p>Suppose your application crashes due to a catastrophic error, like a memory access error. These may be rare, but they do happen, particularly if your app relies on native code.</p>\n\n<p>To investigate these sorts of issues, you can use <a href=\"https://github.com/nodejs/llnode\">llnode</a>. When your program crashes, <code>llnode</code> can be used to inspect JavaScript stack frames and objects by mapping them to objects on the C/C++ side. In order to use it, you first need a core dump of your program. To do this, you will need to use <code>process.abort</code> instead of <code>process.exit</code> to shut down processes in your code. When you use <code>process.abort</code>, the Node process generates a core dump file on exit.</p>\n\n<p>To better understand what <code>llnode</code> can provide, <a href=\"https://asciinema.org/a/29589\">here is a video</a> which demonstrates some of its capabilities.</p>\n<h2 class=\"anchored\">\n  <a name=\"useful-node-modules\" href=\"#useful-node-modules\">Useful Node Modules</a>\n</h2>\n\n<p>Aside from all of the above, there are also a few third-party packages that we can recommend for further debugging.</p>\n<h3 class=\"anchored\">\n  <a name=\"debug\" href=\"#debug\">debug</a>\n</h3>\n\n<p>The first of these is called, simply enough, <a href=\"https://www.npmjs.com/package/debug\">debug</a>. With debug, you can assign a specific namespace to your log messages, based on a function name or an entire module. You can then selectively choose which messages are printed to the console via a specific environment variable.</p>\n\n<p>For example, here's a Node.js server which is logging several messages from the entire application and middleware stack, like <code>sequelize</code>, <code>express:application</code>, and <code>express:router</code>:</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1595873233-image2.png\" alt=\"Debug module full output\"></p>\n\n<p>If we set the <code>DEBUG</code> environment variable to <code>express:router</code> and start the same program, only the messages tagged as <code>express:router</code> are shown:</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1595873290-image3.png\" alt=\"Debug module filtered output\"></p>\n\n<p>By filtering messages in this way, we can hone in on how a single segment of the application is behaving, without needing to drastically change the logging of the code.</p>\n<h3 class=\"anchored\">\n  <a name=\"trace-and-clarify\" href=\"#trace-and-clarify\">trace and clarify</a>\n</h3>\n\n<p>Two more modules that go together are <a href=\"https://github.com/AndreasMadsen/trace\">trace</a> and <a href=\"https://github.com/AndreasMadsen/clarify\">clarify</a>.</p>\n\n<p><code>trace</code> augments your asynchronous stack traces by providing much more detailed information on the async methods that are being called, a roadmap which Node.js does not provide by default. <code>clarify</code> helps by removing all of the information from stack traces which are specific to Node.js internals. This allows you to concentrate on the function calls that are just specific to your application.</p>\n\n<p>Neither of these modules are recommended for running in production! You should only enable them when debugging issues in your local development environment.</p>\n<h2 class=\"anchored\">\n  <a name=\"find-out-more\" href=\"#find-out-more\">Find out more</a>\n</h2>\n\n<p>If you'd like to follow along with how to use these debugging tools in practice, <a href=\"https://vimeo.com/428003519/f132859d08\">here is a video recording</a> which provides more detail. It includes some live demos of how to narrow in on problems in your code. Or, if you have any other questions, you can find me on Twitter <a href=\"https://twitter.com/julian_duque\">@julian_duque</a>!</p>","PublishedAt":"2020-08-03 16:08:55+00:00","OriginURL":"https://blog.heroku.com/debug-node-applications","SourceName":"Heroku"}},{"node":{"ID":357,"Title":"Ground Control to Major TOML: Why Buildpacks Use a Most Peculiar Format","Description":"<p>YAML files dominate configuration in the cloud native ecosystem. They’re used by Kuberentes, Helm, Tekton, and many other projects to define custom configuration and workflows. But YAML has its oddities, which is why the Cloud Native Buildpacks project chose TOML as its primary configuration format.</p>\n\n<p>TOML is a minimal configuration file format that's easy to read because of its simple semantics. You can learn more about TOML from the <a href=\"https://toml.io/en/\">official documentation</a>, but a simple buildpack TOML file looks like this:</p>\n\n<!-- more -->\n\n<pre><code class=\"lang-toml\">api = \"0.2\"\n\n[buildpack]\nid = \"heroku/maven\"\nversion = \"1.0\"\nname = \"Maven\"\n</code></pre>\n\n<p>Unlike YAML, TOML doesn’t rely on significant whitespace with difficult to read indentation. TOML is designed to be human readable, which is why it favors simple structures. It’s also easy for machines to read and write; you can even append to a TOML file without reading it first, which makes it a great data interchange format. But data interchange and machine readability aren’t the main driver for using TOML in the Buildpacks project; it’s humans.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1595374389-major-toml.png\" alt=\"Blog post illustration\"></p>\n<h2 class=\"anchored\">\n  <a name=\"put-your-helmet-on\" href=\"#put-your-helmet-on\">Put Your Helmet On</a>\n</h2>\n\n<p>The first time you use Buildpacks, you probably won’t need to write a TOML file. Buildpacks are designed to get out of your way, and disappear into the details. That’s why there’s no need for large configuration files like a <a href=\"https://helm.sh/docs/chart_template_guide/values_files/\">Helm values.yaml</a> or a <a href=\"https://kubernetes.io/docs/concepts/configuration/\">Kubernetes pod configuration</a>.</p>\n\n<p>Buildpacks favor convention over configuration, and therefore don’t require complex customizations to tweak the inner workings of its tooling. Instead, Buildpacks detect what to do based on the contents of an application, which means configuration is usually limited to simple properties that are defined by a human.</p>\n\n<p>Buildpacks also favor infrastructure as <em>imperative</em> code (rather than <em>declarative</em>). Buildpacks themselves are functions that run against an application, and are best implemented in higher level languages, which can use libraries and testing.</p>\n\n<p>All of these properties lend to a simple configuration format and schema that doesn’t define complex structures. But that doesn’t mean the decision to use TOML was simple.</p>\n<h2 class=\"anchored\">\n  <a name=\"can-you-hear-me-major-toml\" href=\"#can-you-hear-me-major-toml\">Can You Hear Me, Major TOML?</a>\n</h2>\n\n<p>There are many other formats the Buildpacks project could have used besides YAML or TOML, and the Buildpacks core team considered all of these in the early days of the project.</p>\n\n<p>JSON has simple syntax and semantics that are great for data interchange, but it doesn’t make a great human-readable format; in part because it doesn’t allow for comments. Buildpacks use JSON for machine readable config, like the OCI image metadata. But it shouldn’t be used for anything a human writes. </p>\n\n<p>XML has incredibly powerful properties including schema validation, transformation tools, and rich semantics. It’s great for markup (like HTML) but it's much too heavy of a format for what Buildpacks require.</p>\n\n<p>In the end, the Buildpacks project was comfortable choosing TOML because there was solid prior art (even though the format is somewhat obscure). In the cloud native ecosystem, the <a href=\"https://containerd.io/\">containerd</a> project uses TOML. Additionally, many language ecosystem tools like <a href=\"http://doc.crates.io/\">Cargo</a> (for Rust) and <a href=\"https://python-poetry.org/\">Poetry</a> (for Python) use TOML to configure application dependencies. </p>\n<h2 class=\"anchored\">\n  <a name=\"commencing-countdown-engines-on\" href=\"#commencing-countdown-engines-on\">Commencing Countdown, Engines On</a>\n</h2>\n\n<p>The main disadvantage of TOML is its ubiquity. Tools that parse and query TOML files (something comparable to <code>jq</code>) aren’t readily available, and the format can still be jarring to new users even though it’s fairly simple.</p>\n\n<p>Every trend has to start somewhere, and the Cloud Native Buildpacks project is happy to be one of the projects stepping through the door.</p>\n\n<p>If you want to learn more or have any questions around Cloud Native Buildpacks, we will be hosting a <a href=\"https://community.hackernoon.com/t/cloud-native-buildpacks-ama-with-terence-lee-and-joe-kutner-of-heroku/52494\">Live AMA at Hackernoon</a> on July 28th at 2pm PDT. See you there!</p>","PublishedAt":"2020-07-22 15:08:00+00:00","OriginURL":"https://blog.heroku.com/why-buildpacks-use-toml","SourceName":"Heroku"}},{"node":{"ID":358,"Title":"Making Time to Save You Time: How We Sped Up Time-Related Syscalls on Dynos","Description":"<p>I work on Heroku’s Runtime Infrastructure team, which focuses on most of the underlying compute and containerization here at Heroku. Over the years, we’ve tuned our infrastructure in a number of ways to improve performance of customer dynos and harden security.</p>\n\n<p>We recently received a support ticket from a customer inquiring about poor performance in two <a href=\"https://en.wikipedia.org/wiki/System_call\">system calls</a> (more commonly referred to as syscalls) their application was making frequently: <a href=\"https://manpages.ubuntu.com/manpages/bionic/man2/clock_getres.2.html\"><code>clock_gettime(3)</code></a> and <a href=\"https://manpages.ubuntu.com/manpages/bionic/man2/gettimeofday.2.html\"><code>gettimeofday(2)</code></a>.</p>\n\n<p>In this customer’s case, they were using a tool to do transaction tracing to monitor the performance of their application. This tool made many such system calls to measure how long different parts of their application took to execute. Unfortunately, these two system calls were very slow for them. Every request was impacted waiting for the time to return, slowing down the app for their users.</p>\n\n<p>To help diagnose the problem we first examined our existing clocksource configuration. The clocksource determines how the Linux kernel gets the current time. The kernel attempts to choose the \"best\" clocksource from the sources available. In our case, the kernel was defaulting to the <code>xen</code> clocksource, which seems reasonable at a glance since the EC2 infrastructure that powers Heroku’s Common Runtime and Private Spaces products uses the Xen hypervisor under the hood.</p>\n\n<p>Unfortunately, the version of Xen in use does not support a particular optimization—virtual dynamic shared object (or \"<a href=\"https://lwn.net/Articles/615809/\">vDSO</a>\")—for the two system calls in question. In short, vDSO allows certain operations to be performed entirely in userspace rather than having to context switch into kernelspace by mapping some kernel functionality into the current process. Context switching between userspace and kernelspace is a somewhat expensive operation—it takes a lot of CPU time. Most applications won’t see a large impact from occasional context switches, but when context switches are happening hundreds or thousands of times per web request, they can add up very quickly!</p>\n\n<p>Thankfully, there are often several available clocksources to choose from. The available clocksources depends on a combination of the CPU, the Linux kernel version, and the hardware virtualization software being used. Our research revealed <code>tsc</code> seemed to be the most promising clocksource and would support vDSO. <code>tsc</code> utilizes the <a href=\"https://en.wikipedia.org/wiki/Time_Stamp_Counter\">Time Stamp Counter</a> to determine the System Time.</p>\n\n<p>During our research, we also encountered a few other <a href=\"https://blog.packagecloud.io/eng/2017/03/08/system-calls-are-much-slower-on-ec2/\">blog</a> <a href=\"https://heap.io/blog/engineering/clocksource-aws-ec2-vdso\">posts</a> about TSC. Every source we referenced agreed that non-vDSO accelerated system calls were significantly slower, but there was some disagreement on how safe use of TSC would be. The Wikipedia article linked in the previous paragraph also lists some of these safety concerns. The two primary concerns centered around backwards clock drift that could occur due to: (1) TSC inconsistency that plagued older processors in hyper-threaded or multi-CPU configurations, and (2) when freezing/unfreezing Xen virtual machines. To the first concern, Heroku uses newer Intel CPUs for all dynos that have significantly safer TSC implementations. To the second concern, EC2 instances, which Heroku dynos use, do not utilize freezing/unfreezing today. We decided that <code>tsc</code> would be the best clocksource choice to support vDSO for these system calls without introducing negative side effects.</p>\n\n<p>We were able to confirm using the <code>tsc</code> clocksource enabled vDSO acceleration with the excellent <a href=\"https://github.com/nlynch-mentor/vdsotest\">vdsotest</a> tool (although you can verify your own results using <code>strace</code>). After our internal testing, we deployed the <code>tsc</code> clocksource configuration change to the Heroku Common Runtime and Private Spaces dyno fleet.</p>\n\n<p>While the customer who filed the initial support ticket that led to this change noticed the improvement, the biggest surprise for us was when other customers started inquiring about unexpected performance improvements (which we knew to be a result of this change). It’s always nice for us when our work to solve a problem for a specific customer has a significant positive impact for all customers.</p>\n\n<p>We're glad to be able to make changes like this that benefit all Heroku users. Detailed diagnostic and tuning work like this may not be worth the time investment for an individual engineering team managing their own infrastructure outside of Heroku. Heroku’s scale allows us to identify unique optimization opportunities and invest time into validating and implementing tweaks like this that make apps on Heroku run faster and more reliably.</p>","PublishedAt":"2020-07-16 16:50:00+00:00","OriginURL":"https://blog.heroku.com/clocksource-tuning","SourceName":"Heroku"}},{"node":{"ID":359,"Title":"A Fast Car Needs Good Brakes: How We Added Client Rate Throttling to the Platform API Gem","Description":"<p>When API requests are made one-after-the-other they'll quickly hit rate limits and when that happens:</p>\n\n<p></p><blockquote class=\"twitter-tweet tw-align-center\">\n<p lang=\"en\" dir=\"ltr\">If you provide an API client that doesn't include rate limiting, you don't really have an API client. You've got an exception generator with a remote timer.</p>— Richard Schneeman 🤠 Stay Inside (@schneems) <a href=\"https://twitter.com/schneems/status/1138899094137651200?ref_src=twsrc%5Etfw\">June 12, 2019</a>\n</blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n<p>That tweet spawned a discussion that generated a quest to add rate throttling logic to the <a href=\"https://rubygems.org/gems/platform-api\"><code>platform-api</code></a> gem that Heroku maintains for talking to its API in Ruby.</p>\n\n<blockquote>\n<p>If the term \"rate throttling\" is new to you, read <a href=\"https://schneems.com/2020/06/25/rate-limiting-rate-throttling-and-how-they-work-together/\">Rate limiting, rate throttling, and how they work together</a></p>\n</blockquote>\n\n<p>The Heroku API uses <a href=\"https://brandur.org/rate-limiting\">Genetic Cell Rate Algorithm (GCRA) as described by Brandur in this post</a> on the server-side. Heroku's <a href=\"https://devcenter.heroku.com/articles/platform-api-reference#rate-limits\">API docs</a> state:</p>\n\n<blockquote>\n<p>The API limits the number of requests each user can make per hour to protect against abuse and buggy code. Each account has a pool of request tokens that can hold at most 4500 tokens. Each API call removes one token from the pool. Tokens are added to the account pool at a rate of roughly 75 per minute (or 4500 per hour), up to a maximum of 4500. If no tokens remain, further calls will return 429 Too Many Requests until more tokens become available.</p>\n</blockquote>\n\n<p>I needed to write an algorithm that never errored as a result of a 429 response. A \"simple\" solution would be to add a retry to all requests when they see a 429, but that would effectively DDoS the API. I made it a goal for the rate throttling client also to minimize its retry rate. That is, if the client makes 100 requests, and 10 of them are a 429 response that its retry rate is 10%. Since the code needed to be contained entirely in the client library, it needed to be able to function without distributed coordination between multiple clients on multiple machines except for whatever information the Heroku API returned.</p>\n<h2 class=\"anchored\">\n  <a name=\"making-client-throttling-maintainable\" href=\"#making-client-throttling-maintainable\">Making client throttling maintainable</a>\n</h2>\n\n<p>Before we can get into what logic goes into a quality rate throttling algorithm, I want to talk about the process that I used as I think the journey is just as fascinating as the destination.</p>\n\n<p>I initially started wanting to write tests for my rate throttling strategy. I quickly realized that while testing the behavior \"retries a request after a 429 response,\" it is easy to check. I also found that checking for quality \"this rate throttle strategy is better than others\" could not be checked quite as easily. The solution that I came up with was to write a simulator in addition to tests. I would simulate the server's behavior, and then boot up several processes and threads and hit the simulated server with requests to observe the system's behavior.</p>\n\n<p>I initially just output values to the CLI as the simulation ran, but found it challenging to make sense of them all, so I added charting. I found my simulation took too long to run and so I added a mechanism to speed up the simulated time. I used those two outputs to write what I thought was a pretty good rate throttling algorithm. The next task was wiring it up to the <code>platform-api</code> gem.</p>\n\n<p>To help out I paired with <a href=\"https://twitter.com/lolaodelola\">a Heroku Engineer, Lola</a>, we ended up making several PRs to a bunch of related projects, and that's its own story to tell. Finally, the day came where we were ready to get rate throttling into the <code>platform-api</code> gem; all we needed was a review.</p>\n\n<p>Unfortunately, the algorithm I developed from \"watching some charts for a few hours\" didn't make a whole lot of sense, and it was painfully apparent that it wasn't maintainable. While I had developed a good gut feel for what a \"good\" algorithm did and how it behaved, I had no way of solidifying that knowledge into something that others could run with. Imagine someone in the future wants to make a change to the algorithm, and I'm no longer here. The tests I had could prevent them from breaking some expectations, but there was nothing to help them make a better algorithm.</p>\n<h2 class=\"anchored\">\n  <a name=\"the-making-of-an-algorithm\" href=\"#the-making-of-an-algorithm\">The making of an algorithm</a>\n</h2>\n\n<p>At this point, I could explain the approach I had taken to build an algorithm, but I had no way to quantify the \"goodness\" of my algorithm. That's when I decided to throw it all away and start from first principles. Instead of asking \"what would make my algorithm better,\" I asked, \"how would I know a change to my algorithm is better\" and then worked to develop some ways to quantify what \"better\" meant. Here are the goals I ended up coming up with:</p>\n\n<ul>\n<li>Minimize average retry rate: The fewer failed API requests, the better</li>\n<li>Minimize maximum sleep time: Rate throttling involves waiting, and no one wants to wait for too long</li>\n<li>Minimize variance of request count between clients: No one likes working with a greedy co-worker, API clients are no different. No client in the distributed system should be an extended outlier</li>\n<li>Minimize time to clear a large request capacity: As the system changes, clients should respond quickly to changes.</li>\n</ul>\n\n<p>I figured that if I could generate metrics on my rate-throttle algorithm and compare it to simpler algorithms, then I could show why individual decisions were made.</p>\n\n<p>I moved my hacky scripts for my simulation into a separate repo and, rather than relying on watching charts and logs, moved to have my simulation <a href=\"https://github.com/zombocom/rate_throttle_client/blob/master/lib/rate_throttle_client/demo.rb\">produce numbers that could be used to quantify and compare algorithms</a>.</p>\n\n<p>With that work under my belt, I threw away everything I knew about rate-throttling and decided to use science and measurement to guide my way.</p>\n<h2 class=\"anchored\">\n  <a name=\"writing-a-better-rate-throttling-algorithm-with-science-exponential-backoff\" href=\"#writing-a-better-rate-throttling-algorithm-with-science-exponential-backoff\">Writing a better rate-throttling algorithm with science: exponential backoff</a>\n</h2>\n\n<p>Earlier I mentioned that a \"simple\" algorithm would be to retry requests. A step up in complexity and functionality would be to retry requests after an exponential backoff. I coded it up and got some numbers for a simulated 30-minute run (which takes 3 minutes of real-time):</p>\n\n<pre><code>Avg retry rate:      60.08 %\nMax sleep time:      854.89 seconds\nStdev Request Count: 387.82\n\nTime to clear workload (4500 requests, starting_sleep: 1s):\n74.23 seconds\n</code></pre>\n\n<p>Now that we've got baseline numbers, how could we work to minimize any of these values? In my initial exponential backoff model, I multiplied sleep by a factor of 2.0, what would happen if I increased it to 3.0 or decreased it to 1.2?</p>\n\n<p>To find out, I plugged in those values and re-ran my simulations. I found that there was a correlation between retry rate and max sleep value with the backoff factor, but they were inverse. I could lower the retry rate by increasing the factor (to 3.0), but this increased my maximum sleep time. I could reduce the maximum sleep time by decreasing the factor (to 1.2), but it increased my retry rate.</p>\n\n<p>That experiment told me that if I wanted to optimize both retry rate and sleep time, I could not do it via only changing the exponential factor since an improvement in one meant a degradation in the other value.</p>\n\n<p>At this point, we could theoretically do anything, but our metrics judge our success. We could put a cap on the maximum sleep time, for example, we could write code that says \"don't sleep longer than 300 seconds\", but it too would hurt the retry rate. The biggest concern for me in this example is the maximum sleep time, 854 seconds is over 14 minutes which is WAAAYY too long for a single client to be sleeping.</p>\n\n<p>I ended up picking the 1.2 factor to decrease that value at the cost of a worse retry-rate:</p>\n\n<pre><code>Avg retry rate:      80.41 %\nMax sleep time:      46.72 seconds\nStdev Request Count: 147.84\n\nTime to clear workload (4500 requests, starting_sleep: 1s):\n74.33 seconds\n</code></pre>\n\n<p>Forty-six seconds is better than 14 minutes of sleep by a long shot. How could we get the retry rate down?</p>\n<h2 class=\"anchored\">\n  <a name=\"incremental-improvement-exponential-sleep-with-a-gradual-decrease\" href=\"#incremental-improvement-exponential-sleep-with-a-gradual-decrease\">Incremental improvement: exponential sleep with a gradual decrease</a>\n</h2>\n\n<p>In the exponential backoff model, it backs-off once it sees a 429, but as soon as it hits a success response, it doesn't sleep at all. One way to reduce the retry-rate would be to assume that once a request had been rate-throttled, that future requests would need to wait as well. Essentially we would make the sleep value \"sticky\" and sleep before all requests. If we only remembered the sleep value, our rate throttle strategy wouldn't be responsive to any changes in the system, and it would have a poor \"time to clear workload.\" Instead of only remembering the sleep value, we can gradually reduce it after every successful request. This logic is very similar to <a href=\"https://en.wikipedia.org/wiki/TCP_congestion_control#Slow_start\">TCP slow start</a>.</p>\n\n<p>How does it play out in the numbers?</p>\n\n<pre><code>Avg retry rate:      40.56 %\nMax sleep time:      139.91 seconds\nStdev Request Count: 867.73\n\nTime to clear workload (4500 requests, starting_sleep: 1s):\n115.54 seconds\n</code></pre>\n\n<p>Retry rate did go down by about half. Sleep time went up, but it's still well under the 14-minute mark we saw earlier. But there's a problem with a metric I've not talked about before, the \"stdev request count.\" It's easier to understand if you look at a chart to see what's going on:</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1594232091-ExponentialBackoff.png\" alt=\"Exponential sleep with gradual decrease chart\"></p>\n\n<p>Here you can see one client is sleeping a lot (the red client) while other clients are not sleeping at all and chewing through all the available requests at the bottom. Not all the clients are behaving equitably. This behavior makes it harder to tune the system.</p>\n\n<p>One reason for this inequity is that all clients are decreasing by the same constant value for every successful request. For example, let's say we have a client A that is sleeping for 44 seconds, and client B that is sleeping for 11 seconds and both decrease their sleep value by 1 second after every request. If both clients ran for 45 seconds, it would look like this:</p>\n\n<pre><code>Client A) Sleep 44 (Decrease value: 1)\nClient B) Sleep 11 (Decrease value: 1)\nClient B) Sleep 10 (Decrease value: 1)\nClient B) Sleep  9 (Decrease value: 1)\nClient B) Sleep  8 (Decrease value: 1)\nClient B) Sleep  7 (Decrease value: 1)\nClient A) Sleep 43 (Decrease value: 1)\n</code></pre>\n\n<p>So while client A has decreased by 1 second total, client B has reduced by 4 seconds total, since it is firing 4x as fast (i.e., it's sleep time is 4x lower). So while the decrease rate is equal, it is not equitable. Ideally, we would want all clients to decrease at the same rate.</p>\n<h2 class=\"anchored\">\n  <a name=\"all-clients-created-equal-exponential-increase-proportional-decrease\" href=\"#all-clients-created-equal-exponential-increase-proportional-decrease\">All clients created equal: exponential increase proportional decrease</a>\n</h2>\n\n<p>Since clients cannot communicate with each other in our distributed system, one way to guaranteed proportional decreases is to use the sleep value in the decrease amount:</p>\n\n<pre><code>decrease_value = (sleep_time) / some_value\n</code></pre>\n\n<p>Where <code>some_value</code> is a magic number. In this scenario the same clients A and B running for 45 seconds would look like this with a value of 100:</p>\n\n<pre><code>Client A) Sleep 44\nClient B) Sleep 11\nClient B) Sleep 10.89 (Decrease value: 11.00/100 = 0.1100)\nClient B) Sleep 10.78 (Decrease value: 10.89/100 = 0.1089)\nClient B) Sleep 10.67 (Decrease value: 10.78/100 = 0.1078)\nClient B) Sleep 10.56 (Decrease value: 10.67/100 = 0.1067)\nClient A) Sleep 43.56 (Decrease value: 44.00/100 = 0.4400)\n</code></pre>\n\n<p>Now client A has had a decrease of 0.44, and client B has had a reduction of 0.4334 (11 seconds - 10.56 seconds), which is a lot more equitable than before. Since <code>some_value</code> is tunable, I wanted to use a larger number so that the retry rate would be lower than 40%. I chose 4500 since that's the maximum number of requests in the GCRA bucket for Heroku's API.</p>\n\n<p>Here's what the results looked like:</p>\n\n<pre><code>Avg retry rate:      3.66 %\nMax sleep time:      17.31 seconds\nStdev Request Count: 101.94\n\nTime to clear workload (4500 requests, starting_sleep: 1s):\n551.10 seconds\n</code></pre>\n\n<p>The retry rate went WAAAY down, which makes sense since we're decreasing slower than before (the constant decrease value previously was 0.8). Stdev went way down as well. It's about 8x lower. Surprisingly the max sleep time went down as well. I believe this to be a factor of a decrease in the number of required exponential backoff events. Here's what this algorithm looks like:</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1594232161-ExponentialIncreaseProportionalDecrease.png\" alt=\"Exponential increase proportional decrease chart\"></p>\n\n<p>The only problem here is that the \"time to clear workload\" is 5x higher than before. What exactly is being measured here? In this scenario, we're simulating a cyclical workflow where clients are running under high load, then go through a light load, and then back to a high load. The simulation starts all clients with a sleep value, but the server's rate-limit is reset to 4500. The time is how long it takes the client to clear all 4500 requests.</p>\n\n<p>What this metric of 551 seconds is telling me is that this strategy is not very responsive to a change in the system. To illustrate this problem, I ran the same algorithm starting each client at 8 seconds of sleep instead of 1 second to see how long it would take to trigger a rate limit:</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1594143623-CleanShot%202020-07-07%20at%2010.39.35%402x.png\" alt=\"Exponential increase proportional decrease chart 7-hour torture test\"></p>\n\n<p>The graph shows that it takes about 7 hours to clear all these requests, which is not good. What we need is a way to clear requests faster when there are more requests.</p>\n<h2 class=\"anchored\">\n  <a name=\"the-only-remaining-option-exponential-increase-proportional-remaining-decrease\" href=\"#the-only-remaining-option-exponential-increase-proportional-remaining-decrease\">The only remaining option: exponential increase proportional remaining decrease</a>\n</h2>\n\n<p>When you make a request to the Heroku API, it tells you how many requests you have left remaining in your bucket in a header. Our problem with the \"proportional decrease\" is mostly that when there are lots of requests remaining in the bucket, it takes a long time to clear them (if the prior sleep rate was high, such as in a varying workload). To account for this, we can decrease the sleep value quicker when the remaining bucket is full and slower when the remaining bucket is almost empty. To express that in an expression, it might look like this:</p>\n\n<pre><code>decrease_value = (sleep_time * request_count_remaining) / some_value\n</code></pre>\n\n<p>In my case, I chose <code>some_value</code> to be the maximum number of requests possible in a bucket, which is 4500. You can imagine a scenario where workers were very busy for a period and being rate limited. Then no jobs came in for over an hour - perhaps the workday was over, and the number of requests remaining in the bucket re-filled to 4500. On the next request, this algorithm would reduce the sleep value by itself since 4500/4500 is one:</p>\n\n<pre><code>decrease_value = sleep_time * 4500 / 4500\n</code></pre>\n\n<p>That means it doesn't matter how immense the sleep value is, it will adjust fairly quickly to a change in workload. Good in theory, how does it perform in the simulation?</p>\n\n<pre><code>Avg retry rate:      3.07 %\nMax sleep time:      17.32 seconds\nStdev Request Count: 78.44\n\nTime to clear workload (4500 requests, starting_sleep: 1s):\n84.23 seconds\n</code></pre>\n\n<p>This rate throttle strategy performs very well on all metrics. It is the best (or very close) to several metrics. Here's a chart:</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1594232264-ExponentialIncreaseProportionalRemainingDecrease.png\" alt=\"Exponential increase proportional remaining decrease chart\"></p>\n\n<p>This strategy is the \"winner\" of my experiments and the algorithm that I  chose to go into the <code>platform-api</code> gem.</p>\n<h2 class=\"anchored\">\n  <a name=\"my-original-solution\" href=\"#my-original-solution\">My original solution</a>\n</h2>\n\n<p>While I originally built this whole elaborate scheme to prove how my solution was optimal, I did something by accident. By following a scientific and measurement-based approach, I accidentally found a simpler solution that performed better than my original answer. Which I'm happier about, it shows that the extra effort was worth it. To \"prove\" what I found by observation and tinkering could be not only quantified by numbers but improved upon is fantastic.</p>\n\n<p>While my original solution had some scripts and charts, this new solution has tests covering the behavior of the simulation and charting code. My initial solution was very brittle. I didn't feel very comfortable coming back and making changes to it; this new solution and the accompanying support code is a joy to work with. My favorite part though is that now if anyone asks me, \"what about trying <x>\" or \"have you considered <y>\" is that I can point them at <a href=\"https://github.com/zombocom/rate_throttle_client\">my rate client throttling library</a>, they have all the tools to implement their idea, test it, and report back with a swift feedback loop.</y></x></p>\n<h2 class=\"anchored\">\n  <a name=\"code-gem-39-platform-api-39-39-gt-3-0-39-code\" href=\"#code-gem-39-platform-api-39-39-gt-3-0-39-code\"><code>gem 'platform-api', '~&gt; 3.0'</code></a>\n</h2>\n\n<p>While I mostly wanted to talk about the process of writing rate-throttling code, this whole thing started from a desire to get client rate-throttling into the <code>platform-api</code> gem. Once I did the work to prove my solution was reasonable, we worked on a rollout strategy. We released a version of the gem in a minor bump with rate-throttling available, but with a \"null\" strategy that would preserve existing behavior. This release strategy allowed us to issue a warning to anyone depending on the original behavior. Then we released a major version with the rate-throttling strategy enabled by default. We did this first with \"pre\" release versions and then actual versions to be extra safe.</p>\n\n<p>So far, the feedback has been overwhelming that no one has noticed. We didn't cause any significant breaks or introduce any severe disfunction to any applications. If you've not already, I invite you to upgrade to 3.0.0+ of the <code>platform-api</code> gem and give it a spin. I would love to hear your feedback.</p>\n\n<p><em>Get ahold of Richard and stay up-to-date with Ruby, Rails, and other programming related content through a <a href=\"https://www.schneems.com/mailinglist\">subscription to his mailing list</a>.</em></p>","PublishedAt":"2020-07-07 20:30:00+00:00","OriginURL":"https://blog.heroku.com/rate-throttle-api-client","SourceName":"Heroku"}},{"node":{"ID":360,"Title":"Building a GraphQL API in JavaScript","Description":"<p>Over the last few years, <a href=\"https://graphql.org/\">GraphQL</a> has emerged as a very popular API specification that focuses on making data fetching easier for clients, whether the clients are a front-end or a third-party.</p>\n\n<p>In a traditional REST-based API approach, the client makes a request, and the server dictates the response:</p>\n\n<pre><code class=\"lang-term\">$ curl https://api.heroku.space/users/1\n\n{\n  \"id\": 1,\n  \"name\": \"Luke\",\n  \"email\": \"luke@heroku.space\",\n  \"addresses\": [\n    {\n    \"street\": \"1234 Rodeo Drive\",\n    \"city\": \"Los Angeles\",\n    \"country\": \"USA\"\n    }\n  ]\n}\n</code></pre>\n\n<p>But, in GraphQL, the client determines precisely the data it wants from the server. For example, the client may want only the user's name and email, and none of the address information:</p>\n\n<pre><code class=\"lang-term\">$ curl -X POST https://api.heroku.space/graphql -d '\nquery {\n  user(id: 1) {\n    name\n    email\n  }\n}\n'\n\n{\n  \"data\":\n    {\n    \"name\": \"Luke\",\n    \"email\": \"luke@heroku.space\"\n    }\n}\n</code></pre>\n\n<p>With this new paradigm, clients can make more efficient queries to a server by trimming down the response to meet their needs. For single-page apps (SPAs) or other front-end heavy client-side applications, this speeds up rendering time by reducing the payload size. However, as with any framework or language, GraphQL has its trade-offs. In this post, we'll take a look at some of the pros and cons of using GraphQL as a query language for APIs, as well as how to get started building an implementation.</p>\n<h2 class=\"anchored\">\n  <a name=\"why-would-you-choose-graphql\" href=\"#why-would-you-choose-graphql\">Why would you choose GraphQL?</a>\n</h2>\n\n<p>As with any technical decision, it's important to understand what advantages GraphQL offers to your project, rather than simply choosing it because it's a buzzword.</p>\n\n<p>Consider a SaaS application that uses an API to connect to a remote database; you'd like to render a user's profile page. You might need to make one API <code>GET</code> call to fetch information about the user, like their name or email. You might then need to make another API call to fetch information about the address, which is stored in a different table. As the application evolves, because of the way it's architected, you might need to continue to make more API calls to different locations. While each of these API calls can be done asynchronously, you must also handle their responses, whether there's an error, a network timeout, or even pausing the page render until all the data is received. As noted above, the payloads from these responses might be more than necessary to render your current pages. And each API call has network latency and the total latencies added up can be substantial. </p>\n\n<p>With GraphQL, instead of making several API calls, like <code>GET /user/:id</code> and <code>GET /user/:id/addresses</code>, you make one API call and submit your query to a single endpoint:</p>\n\n<pre><code class=\"lang-graphql\">query {\n  user(id: 1) {\n    name\n    email\n    addresses {\n    street\n    city\n    country\n    }\n  }\n}\n</code></pre>\n\n<p>GraphQL, then, gives you just one endpoint to query for all the domain logic that you need. If your application grows, and you find yourself adding more data stores to your architecture—PostgreSQL might be a good place to store user information, while Redis might be good for other kinds—a single call to a GraphQL endpoint will resolve all of these disparate locations and respond to a client with the data they requested.</p>\n\n<p>If you're unsure of the needs of your application and how data will be stored in the future, GraphQL can prove useful here, too. To modify a query, you'd only need to add the name of the field you want:</p>\n\n<pre><code class=\"lang-diff\">    addresses {\n      street\n+     apartmentNumber   # new information\n      city\n      country\n    }\n</code></pre>\n\n<p>This vastly simplifies the process of evolving your application over time.</p>\n<h2 class=\"anchored\">\n  <a name=\"defining-a-graphql-schema\" href=\"#defining-a-graphql-schema\">Defining a GraphQL schema</a>\n</h2>\n\n<p>There are GraphQL server implementations in a variety of programming languages, but before you get started, you'll need to identify the objects in your business domain, as with any API. Just as a REST API might use something like <a href=\"https://json-schema.org/\">JSON schema</a>, GraphQL defines its schema using SDL, or <a href=\"https://graphql.org/learn/schema/\">Schema Definition Language</a>, an idempotent way to describe all the objects and fields available by your GraphQL API. The general format for an SDL entry looks like this:</p>\n\n<pre><code class=\"lang-graphql\">type $OBJECT_TYPE {\n  $FIELD_NAME($ARGUMENTS): $FIELD_TYPE\n}\n</code></pre>\n\n<p>Let's build on our earlier example by defining what entries for the user and address might look like:</p>\n\n<pre><code class=\"lang-graphql\">type User {\n  name:     String\n  email:    String\n  addresses:   [Address]\n}\n\ntype Address {\n  street:   String\n  city:     String\n  country:  String\n}\n</code></pre>\n\n<p><code>User</code> defines two <code>String</code> fields called <code>name</code> and <code>email</code>. It also includes a field called <code>addresses</code>, which is an array of <code>Address</code> objects. <code>Address</code> also defines a few fields of its own. (By the way, there's more to a GraphQL <a href=\"https://graphql.org/learn/schema/\">schema</a> than just objects, fields, and scalar types. You can also incorporate interfaces, unions, and arguments, to build more complex models, but we won’t cover those for this post.)</p>\n\n<p>There's one more type we need to define, which is the entry point to our GraphQL API. You'll remember that earlier, we said a GraphQL query looked like this:</p>\n\n<pre><code class=\"lang-graphql\">query {\n  user(id: 1) {\n    name\n    email\n  }\n}\n</code></pre>\n\n<p>That <code>query</code> field belongs to a special reserved type called <code>Query</code>. This specifies the main entry point to fetching objects. (There’s also a <code>Mutation</code> type for modifying objects.) Here, we define a <code>user</code> field, which returns a <code>User</code> object, so our schema needs to define this too:</p>\n\n<pre><code class=\"lang-graphql\">type Query {\n  user(id: Int!): User\n}\n\ntype User { ... }\ntype Address { ... }\n</code></pre>\n\n<p>Arguments on a field are a comma-separated list, which takes the form of <code>$NAME: $TYPE</code>. The <code>!</code> is GraphQL's way of denoting that the argument is required—omitting means it's optional.</p>\n\n<p>Depending on your language of choice, the process of incorporating this schema into your server varies, but in general, consuming this information as a string is enough. Node.js has <a href=\"https://www.npmjs.com/package/graphql\">the <code>graphql</code> package</a> to prepare a GraphQL schema, but we're going to use <a href=\"https://www.npmjs.com/package/graphql-tools\">the <code>graphql-tools</code> package</a> instead, because it provides a few more niceties. Let's import the package and read our type definitions in preparation for future development:</p>\n\n<pre><code class=\"lang-javascript\">const fs = require('fs')\nconst { makeExecutableSchema } = require(\"graphql-tools\");\n\nlet typeDefs = fs.readFileSync(\"schema.graphql\", {\n  encoding: \"utf8\",\n  flag: \"r\",\n});\n</code></pre>\n<h2 class=\"anchored\">\n  <a name=\"setting-up-resolvers\" href=\"#setting-up-resolvers\">Setting up resolvers</a>\n</h2>\n\n<p>A schema sets up the ways in which queries can be constructed but establishing a schema to define your data model is just one part of the GraphQL specification. The other portion deals with actually fetching the data. This is done through the use of <a href=\"https://graphql.org/learn/execution/#root-fields-resolvers\"><em>resolvers</em></a>. A resolver is a function that returns a field's underlying value.</p>\n\n<p>Let's take a look at how you might implement resolvers in Node.js. The intent is to solidify concepts around how resolvers operate in conjunction with schemas, so we won't go into too much detail around how the data stores are set up. In the \"real world\", we might establish a database connection with something like <a href=\"https://knexjs.org/\">knex</a>. For now, let's just set up some dummy data:</p>\n\n<pre><code class=\"lang-javascript\">const users = {\n  1: {\n    name: \"Luke\",\n    email: \"luke@heroku.space\",\n    addresses: [\n    {\n          street: \"1234 Rodeo Drive\",\n          city: \"Los Angeles\",\n          country: \"USA\",\n    },\n    ],\n  },\n  2: {\n    name: \"Jane\",\n    email: \"jane@heroku.space\",\n    addresses: [\n    {\n          street: \"1234 Lincoln Place\",\n          city: \"Brooklyn\",\n          country: \"USA\",\n    },\n    ],\n  },\n};\n</code></pre>\n\n<p>GraphQL resolvers in Node.js amount to an Object with the key as the name of the field to be retrieved, and the value being a function that returns the data. Let's start with a barebones example of the initial <code>user</code> lookup by id:</p>\n\n<pre><code class=\"lang-javascript\">const resolvers = {\n  Query: {\n    user: function (parent, { id }) {\n      // user lookup logic\n    },\n  },\n}\n</code></pre>\n\n<p>This resolver takes two arguments: an object representing the parent (which in the initial root query is often unused), and a JSON object containing the arguments passed to your field. Not every field will have arguments, but in this case, we will, because we need to retrieve our user by their ID. The rest of the function is straightforward:</p>\n\n<pre><code class=\"lang-javascript\">const resolvers = {\n  Query: {\n    user: function (_, { id }) {\n      return users[id];\n    },\n  }\n}\n</code></pre>\n\n<p>You'll notice that we didn't explicitly define a resolver for <code>User</code> or <code>Addresses</code>. The <code>graphql-tools</code> package is intelligent enough to automatically map these for us. We can override these if we choose, but with our type definitions and resolvers now defined, we can build our complete schema:</p>\n\n<pre><code class=\"lang-javascript\">const schema = makeExecutableSchema({ typeDefs, resolvers });\n</code></pre>\n<h2 class=\"anchored\">\n  <a name=\"running-the-server\" href=\"#running-the-server\">Running the server</a>\n</h2>\n\n<p>Finally, let's get this demo running! Since we're using Express, we can use <a href=\"https://www.npmjs.com/package/express-graphql\">the <code>express-graphql</code> package</a> to expose our schema as an endpoint. The package requires two arguments: your schema, and your root value. It takes one optional argument, <code>graphiql</code>, which we'll talk about in a bit.</p>\n\n<p>Set up your Express server on your favorite port with the GraphQL middleware like this:</p>\n\n<pre><code class=\"lang-javascript\">const express = require(\"express\");\nconst express_graphql = require(\"express-graphql\");\n\nconst app = express();\napp.use(\n  \"/graphql\",\n  express_graphql({\n    schema: schema,\n    graphiql: true,\n  })\n);\napp.listen(5000, () =&gt; console.log(\"Express is now live at localhost:5000\"));\n</code></pre>\n\n<p>Navigate your browser to <code>http://localhost:5000/graphql</code>, and you should see a sort of IDE interface. On the left pane, you can enter any valid GraphQL query you like, and on your right you'll get the results. This is what <code>graphiql: true</code> provides: a convenient way of testing out your queries. You probably wouldn't want to expose this in a production environment, but it makes testing much easier.</p>\n\n<p>Try entering the query we demonstrated above:</p>\n\n<pre><code class=\"lang-graphql\">query {\n  user(id: 1) {\n    name\n    email\n  }\n}\n</code></pre>\n\n<p>To explore GraphQL's typing capabilities, try passing in a string instead of an integer for the ID argument:</p>\n\n<pre><code class=\"lang-graphql\"># this doesn't work\nquery {\n  user(id: \"1\") {\n    name\n    email\n  }\n}\n</code></pre>\n\n<p>You can even try requesting fields that don't exist:</p>\n\n<pre><code class=\"lang-graphql\"># this doesn't work\nquery {\n  user(id: 1) {\n    name\n    zodiac\n  }\n}\n</code></pre>\n\n<p>With just a few clear lines of code expressed by the schema, a strongly-typed contract between the client and server is established. This protects your services from receiving bogus data and expresses errors clearly to the requester.</p>\n<h2 class=\"anchored\">\n  <a name=\"performance-considerations\" href=\"#performance-considerations\">Performance considerations</a>\n</h2>\n\n<p>For as much as GraphQL takes care of for you, it doesn't solve every problem inherent in building APIs. In particular, caching and authorization are just two areas that require some forethought to prevent performance issues. The GraphQL spec does not provide any guidance for implementing either of these, which means that the responsibility for building them falls onto you.</p>\n<h3 class=\"anchored\">\n  <a name=\"caching\" href=\"#caching\">Caching</a>\n</h3>\n\n<p>REST-based APIs don't need to be overly concerned when it comes to caching, because they can build on <a href=\"https://restfulapi.net/caching/\">existing HTTP header strategies</a> that the rest of the web uses. GraphQL doesn't come with these caching mechanisms, which can place undue processing burden on your servers for repeated requests. Consider the following two queries:</p>\n\n<pre><code class=\"lang-graphql\">query {\n  user(id: 1) {\n    name\n  }\n}\n\nquery {\n  user(id: 1) {\n    email\n  }\n}\n</code></pre>\n\n<p>Without some sort of caching in place, this would result in two database queries to fetch the <code>User</code> with an ID of <code>1</code>, just to retrieve two different columns. In fact, since GraphQL also allows for <a href=\"https://graphql.org/learn/queries/#aliases\">aliases</a>, the following query is valid and also performs two lookups:</p>\n\n<pre><code class=\"lang-graphql\">query {\n  one: user(id: 1) {\n    name\n  }\n  two: user(id: 2) {\n    name\n  }\n}\n</code></pre>\n\n<p>This second example exposes the problem of how to batch queries. In order to be fast and efficient, we want GraphQL to access the same database rows with as few roundtrips as possible.</p>\n\n<p><a href=\"https://github.com/graphql/dataloader\">The <code>dataloader</code> package</a> was designed to handle both of these issues. Given an array of IDs, we will fetch all of those at once from the database; as well, subsequent calls to the same ID will fetch the item from the cache. To build this out using <code>dataloader</code>, we need two things. First, we need a function to load all of the requested objects. In our sample, that looks something like this:</p>\n\n<pre><code class=\"lang-javascript\">const DataLoader = require('dataloader');\nconst batchGetUserById = async (ids) =&gt; {\n   // in real life, this would be a DB call\n  return ids.map(id =&gt; users[id]);\n};\n// userLoader is now our \"batch loading function\"\nconst userLoader = new DataLoader(batchGetUserById);\n</code></pre>\n\n<p>This takes care of the issue with batching. To load the data, and work with the cache, we'll replace our previous data lookup with a call to the <code>load</code> method and pass in our user ID:</p>\n\n<pre><code class=\"lang-javascript\">const resolvers = {\n  Query: {\n    user: function (_, { id }) {\n      return userLoader.load(id);\n    },\n  },\n}\n</code></pre>\n<h3 class=\"anchored\">\n  <a name=\"authorization\" href=\"#authorization\">Authorization</a>\n</h3>\n\n<p>Authorization is an entirely different problem with GraphQL. In a nutshell, it's the process of identifying whether a given user has permission to see some data. We can imagine scenarios where an authenticated user can execute queries to get their own address information, but they should not be able to get the addresses of other users.</p>\n\n<p>To handle this, we need to modify our resolver functions. In addition to a field's arguments, a resolver also has access to its parent, as well as a special  <em>context</em> value passed in, which can provide information about the currently authenticated user. Since we know that <code>addresses</code> is a sensitive field, we need to change our code such that a call to users doesn't just return a list of addresses, but actually, calls out to some business logic to validate the request:</p>\n\n<pre><code class=\"lang-javascript\">const getAddresses = function(currUser, user) {\n  if (currUser.id == user.id) {\n    return user.addresses\n  }\n\n  return [];\n}\n\nconst resolvers = {\n  Query: {\n    user: function (_, { id }) {\n      return users[id];\n    },\n  },\n  User: {\n    addresses: function (parentObj, {}, context) {\n          return getAddresses(context.currUser, parentObj);\n    },\n  },\n};\n</code></pre>\n\n<p>Again, we don't need to explicitly define a resolver for each <code>User</code> field—only the one which we want to modify.</p>\n\n<p>By default, <code>express-graphql</code> passes the current HTTP <code>request</code> as a value for <code>context</code>, but this can be changed when setting up your server:</p>\n\n<pre><code class=\"lang-javascript\">app.use(\n  \"/graphql\",\n  express_graphql({\n    schema: schema,\n    graphiql: true,\n    context: {\n      currUser: user // currently authenticated user\n    }\n  })\n);\n</code></pre>\n<h2 class=\"anchored\">\n  <a name=\"schema-best-practices\" href=\"#schema-best-practices\">Schema best practices</a>\n</h2>\n\n<p>One aspect missing from the GraphQL spec is the lack of guidance on versioning schemas. As applications grow and change over time, so too will their APIs, and it's likely that GraphQL fields and objects will need to be removed or modified. But this downside can also be positive: by designing your GraphQL schema carefully, you can avoid pitfalls apparent in easier to implement (and easier to break) REST endpoints, such as inconsistencies in naming and confusing relationships. Marc-Andre has <a href=\"https://www.apollographql.com/blog/graphql-schema-design-building-evolvable-schemas-1501f3c59ed5\">listed several strategies</a> for building evolvable schemas which we highly recommend reading through.</p>\n\n<p>In addition, you should try to keep as much of <a href=\"https://graphql.org/learn/thinking-in-graphs/#business-logic-layer\">your business logic separate from your resolver logic</a>. Your business logic should be a single source of truth for your entire application. It can be tempting to perform validation checks within a resolver, but as your schema grows, it will become an untenable strategy.</p>\n<h2 class=\"anchored\">\n  <a name=\"when-is-graphql-not-a-good-fit\" href=\"#when-is-graphql-not-a-good-fit\">When is GraphQL not a good fit?</a>\n</h2>\n\n<p>GraphQL doesn't mold precisely to the needs of HTTP communication the same way that REST does. For example, GraphQL specifies only a single status code—<code>200 OK</code>—regardless of the query’s success. A special <code>errors</code> key is returned in this response for clients to parse and identify what went wrong. Because of this, error handling can be a bit trickier.</p>\n\n<p>As well, GraphQL is just a specification, and it won't automatically solve every problem your application faces. Performance issues won't disappear, database queries won't become faster, and in general, you'll need to rethink everything about your API: authorization, logging, monitoring, caching. Versioning your GraphQL API can also be a challenge, as the official spec currently has no support for handling breaking changes, an inevitable part of building any software. If you're interested in exploring GraphQL, you will need to dedicate some time to learning how to best integrate it with your needs.</p>\n<h2 class=\"anchored\">\n  <a name=\"learning-more\" href=\"#learning-more\">Learning more</a>\n</h2>\n\n<p>The community has rallied around this new paradigm and come up with <a href=\"https://github.com/chentsulin/awesome-graphql\">a list of awesome GraphQL resources</a>, for both frontend and backend engineers. You can also see what queries and types look like by <a href=\"https://graphql.org/swapi-graphql/\">making real requests on the official playground</a>.</p>\n\n<p>We also have a <a href=\"https://www.heroku.com/podcasts/codeish/44-graphqls-benefits-and-costs\">Code[ish] podcast episode</a> dedicated entirely to the benefits and costs of GraphQL.</p>","PublishedAt":"2020-06-24 15:30:00+00:00","OriginURL":"https://blog.heroku.com/building-graphql-api-javascript","SourceName":"Heroku"}},{"node":{"ID":361,"Title":"From Project to Productionized with Python","Description":"<p>We hope that you and your loved ones are staying safe from the COVID-19 pandemic. As a result of its effect on large gatherings, <a href=\"https://pycon.blogspot.com/2020/03/pycon-us-2020-in-pittsburgh.html\">PyCon 2020 was <del>cancelled</del> changed to an online event</a>. Although not being able to gather in person was disheartening for organizers, speakers, and attendees, the Python community shared virtual high-fives and hugs with <a href=\"https://us.pycon.org/2020/online/\">PyCon US 2020 Online.</a> We <a href=\"https://www.youtube.com/watch?v=1923eduj0Gg\">recorded our planned Heroku workshop for the event</a>, on which this blog post is based.</p>\n\n<div class=\"embedded-video-wrapper\">\n<iframe title=\"From Project to Productionized on Heroku\" src=\"https://www.youtube-nocookie.com/embed/1923eduj0Gg\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n\n\n<p>Imagine that you've just spent the last two weeks pouring all your energy into an application. It's magnificent, and you're finally ready to share it on the Internet. How do you do it? In this post, we're going to walk through the hands-on process aimed at Python developers deploying their local application to Heroku.</p>\n\n<p>An application running on Heroku works best as <a href=\"https://12factor.net/\">a 12-factor application</a>. This is actually a concept that Heroku championed over 10 years ago. It's the idea that you build an application with robust redeployments in mind. Most of this workshop is actually not specific to Heroku, but rather, about taking a regular Django application and making it meet the 12 factor app methodology, which has become a standard that most cloud deployment providers not only support but recommend.</p>\n<h2 class=\"anchored\">\n  <a name=\"prerequisites\" href=\"#prerequisites\">Prerequisites</a>\n</h2>\n\n<p>Before completing this workshop, we're going to make a few assumptions about you, dear reader. First, this is not going to be a Django tutorial. If you're looking for an introduction to Django, <a href=\"https://www.djangoproject.com/start/\">their documentation has some excellent tutorials to follow</a>. You will also need a little bit of  <a href=\"https://git-scm.com\">Git</a> familiarity, and have it installed on your machine.</p>\n\n<p>In order to complete this workshop, you'll need a few things:</p>\n\n<ol>\n<li>\n<a href=\"https://signup.heroku.com/\">An account on Heroku</a>. This is completely free and doesn't require any payment information.</li>\n<li>\n<a href=\"https://devcenter.heroku.com/articles/heroku-cli#download-and-install\">The Heroku CLI</a>. Once your application is on Heroku, this will make managing it much easier.</li>\n<li>You'll need to <a href=\"https://github.com/heroku-python/PyCon2020\">clone the repository for this workship</a>, and be able to open it in a text editor.</li>\n</ol>\n\n<p>With all that sorted, it's time to begin!</p>\n<h2 class=\"anchored\">\n  <a name=\"look-around-you\" href=\"#look-around-you\">Look around you</a>\n</h2>\n\n<p>With the project cloned and available on your computer, take a moment to explore its structure. We'll be modifying the <code>manage.py</code> and <code>requirements.txt</code> files, as well as <code>settings.py</code> and <code>wsgi.py</code> in the <code>gettingstarted</code> folder.</p>\n<h3 class=\"anchored\">\n  <a name=\"updating-code-gitignore-code\" href=\"#updating-code-gitignore-code\">Updating <code>.gitignore</code></a>\n</h3>\n\n<p>To begin with, we'll be updating the gitignore file. <a href=\"https://git-scm.com/docs/gitignore\">A gitignore file excludes files</a> which you don't want to check into your repository. In order to deploy to Heroku, you don't technically need a gitignore file. You can deploy successfully without one, but it's highly recommended to always have one (and not just for Heroku). A gitignore can be essential for keeping out passwords and credentials keys, large binary files, local configurations, or anything else that you don't want to expose to the public.</p>\n\n<p>Copy the following block of code and paste it into the gitignore file in the root of your project:</p>\n\n<pre><code>/venv\n__pycache__\ndb.sqlite3          # not needed if you're using Postgres locally\ngettingstarted/static/\n</code></pre>\n\n<p>The <code>venv</code> directory contains <a href=\"https://docs.python.org/3/tutorial/venv.html\">a virtual environment</a> with the packages necessary for your local Python version. Similarly, the <code>__pycache__</code> directory contains <a href=\"https://docs.python.org/3/tutorial/modules.html#compiled-python-files\">precompiled modules unique to your system</a>. We don't want to check in our database (<code>db.sqlite3</code>), as we don't want to expose any local data. Last, the static files will be automatically generated for us during the build and deploy process to Heroku, so we'll exclude the <code>gettingstarted/static/</code> directory.</p>\n\n<p>Go ahead and run <code>git status</code> on your terminal to make sure that gitignore is the only file that's been modified. After that, call <code>git add</code>, then <code>git commit -m \"step 1 add git ignore\"</code>.</p>\n<h3 class=\"anchored\">\n  <a name=\"modularize-your-settings\" href=\"#modularize-your-settings\">Modularize your settings</a>\n</h3>\n\n<p>Next up, we want to modularize our Django settings. To do that, add a new folder within <code>gettingstarted</code> called <code>settings</code>. Then, move the <code>settings.py</code> file into that directory. Since this naming scheme is a bit confusing, let's go ahead and rename that file to <code>base.py</code>. We'll call it that because it will serve as the base (or default) configuration that all the other configurations are going to pull from. If something like <code>dev.py</code> or <code>local.py</code> makes more sense to you, feel free to use that instead!</p>\n\n<p>Local projects only have one environment to keep track of: your local machine. But once you want to deploy to different places, it's important to keep track of what settings go where. Nesting our settings files this way makes it easy for us to keep track of where those settings are, as well as take advantage of Heroku's continuous delivery tool pipelines.</p>\n\n<p>By moving and renaming the settings file, our Django application now has two broken references. Let's fix them before we move on.</p>\n\n<p>The first is in the <code>wsgi.py</code> in your <code>gettingstarted</code> folder. Open it up, and on <a href=\"https://github.com/heroku-python/PyCon2020/blob/1a4cf7eabfcc994f60f4b8efeed1f0d9a245e768/gettingstarted/wsgi.py#L12\">line 12</a> you'll see that a default Django settings module is being set to <code>gettingstarted.settings</code>, a file which no longer exists:</p>\n\n<pre><code class=\"lang-python\">os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"gettingstarted.settings\")\n</code></pre>\n\n<p>To fix this, append the name of the file you just created in the settings subfolder. For example, since we called ours <code>base.py</code>, the line should now look like this:</p>\n\n<pre><code class=\"lang-python\">os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"gettingstarted.settings.base\")\n</code></pre>\n\n<p>After saving that, navigate up one directory to <code>manage.py</code>. On <a href=\"https://github.com/heroku-python/PyCon2020/blob/1a4cf7eabfcc994f60f4b8efeed1f0d9a245e768/manage.py#L6\">line 6</a>, you'll see the same default being set for the Django settings module. Once again, append <code>.base</code> to the end of this line, then commit both of them to Git.</p>\n<h2 class=\"anchored\">\n  <a name=\"continuous-delivery-pipelines\" href=\"#continuous-delivery-pipelines\">Continuous delivery pipelines</a>\n</h2>\n\n<p>In an application's deployment lifecycle, there are typically four stages:</p>\n\n<ol>\n<li>You build your app in the development stage on your local machine to make sure it works.</li>\n<li>Next comes the review stage, where you check to see if your changes pass with the full test suite of your code base.</li>\n<li>If that goes well, you merge your changes to staging. This is where you have conditions as close to public as possible, perhaps with some dummy data available, in order to more accurately predict how the change will impact your users.</li>\n<li>Lastly, if all that goes well, you push to production, where the change is now live for your customers.</li>\n</ol>\n\n<p>Continuous delivery (CD) workflows are designed to test your change in conditions progressively closer and closer to production and with more and more detail. Continuous delivery is a powerful workflow that can make all of the difference in your experience as a developer once you've productionized your application. Heroku can save you a lot of time here, as we've already built the tools for you to have a continuous delivery workflow. From your dashboard on Heroku, you can—with the mere click of a button!–<a href=\"https://devcenter.heroku.com/articles/pipelines\">set up a pipeline</a>, add applications to staging and production, and deploy them.</p>\n\n<p>If you <a href=\"https://devcenter.heroku.com/articles/github-integration\">connect your GitHub repository</a>, pipelines can also automatically deploy and test new PRs opened on your repo. By providing the tooling and automating these processes, Heroku's continuous delivery workflow is powerful enough to help you keep up with your development cycle.</p>\n<h2 class=\"anchored\">\n  <a name=\"adding-new-middleware-to-code-base-py-code\" href=\"#adding-new-middleware-to-code-base-py-code\">Adding new middleware to <code>base.py</code></a>\n</h2>\n\n<p>Modularizing your Django settings is a great way to take advantage of this continuous delivery workflow by splitting up your settings, whether you're deploying to Heroku or elsewhere, but there's one more change we have to make to <code>base.py</code>.</p>\n\n<p>Django static assets work best when you also use the <a href=\"http://whitenoise.evans.io/en/stable/django.html\">whitenoise</a> package to manage your static assets. It's really easy to add to your project.</p>\n\n<p>In your <code>base.py</code> file, scroll down to about line 43, and you should see an array of package names like this:</p>\n\n<pre><code class=\"lang-python\">MIDDLEWARE = [\n    \"django.middleware.security.SecurityMiddleware\",\n    # Whitenoise goes here\n    \"django.contrib.sessions.middleware.SessionMiddleware\",\n    \"django.middleware.common.CommonMiddleware\",\n    \"django.middleware.csrf.CsrfViewMiddleware\",\n    \"django.contrib.auth.middleware.AuthenticationMiddleware\",\n    \"django.contrib.messages.middleware.MessageMiddleware\",\n    \"django.middleware.clickjacking.XFrameOptionsMiddleware\",\n]\n</code></pre>\n\n<p>This is your list of <a href=\"https://docs.djangoproject.com/en/3.0/topics/http/middleware/\">Django middleware</a>, which are sort of like plugins for your server. Django loads your middleware in the order that it's listed, so you always want your security middleware first, but it's important to add whitenoise as the second step in this base file.</p>\n\n<p>Copy the following line of code and replace the line that says <code>Whitenoise goes here</code> with this:</p>\n\n<pre><code class=\"lang-python\">\"whitenoise.middleware.WhiteNoiseMiddleware\",\n</code></pre>\n\n<p>We've loaded whitenoise as middleware, but to actually <em>use</em> the whitenoise compression, we need to set one more variable. Copy the following code and paste it right at the end of your <code>base.py</code> file:</p>\n\n<pre><code class=\"lang-python\">STATICFILES_STORAGE = \"whitenoise.storage.CompressedManifestStaticFilesStorage\"\n</code></pre>\n\n<p>With that, we're done with <code>base.py</code>. Congratulations! Save your work and commit it to Git.</p>\n<h2 class=\"anchored\">\n  <a name=\"setting-up-code-heroku-py-code\" href=\"#setting-up-code-heroku-py-code\">Setting up <code>heroku.py</code></a>\n</h2>\n\n<p>Our base settings are complete, but now we need our Heroku-specific settings. Create a new file under <code>gettingstarted/settings</code> called <code>heroku.py</code> and paste the following block of code:</p>\n\n<pre><code class=\"lang-python\">\"\"\"\nProduction Settings for Heroku\n\"\"\"\n\nimport environ\n\n# If using in your own project, update the project namespace below\nfrom gettingstarted.settings.base import *\n\nenv = environ.Env(\n    # set casting, default value\n    DEBUG=(bool, False)\n)\n\n# False if not in os.environ\nDEBUG = env('DEBUG')\n\n# Raises django's ImproperlyConfigured exception if SECRET_KEY not in os.environ\nSECRET_KEY = env('SECRET_KEY')\n\nALLOWED_HOSTS = env.list('ALLOWED_HOSTS')\n\n# Parse database connection url strings like psql://user:pass@127.0.0.1:8458/db\nDATABASES = {\n    # read os.environ['DATABASE_URL'] and raises ImproperlyConfigured exception if not found\n    'default': env.db(),\n}\n</code></pre>\n\n<p>You can see in this file the values that we're listing here are the ones that we're overriding from our base settings, so these are the settings that will be different and unique for Heroku.</p>\n\n<p>To do this, we're using one of my favorite packages, <a href=\"https://github.com/joke2k/django-environ\">Django-environ</a>. This allows us to quickly and easily interface with the operating system environment without knowing much about it. It has built-in type conversions, and in particular it has automatic database parsing. This is all we need in order to parse our Heroku Postgres database URL that we will be given. It's just really convenient.</p>\n<h2 class=\"anchored\">\n  <a name=\"heroku-specific-files\" href=\"#heroku-specific-files\">Heroku-specific files</a>\n</h2>\n\n<p>That's all the work we need to do to get our application into 12 factored shape, but there are three more files we need in order to deploy to Heroku.</p>\n<h3 class=\"anchored\">\n  <a name=\"code-requirements-txt-code\" href=\"#code-requirements-txt-code\"><code>requirements.txt</code></a>\n</h3>\n\n<p>In addition to the packages your project already uses, there are a few more you need to deploy to Heroku. If we take a look at the provided <code>requirements.txt</code> file, you can see these required packages here. We've already talked about Django, Django-environ, and whitenoise, and we've already configured those for use. But the other two are also important and needed for deployment.</p>\n\n<p>The first one is called <a href=\"https://gunicorn.org/\">Gunicorn</a>. This is the recommended WSGI server for Heroku. We'll take a look at configuring this in just a bit. The next one is <a href=\"https://www.psycopg.org/\">psychopg2</a>. This is a Postgres database adapter. You need it in your <code>requirements.txt</code> file to deploy, but you don't need any code changes in order to activate it.</p>\n\n<p>A quick side note: we're keeping our discussion on packages simple for the purpose of this demo, but when you're ready to deploy a real project to Heroku, consider freezing your dependencies. You can do this with the <a href=\"https://pip.pypa.io/en/stable/reference/pip_freeze/\"><code>pip freeze</code></a> command. This will make your build a little bit more predictable by locking your exact dependency versions into your Git repo. If your dependencies aren't locked, you might find yourself deploying one version of Django one day and a new one the next.</p>\n<h3 class=\"anchored\">\n  <a name=\"code-runtime-txt-code\" href=\"#code-runtime-txt-code\"><code>runtime.txt</code></a>\n</h3>\n\n<p>Heroku will install a default Python version if you don't specify one, but if you want to pick your Python version, you'll need a <code>runtime.txt</code> file. Create one in the root directory, next to your <code>requirements.txt</code>, <code>manage.py</code>, <code>.gitignore</code> and the rest. Specify your Python version with the prefix <code>python-</code>, followed by the major, minor, and patch version that you want your application to run on:</p>\n\n<pre><code class=\"lang-txt\">python-3.8.2\n</code></pre>\n<h3 class=\"anchored\">\n  <a name=\"code-procfile-code\" href=\"#code-procfile-code\"><code>Procfile</code></a>\n</h3>\n\n<p>The last file we need to add is a file specific to Heroku: <a href=\"https://devcenter.heroku.com/articles/procfile\">the <code>Procfile</code></a>. This is what we use to specify the processes our application should run. The processes specified in this file will automatically boot on deploy to Heroku. Create a file named <code>Procfile</code> in the root level directory, right next to your <code>requirements.txt</code> and <code>runtime.txt</code> files. (Make sure to capitalize the P of Procfile otherwise Heroku might not recognize it!) Copy-paste the following lines into it:</p>\n\n<pre><code class=\"lang-txt\">release: python3 manage.py migrate\nweb: gunicorn gettingstarted.wsgi --preload --log-file -\n</code></pre>\n\n<p><a href=\"https://devcenter.heroku.com/articles/release-phase\">The <code>release</code> phase</a> of a Heroku deployment is the best place to run tasks, like migrations or updates. The command we will run during this phase is to simply run the <code>migrate</code> task defined in <code>manage.py</code>.</p>\n\n<p>The other process is the <code>web</code> process, which is very important, if not outright essential, for any web application. This is where we pass our Gunicorn config, the same things we need when running the server locally. We pass it our WSGI file, which is located in the <code>gettingstarted</code> directory, and then we pass a few more flags to add it a bit more configuration. The <code>--preload</code> flag ensures that the app can receive requests just a little bit faster; the <code>--logfile</code> just specifies that the log file should get routed to Heroku.</p>\n<h2 class=\"anchored\">\n  <a name=\"readying-for-deployment\" href=\"#readying-for-deployment\">Readying for deployment</a>\n</h2>\n\n<p>Take a second before moving on and just double check that you've saved and committed all of your changes to Git. Remember, we need those changes in the Git repo in order for them to successfully deploy. After that, let's get ready to make an app!</p>\n<h3 class=\"anchored\">\n  <a name=\"creating-an-app-with-code-heroku-create-code\" href=\"#creating-an-app-with-code-heroku-create-code\">Creating an app with <code>heroku create</code></a>\n</h3>\n\n<p>Since we have the Heroku CLI installed, we can <a href=\"https://devcenter.heroku.com/articles/creating-apps\">call <code>heroku create</code> on the command line to have an app generated</a> for us:</p>\n\n<pre><code class=\"lang-term\">$ heroku create\nCreating app... done, ⬢ mystic-wind-83\nCreated http://mystic-wind-83.herokuapp.com/ | git@heroku.com:mystic-wind-83.git\n</code></pre>\n\n<p>Your app will be assigned a random name—in this example, it's <code>mystic-wind-83</code>—as well as a publicly accessible URL.</p>\n<h3 class=\"anchored\">\n  <a name=\"setting-environment-variables-on-heroku\" href=\"#setting-environment-variables-on-heroku\">Setting environment variables on Heroku</a>\n</h3>\n\n<p>When we created our <code>heroku.py</code> settings file, we used Django-environ to load environment variables into our settings config. <a href=\"https://devcenter.heroku.com/articles/config-vars\">Those environment variables also need to be present in our Heroku environment</a>, so let's set those now.</p>\n\n<p>The Heroku CLI command we'll be using for this is <code>heroku config:set</code>. This will take in key-value pairs as arguments and set them in your Heroku runtime environment. First, let's configure our allowed hosts. Type the following line, and replace <code>YOUR_UNIQUE_URL</code> with the URL generated by <code>heroku create</code>:</p>\n\n<pre><code class=\"lang-term\">$ heroku config:set ALLOWED_HOSTS=&lt;YOUR_UNIQUE_URL&gt;\n</code></pre>\n\n<p>Next, let's set our Django settings module. This is what determines what settings configuration we use on this platform. Instead of using the default of <code>base</code>, we want the Heroku-specific settings:</p>\n\n<pre><code class=\"lang-term\">$ heroku config:set DJANGO_SETTINGS_MODULE=gettingstarted.settings.heroku\n</code></pre>\n\n<p>Lastly, we'll need to create a <code>SECRET_KEY</code>. For this demo, it doesn't matter what its value is. You can use a secure hash generator like <code>md5</code>, or a password manager's generator. Just be sure to keep this value secure, don't reuse it, and NEVER check it into source code! You can set it using the same CLI command:</p>\n\n<pre><code class=\"lang-term\">$ heroku config:set SECRET_KEY=&lt;gobbledygook&gt;\n</code></pre>\n<h2 class=\"anchored\">\n  <a name=\"provisioning-our-database\" href=\"#provisioning-our-database\">Provisioning our database</a>\n</h2>\n\n<p>Locally, Django is configured to use a SQLite database but we're productionizing. We need something a little bit more robust. Let's provision a Postgres database for production.</p>\n\n<p>First, let's check if we have a database already. The <a href=\"https://devcenter.heroku.com/articles/heroku-cli-commands#heroku-addons-all-app-app\"><code>heroku addons</code></a> command will tell us if one exists:</p>\n\n<pre><code class=\"lang-term\">$ heroku addons\nNo add-ons for app mystic-wind-83.\n</code></pre>\n\n<p>No add-ons exist for our app, which makes sense—we just created it! To add a Postgres database, we can use the <code>addons:create</code> command like this:</p>\n\n<pre><code class=\"lang-term\">$ heroku addons:create heroku-postgresql:hobby-dev\n</code></pre>\n\n<p>Heroku offers several tiers of Postgres databases. <code>hobby-dev</code> is the free tier, so you can play around with this without paying a dime.</p>\n<h2 class=\"anchored\">\n  <a name=\"going-live\" href=\"#going-live\">Going live</a>\n</h2>\n\n<p>It is time. Your code is ready, your Heroku app is configured, you are ready to deploy. This is the easy part!</p>\n\n<p>Just type out</p>\n\n<pre><code class=\"lang-term\">$ git push heroku master\n</code></pre>\n\n<p>And we'll take care of the rest! You'll see your build logs scrolling through your terminal. This will show you what we're installing on your behalf and where you are in the build process. You'll also see the <code>release</code> phase as well that we specified earlier.</p>\n<h2 class=\"anchored\">\n  <a name=\"scaling-up\" href=\"#scaling-up\">Scaling up</a>\n</h2>\n\n<p>The last step is to scale up our web process. This creates new dynos, or, in other words, copies of your code on Heroku servers to handle more web traffic. You can do this using the following command:</p>\n\n<pre><code class=\"lang-term\">$ heroku ps:scale web=1\n</code></pre>\n\n<p>To see your app online, enter <code>heroku open</code> on the terminal. This should pop open a web browser with the site you just built.</p>\n<h2 class=\"anchored\">\n  <a name=\"debugging\" href=\"#debugging\">Debugging</a>\n</h2>\n\n<p>If you hit some snags, don't worry, we have some tips that might help:</p>\n\n<ul>\n<li>Are all of your changes saved and checked into Git?</li>\n<li>Are your changes on the <code>master</code> branch or are they on a different branch? Make sure that whatever you're deploying, all of your changes are in that Git branch.</li>\n<li>Did you deploy from the root directory of your project? Did you also call <code>heroku create</code> from the root directory of your project? If not, this could absolutely cause a trip up.</li>\n<li>Did you remove anything from the code in the provided demo that we didn't discuss?</li>\n</ul>\n<h3 class=\"anchored\">\n  <a name=\"logging\" href=\"#logging\">Logging</a>\n</h3>\n\n<p>If you've run through this list and still have issues, take a look at your log files. In addition to your build logs—which will tell you whether your application successfully deployed or not—you have access to all logs produced by Heroku and by your application. You can get to these through a couple of different ways, but the quickest way is just to run the following command:</p>\n\n<pre><code class=\"lang-term\">$ heroku logs --tail\n</code></pre>\n<h3 class=\"anchored\">\n  <a name=\"remote-console\" href=\"#remote-console\">Remote console</a>\n</h3>\n\n<p>Another tool you have is the <code>heroku run bash</code> command. This provides you with direct access from your terminal to a Heroku dyno with your code deployed to it. If you type <code>ls</code>, you can see that this is your deployed application. It can be useful to check that what is up here matches what is locally on your machine. If not, you might see some issues.</p>\n<h2 class=\"anchored\">\n  <a name=\"wrapping-up\" href=\"#wrapping-up\">Wrapping up</a>\n</h2>\n\n<p>Congratulations on successfully deploying your productionized app onto Heroku!</p>\n\n<p>To help you learn about Heroku, we also have a wealth of technical documentation. Our <a href=\"https://devcenter.heroku.com\">Dev Center</a> is where you'll find most of our technical how-to and supported technologies information. If you're having a technical issue, chances are someone else has asked the same question and it's been answered on our help docs. Use these resources to solve your problems as well as to learn about best practices when deploying to Heroku.</p>","PublishedAt":"2020-06-22 16:00:00+00:00","OriginURL":"https://blog.heroku.com/from-project-to-productionized-python","SourceName":"Heroku"}},{"node":{"ID":362,"Title":"Evolving Alongside your Tech Stack","Description":"<p><em>This blog post is adapted from a discussion during <a href=\"https://www.heroku.com/podcasts/codeish/39-evolving-alongside-your-tech-stack\">an episode of our podcast, Code[ish]</a>.</em></p>\n\n<p>Over the last twenty years, software development has advanced so rapidly that it's possible to create amazing user experiences, powerful machine learning algorithms, and memory efficient applications with incredible ease. But as the capabilities tech provides has changed, so too have the requirements of individual developers morphed to encompass a variety of skills. Not only should you be writing efficient code; you need to understand how that code communicates with all the other systems involved and make it all work together.</p>\n\n<p>In this post, we'll explore how you can stay on top of the changing software development landscape, without sacrificing your desires to learn or the success of your product.</p>\n<h2 class=\"anchored\">\n  <a name=\"user-experience-depends-on-technical-expertise\" href=\"#user-experience-depends-on-technical-expertise\">User experience depends on technical expertise</a>\n</h2>\n\n<p>When the iPhone first came out in 2007, it was rather limited in technical capabilities. There was no support for multitasking and gestures, no ability to copy and paste text, and there wasn't any support for third-party software. It's not that these ideas were not useful, it’s just that the first generation of the phone's hardware and operating system could not support such features. This serves as a good example to underscore how UX has sometimes been constrained by technology.</p>\n\n<p>Now, the situation has changed somewhat. Tools have advanced to the point where it's really easy to create a desktop or mobile app which accepts a variety of gestures and inputs. The consequences of this are twofold. First, users have come to expect a certain level of quality in software. Gone are the days of simply \"throwing something together\"; software, websites, and mobile apps all need to look polished. This requires developers to have a high level of design sensibility (or work with someone else who does). Second, it means that the role of the engineer has expanded beyond just writing code. They need to understand why they're building whatever it is they're building, why it's important to their users, and how it functionally integrates with the rest of the app. If you design an API, for example, you’ll need to secure it against abuse; if you design a custom search index, you need to make sure users can actually find what they’re looking for.</p>\n\n<p>On the one hand, because you're running on the same devices and platforms as your users (whether that a smartphone or an operating system), you're intricately familiar with the best UI patterns—how a button should operate, which transitions to make between screens—because every other app has made similar considerations. But on the other hand, you also need to deal with details such as memory management and CPU load to ensure the app is running optimally. </p>\n\n<p>It’s not enough for an app to work well, as it must also look good. It's important to find a balance of both design sensibilities and technical limitations—or at least, a baseline knowledge of how everything works—in order to ship quality software. </p>\n<h2 class=\"anchored\">\n  <a name=\"follow-everything-but-only-learn-em-some-em-things\" href=\"#follow-everything-but-only-learn-em-some-em-things\">Follow everything but only learn <em>some</em> things</a>\n</h2>\n\n<p>When it comes to personal growth, learning to prioritize solutions to the problems you encounter can be critical in your development. For example, suppose you notice one day that your Postgres queries are executing slower than you would like. You should have a general awareness of how higher rates of traffic affects your database querying strategies, or how frequent writes affect the physical tables on disk. But that doesn't necessarily mean that you should sink a massive amount of time and effort to fine-tune these issues towards the most optimal strategy. When developing software, you will always have one of several choices to make, and rarely does one become the only true path forward. Sometimes, having the insight to know the trade-offs and accepting one sub-optimal approach above another makes it easier to cut losses and focus on the parts of your software which matter.</p>\n\n<h2 class=\"pull-quote\" style=\"font-style: italic;\">Sometimes, having the insight to know the trade-offs and accepting one sub-optimal approach above another makes it easier to cut losses and focus on the parts of your software which matter.</h2>\n\n<p>It seems like every year, a new web framework or programming language is released. This makes it difficult, if not impossible, to follow every single new item when they are announced. The inverse is also true. We might feel that adopting new technologies is one way to stay \"relevant,\" but this attitude can be quite dangerous. If you are an early adopter, you run the risk of being on the hook for finding bugs, distracting you from your actual goal of shipping features for your own application. You should take a calculated approach to the pros and cons of any new tech. For example, switching your database entirely to MemSQL because you heard it's \"faster\" is less reasonable than making a switch after reading someone's careful evaluation of the technology, and realizing that it matched your own needs as well.</p>\n<h2 class=\"anchored\">\n  <a name=\"keeping-calm-and-steady\" href=\"#keeping-calm-and-steady\">Keeping calm and steady</a>\n</h2>\n\n<p>At the end of the day, you should be very invested in your own stack and the ecosystem you work in. That work can be something as simple as reading Medium posts or following Twitter accounts. Broaden your knowledge of other services outside your own realm of expertise only if you come across someone confronting problems similar to yours. You should own tools which you know how to operate, rather than keep a shed full of all sorts of shiny objects.</p>","PublishedAt":"2020-04-29 19:59:00+00:00","OriginURL":"https://blog.heroku.com/evolving-alongside-tech-stack","SourceName":"Heroku"}},{"node":{"ID":363,"Title":"Building and Scaling a Global Chatbot using Heroku + Terraform","Description":"<p>Text-based communication has a long history weaved into the evolution of the Internet, from IRC and XMPP to Slack and Discord. And where there have been humans, there have also been chatbots: scriptable programs that respond to a user’s commands, like messages in a chat room.</p>\n\n<p>Chatbots don't require much in terms of computational power or disk storage, as they rely heavily on APIs to send actions and receive responses. But as with any kind of software, scaling them to support millions of user’s requests across the world requires a fail-safe operational strategy. Salesforce offers <a href=\"https://www.salesforce.com/products/service-cloud/features/live-agent/\">a Live Agent support product</a> with a chatbot integration that reacts to customer inquiries.</p>\n\n<p>In this post, we'll take a look at how the team uses Heroku for their chatbot's multi-regional requirements.</p>\n<h2 class=\"anchored\">\n  <a name=\"how-users-interact-with-the-chatbot\" href=\"#how-users-interact-with-the-chatbot\">How users interact with the chatbot</a>\n</h2>\n\n<p>Live Agent is an embeddable chatbot that can be added to any website or mobile app. Users can engage in a conversation with the chatbot, asking questions and performing actions along the way. For example, if a bank customer wants to learn how to set up two-factor authentication, they could ask the chatbot for guidance, rather than call the bank directly.</p>\n\n<p>The aim of Live Agent is to augment a human support agent's capabilities for responding to events that happen at a high scale. Because everybody learns and interacts a little bit differently, it's advantageous to provide help through various mediums, like videos and documentation. Chatbots offer another channel, with <em>guided</em> feedback that offers more interactive information. Rather than providing a series of webpages with static images, a chatbot can make processes friendlier by confirming to users their progress as they go through a sequence of steps.</p>\n\n<p>Live Agent hooks into <a href=\"https://developer.salesforce.com/docs/atlas.en-us.apexcode.meta/apexcode/apex_intro_what_is_apex.htm\">Apex</a>, a Java-like programming language that is tied directly into Salesforce's object models, allowing it to modify and call up CRM records directly. You can also have a Live Agent chatbot call out to any API and pretty much do anything on the web.</p>\n\n<p>With their open-ended nature, chatbots can perform endless operations across a variety of communication platforms. Facebook Messenger, for example, is <a href=\"https://www.businessinsider.com/most-used-smartphone-apps-2017-8?r=US&amp;IR=T#6-instagram-5\">the third most popular app in the world</a>, and you could have a Live Agent backend running on <a href=\"https://developers.facebook.com/docs/messenger-platform\">the Messenger platform</a> to respond to user queries.</p>\n<h2 class=\"anchored\">\n  <a name=\"running-live-agent-on-heroku\" href=\"#running-live-agent-on-heroku\">Running Live Agent on Heroku</a>\n</h2>\n\n<p>With such a large scope across disparate mediums, there's a significant number of requests coming into Live Agent chatbots and vast amounts of data they can access. It may surprise you to learn that there are only eight engineers responsible for running Live Agent! In addition to coding the features, they own the entire product. This means that they are also responsible for being on-call for pager rotations and ensuring that the chatbots can keep up with incoming traffic.</p>\n\n<p>The small team didn't want to waste time configuring their platform to run on bare metal or on a cloud VM, and they didn't want the administrative overhead of managing databases or other third-party services. Since Salesforce customers reside all over the world, the Live Agent chatbots must also be highly available across multiple regions.</p>\n\n<p>The Live Agent team put its trust into Heroku to take care of all of those operational burdens. Heroku already manages millions of Postgres databases for our customers, and we have a dedicated staff to manage backups, perform updates, and respond to potential outages. The Live Agent chatbot runs on Java, and Heroku's platform supports the entire Java ecosystem, with dedicated Java experts to handle language and framework updates, providing new features and responding to security issues.</p>\n\n<p>In order to serve their customers worldwide, the core Live Agent infrastructure matches <a href=\"https://devcenter.heroku.com/articles/regions\">Heroku's availability in every region around the world</a>. All of their services are managed by Heroku, ensuring that their Heroku Postgres, Redis, and Apache Kafka dependencies are blazing fast no matter where a request comes from.</p>\n\n<p>The beauty of it all is how simple it is to scale, without any of Live Agent's team needing to be responsible for any of the maintenance and upkeep.</p>\n<h2 class=\"anchored\">\n  <a name=\"leveraging-terraform-for-replication-and-private-spaces-for-security\" href=\"#leveraging-terraform-for-replication-and-private-spaces-for-security\">Leveraging Terraform for replication and Private Spaces for security</a>\n</h2>\n\n<p>The Live Agent platform is comprised of ten separate apps, each with their own managed add-ons and services. To fully isolate the boundaries of communication, the collection of apps are deployed into a <a href=\"https://www.heroku.com/private-spaces\">Heroku Private Space</a>. Private Spaces establish an isolated runtime for the apps to ensure that the data contained within the network is inaccessible from any outside service.</p>\n\n<p>Private Spaces are available in a variety of regions; if a new region becomes available, the Live Agent team wanted to be able to automatically redeploy the same apps and add-ons there. And if they ever need to create a new app, they also wanted to add it to all of the Private Spaces in those geographic locations.</p>\n\n<p>To easily replicate their architecture, the Live Agent team uses <a href=\"https://www.terraform.io/intro/index.html\">Terraform</a> to automate deployment and configuration of the Live Agent platform. Terraform is the driver behind everything they do on Heroku. With it, they can explicitly and programmatically define their infrastructure--the apps and add-ons, custom domains, and logging and profiling setup--and have it securely available in any region, instantly. Whenever a new configuration is necessary, they can implement that update with just a few lines of code and make it live everywhere with the merge of a pull request.</p>\n\n<p>For example, to automatically set up a Node.js Heroku app that requires a Postgres database and logging through <a href=\"https://elements.heroku.com/addons/papertrail\">Papertrail</a>, a Terraform config file might just look something like this:</p>\n\n<pre><code class=\"language-hcl\">resource \"heroku_app\" \"server\" {\n  name = \"my-app\"\n  region = \"us\"\n\n\n  provisioner \"local-exec\" {\n    command = \"heroku buildpacks:set heroku/nodejs --app ${heroku_app.server.name}\"\n  }\n}\n\nresource \"heroku_addon\" \"database\" {\n  app  = \"${heroku_app.server.name}\"\n  plan = \"heroku-postgresql:hobby-dev\"\n}\n\n# Papertrail addon (for logging)\n\nresource \"heroku_addon\" \"logging\" {\n  app = \"${heroku_app.server.name}\"\n  plan = \"papertrail:choklad\"\n}\n</code></pre>\n\n<p>Here are some details on how to <a href=\"https://devcenter.heroku.com/articles/using-terraform-with-heroku\">use Terraform with Heroku</a>.</p>\n<h2 class=\"anchored\">\n  <a name=\"learning-more\" href=\"#learning-more\">Learning more</a>\n</h2>\n\n<p>If you'd like to learn more about how Live Agent uses Heroku to scale their platform, our podcast Code[ish], has <a href=\"https://www.heroku.com/podcasts/codeish/30-the-infrastructure-behind-salesforces-chatbots\">an interview with their team</a>, where they dive into more of the technical specifics.</p>\n\n<p>We also have not <a href=\"https://dev.to/heroku/eight-devops-things-heroku-does-so-you-don-t-have-to-4o0b\">one</a> but <a href=\"https://dev.to/heroku/seven-more-devops-things-heroku-does-so-you-don-t-have-to-1g1n\"><em>two</em></a> posts on <a href=\"https://dev.to/\">dev.to</a> listing all the DevOps chores which Heroku automatically takes care of for you.</p>","PublishedAt":"2020-04-22 15:33:17+00:00","OriginURL":"https://blog.heroku.com/chatbots-with-heroku-terraform","SourceName":"Heroku"}},{"node":{"ID":364,"Title":"Building with Web Components","Description":"<p>In the early years of web development, there were three standard fundamentals upon which every website was built: HTML, CSS, and JavaScript. As time passed, web developers became more proficient in their construction of fancy UI/UX widgets for websites. With the need for newer ways of crafting a site coming in conflict with the relatively slow adoption of newer standards, more and more developers began to build their own libraries to abstract away some of the technical details. The web ceased being a standard: now your website could be a React site, or an Angular site, or a Vue site, or any number of other web framework that are not interoperable with each other.</p>\n\n<p>Web components seek to tilt the balance of web development back towards a standard agreed upon by browser vendors and developers. Various polyfills and proprietary frameworks have achieved what web components are now trying to standardize: composable units of JavaScript and HTML that can be imported and reused across web applications. Let's explore the history of web components and the advantages they provide over third-party libraries.</p>\n<h2 class=\"anchored\">\n  <a name=\"how-it-all-began\" href=\"#how-it-all-began\">How it all began</a>\n</h2>\n\n<p>After some attempts by browser vendors to create a standard—and subsequent slow progress—front-end developers realized it was up to them to create a browser-agnostic library delivering on the promise of the web components vision. When React was released, it completely changed the paradigm of web development in two key ways. First, with a bit of JavaScript and some XML-like syntax, React allowed you to compose custom HTML tags it called components:</p>\n\n<pre><code class=\"lang-javascript\">class HelloMessage extends React.Component {\n  render() {\n    return (\n    &lt;h1&gt;\n        Hello &lt;span class=\"name\"&gt;{this.props.name}&lt;/span&gt;\n    &lt;/h1&gt;\n    );\n  }\n}\n\nReactDOM.render(\n  &lt;HelloMessage name=\"Johnny\" /&gt;,\n  document.getElementById('hello-example-container')\n);\n</code></pre>\n\n<p>This trivial example shows how you can encapsulate logic to create React components which can be reused across your app and shared with other developers.</p>\n\n<p>Second, React popularized the concept of <a href=\"https://reactkungfu.com/2015/10/the-difference-between-virtual-dom-and-dom/\">a virtual DOM</a>. The DOM is your entire HTML document, all the HTML tags that a browser slurps up to render a website. However, the relationship between HTML tags, JavaScript, and CSS which make up a website is rather fragile. Making changes to one component could inadvertently affect other aspects of the site. One of the benefits of the virtual DOM was to make sure that UI updates only redrew specific chunks of HTML through JavaScript events. Thus, developers could easily build websites rendering massive amounts of changing data without necessarily worrying about the performance implications.</p>\n\n<p>Around 2015, Google began developing <a href=\"https://www.polymer-project.org/\">the Polymer Project</a> as a means of demonstrating how they wanted web standards to evolve through polyfills. Over the years and various releases, the ideas presented by Polymer library began to be incorporated by the W3C for standardization and browser adoption. The work started back in <a href=\"https://www.w3.org/TR/2012/WD-components-intro-20120522/\">2012 by the W3C</a> (and originally introduced by <a href=\"https://fronteers.nl/congres/2011/sessions/web-components-and-model-driven-views-alex-russell\">Alex Russell at Fronteers Conference 2011</a>) began to get more attention, undergoing various design changes to address developers' concerns.</p>\n<h2 class=\"anchored\">\n  <a name=\"the-web-components-toolkit\" href=\"#the-web-components-toolkit\">The web components toolkit</a>\n</h2>\n\n<p>Let's take a look at the web standards which make up web components today.</p>\n<h3 class=\"anchored\">\n  <a name=\"custom-elements\" href=\"#custom-elements\">Custom elements</a>\n</h3>\n\n<p><a href=\"https://developer.mozilla.org/en-US/docs/Web/Web_Components/Using_custom_elements\">Custom elements</a> allows you to create custom HTML tags which can exhibit any JavaScript behavior:</p>\n\n<pre><code class=\"lang-javascript\">class SayHello extends HTMLElement {\n  constructor() {\n    super();\n\n    let p = document.createElement(“p”);\n    let text = document.createTextNode(“Hello world!”);\n    p.appendChild(text);\n\n    this.appendChild(p);\n  }\n}\n\ncustomElements.define('say-hello', SayHello);\n</code></pre>\n\n<p>Custom elements can be used to encapsulate logic across your site and reused wherever necessary. Since they're a web standard, you won't need to load an additional JavaScript framework to support them.</p>\n<h3 class=\"anchored\">\n  <a name=\"html-templates\" href=\"#html-templates\">HTML templates</a>\n</h3>\n\n<p>If you need to reuse markup on a website, it can be helpful to make use of <a href=\"https://developer.mozilla.org/en-US/docs/Web/Web_Components/Using_templates_and_slots\">an HTML template</a>. HTML templates are ignored by the browser until they are called upon to be rendered. Thus, you can create complicated blocks of HTML and render them instantaneously via JavaScript.</p>\n\n<p>To create an HTML template, all you need to do is wrap up your HTML with the new <code>&lt;template&gt;</code> tag:</p>\n\n<pre><code class=\"lang-html\">&lt;template id=\"template\"&gt;\n  &lt;script&gt;\n    const button = document.getElementById('click-button');\n    button.addEventListener('click', event =&gt; alert(event));\n  &lt;/script&gt;\n  &lt;style&gt;\n    #click-button {\n    border: 0;\n    border-radius: 4px;\n    color: white;\n    font-size: 1.5rem;\n    padding: .5rem 1rem;\n    }\n  &lt;/style&gt;\n  &lt;button id=\"click-button\"&gt;Click Me!&lt;/button&gt;\n&lt;/template&gt;\n</code></pre>\n<h3 class=\"anchored\">\n  <a name=\"shadow-dom\" href=\"#shadow-dom\">Shadow DOM</a>\n</h3>\n\n<p><a href=\"https://developer.mozilla.org/en-US/docs/Web/Web_Components/Using_shadow_DOM\">The shadow DOM</a> is another concept which provides support for further web page encapsulation. Any elements within the shadow DOM are not affected by the CSS styles of any other markup on the page, and similarly, any CSS defined within the shadow DOM doesn't affect other elements. They can also be configured to not be affected by external JavaScript, either. Among other advantages, this results in lower memory usage for the browser and faster render times. If it's helpful, you can think of elements in the shadow DOM as more secure <code>iframe</code>s.</p>\n\n<p>To add an element to the shadow DOM, you call <code>attachShadow()</code> on it:</p>\n\n<pre><code class=\"lang-javascript\">class MyWebComponent extends HTMLElement {\n    constructor() {\n        super();\n        this.attachShadow({ mode: \"open\" });\n    }\n    connectedCallback() {\n        this.shadowRoot.innerHTML = `\n            &lt;p&gt;I'm in the Shadow Root!&lt;/p&gt;\n        `;\n    }\n}\n\nwindow.customElements.define(\"my-web-component\", MyWebComponent);\n</code></pre>\n\n<p>This creates a custom element, <code>&lt;my-web-component&gt;</code>, whose <code>p</code> tag would not be affected by any other styles on the page.</p>\n<h2 class=\"anchored\">\n  <a name=\"web-component-ecosystems\" href=\"#web-component-ecosystems\">Web component ecosystems</a>\n</h2>\n\n<p>The greatest advantage web components have over using a library is their ability to provide standards-compliant, <em>composable</em> HTML elements. What this means is that if you have built a web component, you can package it up as a release for other developers to consume as a dependency in their project, just like any other Node or Ruby package, and those developers can be assured that that web component will work across all (well, most) web browsers without requiring the browser to load a front-end framework like React, Angular, or Vue.</p>\n\n<p>To give an example, <a href=\"https://github.com/halvves/shader-doodle\">Shader Doodle</a> is a custom element which sets up the ability to easily create fragment shaders. Developers who need this functionality can just fetch the package and insert it as a <code>&lt;shader-doodle&gt;</code> tag in their HTML, rather than creating the functionality of Share Doodle from scratch.</p>\n\n<p>Now, with the great interoperability that web components give you, many frameworks and libraries like Vue or React have started to provide the option to generate web components out of their proprietary code. That way you don't have to learn all the low-level APIs of the aforementioned standards, and can instead focus on coding. There many other libraries for creating web components, like <a href=\"https://github.com/polymer\">Polymer</a>, <a href=\"https://github.com/x-tag/core\">X-Tag</a>, <a href=\"https://github.com/slimjs/slim.js\">slim.js</a>, <a href=\"https://github.com/riot/riot\">Riot.js</a>, and <a href=\"https://github.com/ionic-team/stencil\">Stencil</a>.</p>\n\n<p>Another great example of this are Salesforce’s <a href=\"https://lwc.dev/\">Lightning Web Components</a>, a lightweight framework that abstracts away the complexity of the different web standards. It provides a standards-compliant foundation for building web components which can be used in any project.</p>\n<h2 class=\"anchored\">\n  <a name=\"getting-more-involved-web-components\" href=\"#getting-more-involved-web-components\">Getting more involved web components</a>\n</h2>\n\n<p>We recorded <a href=\"https://www.heroku.com/podcasts/codeish/38-building-with-web-components\">an episode of Code[ish], our podcast on all things tech, that meticulously went through the history (and future!) of web components</a>. Be sure to check out that interview from <a href=\"https://www.manning.com/books/web-components-in-action\">someone who literally wrote the book on web components</a>.</p>\n\n<p>You can also join <a href=\"https://polymer.slack.com/messages/general/\">the Polymer Slack workspace</a> to chat with other web developers about working with these standards.</p>","PublishedAt":"2020-03-04 16:45:00+00:00","OriginURL":"https://blog.heroku.com/building-with-web-components","SourceName":"Heroku"}},{"node":{"ID":365,"Title":"Chrome's Changes Could Break Your App: Prepare for SameSite Cookie Updates","Description":"<p>In this post, we will cover changes coming to Chrome (and other browsers) that affect how third-party cookies are handled—specifically <code>SameSite</code> changes, how to test to see if your site is impacted and how to fix it.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1580744350-samesite-twitter.png\" alt=\"Blog post cover image showing a jar with cookies in it\"></p>\n\n<ul>\n<li>⚓️ <a href=\"https://blog.heroku.com/chrome-changes-samesite-cookie#what-is-code-samesite-code-and-why-the-big-change\">What is <code>SameSite</code> and why the big change?</a>\n</li>\n<li>⚓️ <a href=\"https://blog.heroku.com/chrome-changes-samesite-cookie#prepare-for-chrome-80-updates\">Prepare for Chrome 80 updates</a>\n\n<ul>\n<li>⚓️ <a href=\"https://blog.heroku.com/chrome-changes-samesite-cookie#step-1-enabling-code-samesite-code-chrome-flags-and-test-to-see-if-your-site-faces-potential-code-samesite-code-errors\">Step 1: Enabling <code>SameSite</code> Chrome flags and test to see if your site faces <code>SameSite</code> errors</a>\n</li>\n<li>⚓️ <a href=\"https://blog.heroku.com/chrome-changes-samesite-cookie#step-2-fixing-cookie-errors-using-appropriate-attributes\">Step 2: Fixing cookie errors using appropriate attributes</a>\n</li>\n</ul>\n</li>\n</ul>\n<h2 class=\"anchored\">\n  <a name=\"what-is-code-samesite-code-and-why-the-big-change\" href=\"#what-is-code-samesite-code-and-why-the-big-change\">What is <code>SameSite</code> and why the big change?</a>\n</h2>\n\n<p>Back in May 2019, Chrome announced its plan to develop a secure-by-default model for handling cookies. This initiative highlights Chrome’s promise of a more secure and faster browsing experience. Chrome's goal is to increase transparency, choice and control. Users should be aware of how they are tracked, who is tracking them, and ways to control the information shared. With the influx of privacy concerns and potential cross-site attacks, Chrome is taking action to protect its users. These changes will dramatically impact advertisers, publishers, or any company relying on cookies to target their audience. Be sure to prepare in advance so your users won't experience disruptions.</p>\n\n<p>Now, the day is finally at hand. Starting February 4, 2020, Google Chrome will stop sending third-party cookies in cross-site requests unless the cookies are secured and flagged using an IETF standard called <a href=\"https://web.dev/samesite-cookies-explained/\"><strong><code>SameSite</code></strong></a>.</p>\n<h3 class=\"anchored\">\n  <a name=\"what-does-this-mean-what-are-third-party-cookies-what-are-cross-site-request\" href=\"#what-does-this-mean-what-are-third-party-cookies-what-are-cross-site-request\">What does this mean? What are third-party cookies? What are cross-site request?</a>\n</h3>\n\n<p>When you visit a website, a browser cookie is generated and saved inside a folder in your web browser. This browser cookie is then used as a way to identify you and provide a personalized browsing experience. </p>\n\n<p>There are two types of cookies — first-party and third-party. Both types can hold the same information; however, they are accessed and created differently.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1580750955-Same-Site%20Cookie%20Comparison.png\" alt=\"SameSite Cookie Comparison\"></p>\n\n<p>As illustrated above, if you visit website  <code>a.com</code> and you attempt to access a service from the same domain name <code>a.com</code>,  cookies generated will be considered first-party cookies. Being that the cookies were created by the same site, you'll be able to enjoy same-site luxuries while visiting <code>a.com</code>'s web service. These luxuries include saved login information, shopping cart items, site preferences, etc.</p>\n\n<p>Whereas, if you visit a website  <code>a.com</code>  but that page includes content (image, iframe, etc.) from a different domain name  <code>b.com</code>, cookies set by <code>b.com</code> will be considered third-party cookies because they come from a different name than in the URL bar:  <code>a.com</code>.</p>\n\n<p>These cookies were created by a different site and <code>b.com</code> accessing them from  <code>a.com</code>  (or any other domain) would constitute a cross-site request. A page on <code>a.com</code> making requests to <code>b.com</code> (for images, iframes, etc.) is what allows services like Facebook, Google Analytics, Doubleclick, etc. to track users and provide online-advertisements. In that example, Facebook, Google, and Doubleclick are the <code>b.com</code>. This allows, for example, Doubleclick to show targeted ads to you on multiple other sites you visit, like a news site, a hotel site, or a blog you read.</p>\n\n<p>As previously stated, Google Chrome will stop sending third-party cookies in cross-site requests unless the cookies are secured and flagged using an IETF standard called <a href=\"https://web.dev/samesite-cookies-explained/\"><strong><code>SameSite</code></strong></a>. In other words, the content from <code>b.com</code> (images, iframe, etc.) on <code>a.com</code>’s page will no longer be able to access <code>b.com</code>'s cookies unless those cookies are secured and flagged appropriately.</p>\n<h3 class=\"anchored\">\n  <a name=\"why-is-google-making-such-a-huge-change\" href=\"#why-is-google-making-such-a-huge-change\">Why is Google making such a huge change?</a>\n</h3>\n\n<p>Sharing cross-site cookies is not always an issue; however, it has the potential for abuse. Google Chrome's current behavior allows third-party websites to access all cookies by default.  This creates the possibility of <a href=\"https://developer.mozilla.org/en-US/docs/Glossary/CSRF\"><strong>cross-site request forgery</strong></a> (CSRF) attacks, other security vulnerabilities and privacy leaks.</p>\n<h3 class=\"anchored\">\n  <a name=\"what-s-cross-site-request-forgery-csrf\" href=\"#what-s-cross-site-request-forgery-csrf\">What’s cross-site request forgery (CSRF)?</a>\n</h3>\n\n<p>Cross-site request forgery is a web security vulnerability that allows a hacker to exploit users through session surfing or one-click attacks. For example, hackers can trick an innocent user to click a believable link. If this user is already logged into a website the hacker wants to access, the hacker can surf on the already authenticated session and make request to a site the user didn't intend to make. Being that the user already authenticated, the site cannot distinguish between the forged or legitimate request.</p>\n\n<p>There are a few ways to create these malicious commands: image tags, link tags, hidden forms, and JavaScript XMLHttpRequests. With Chrome's current default behavior, the requested cookie will be sent by default, and the hacker will have access to the user's session, which means they are effectively logged in as the user. To fight against this web vulnerability, web frameworks often require unique tokens/identifiers that are not accessible to attackers and would not be sent along (like cookies) with requests.</p>\n\n<p>As an example, let’s assume you sign into your bank account.</p>\n\n<pre><code>www.bankpal.com\n</code></pre>\n\n<p>While browsing your transaction history, you get an email letting you know about a recent suspicious transaction. To investigate further, the email requires you to log into your bank account. It provides a convenient link for you as well. </p>\n\n<p>💡Note: You are still logged in to BankPal in another tab.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1580744964-Cross-Forgery%20Email%20%281%29.png\" alt=\"BankPal Cross-Forgery Email\"></p>\n\n<p>The link’s HTML is as follows:</p>\n\n<pre><code class=\"language-html\">&lt;a href=\"http://www.bank.com/transfer?acct=888888&amp;amount=100000\"&gt;Log In&lt;/a&gt;\n</code></pre>\n\n<p>The hacker has already studied BankPal so they know how to mimic account transfers quite well. For example, here is how BankPal typically creates money transfers:</p>\n\n<pre><code>GET http://www.bankpal.com/transfer?acct=AccountId&amp;amount=DollarAmount HTTP/1.1\n</code></pre>\n\n<p>This hacker has sent this email to a large number of bank customers and they know at least one person will click this believable link. </p>\n\n<p>You are that one customer.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1580745029-BUTTON%20click.png\" alt=\"Clicking log in button in email\"></p>\n\n<p>From a quick glance, this email looks legitimate. There is no way it could be suspicious and cause harm to a user's account. It even has the BankPal logo! With full trust, you click the link. Since you are already authenticated in the previous tab, clicking that link ends up creating an unauthorized transaction behind the scenes. This attacker has forged your identity, transferred $100,000 from your account, and has completely ruined your life (or at least your bank account) in seconds.</p>\n\n<p>Let's say BankPal only allows a <code>POST</code> request for money transfers. It would be impossible to create a malicious request using an <code>&lt;a href&gt;</code> tag. This attacker could very well create a <code>&lt;form&gt;</code> tag instead with automatic execution of the embedded JavaScript.</p>\n\n<p>This form's HTML code could look like this:</p>\n\n<pre><code class=\"language-html\"> &lt;body onload=\"document.forms[0].submit()\"&gt;\n   &lt;form action=\"http://www.bankpal.com/transfer\" method=\"POST\"&gt;\n     &lt;input type=\"hidden\" name=\"acct\" value=\"AttackerAccountId\"/&gt;\n     &lt;input type=\"hidden\" name=\"amount\" value=\"100000\"/&gt;\n     &lt;input type=\"submit\" value=\"Log In\"/&gt;\n   &lt;/form&gt;\n &lt;/body&gt;\n</code></pre>\n\n<p>In a real-life scenario, the example above would not happen. Banks prevent CSRF attacks using dynamically generated session tokens, session timeouts and other preventive methods. And now, with the <code>SameSite</code> attribute <code>Strict</code> (read more below), banks have yet another preventive measure. Large companies have found methods of protection; however, there are lots of smaller websites without protection. If an attacker can forge a transaction, they can also forge a password reset request, an email change request, and then gain full control of an account or web application.</p>\n<h3 class=\"anchored\">\n  <a name=\"how-is-chrome-protecting-users-against-csrf-attacks\" href=\"#how-is-chrome-protecting-users-against-csrf-attacks\">How is Chrome protecting users against CSRF attacks?</a>\n</h3>\n\n<p>To alleviate this issue, Chrome version 51 (2016-05-25) introduced the concept of the SameSite attribute. With the SameSite attribute, website developers have the power to set rules around how cookies are shared and accessed.  </p>\n\n<p>The <code>SameSite</code> attribute can be set with the following values: <strong><code>Strict</code></strong>, <strong><code>Lax</code></strong>, or <strong><code>None</code></strong>.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1580745160-Same-Site%20Cookie%20Infographic.png\" alt=\"SameSite Cookie Infographic\"></p>\n\n<p><strong>Strict: Restricts cross-site sharing altogether.</strong>\nCookies with this setting can be accessed only when visiting the domain from which it was initially set. In other words, <code>Strict</code> completely blocks a cookie being sent to <code>a.com</code> when a page from <code>b.com</code> makes the request. Even when clicking a top-level link on a third-party domain to your site, the browser will refuse to send the cookie. This option would be best for applications that require high security, such as banks.</p>\n\n<p><strong>Lax: All the sites belonging to the same domain can set and access cookies.</strong>\nUnlike <code>None</code> where cookies are always sent, <code>Lax</code> cookies are only sent on same-site request like <code>Strict</code>. However,  <code>Lax</code> allows top-level (sometimes called <a href=\"https://publicsuffix.org/\">public suffix</a>) navigation access with a safe HTTP method, like HTTP <code>GET</code>. The cookie will not be sent with cross-domain <code>POST</code> requests or when loading the site in a cross-origin frame, but it will be sent when you navigate to the site via a standard top-level <code>&lt;a href=...&gt;</code> link.</p>\n\n<p><strong>None: Allows third-party cookies to track users across sites.</strong>\nCookies with this setting will work the same way as cookies work today. Cookies will be able to be used across sites. \n💡Note that you need both the <code>None</code> and <code>Secure</code> attributes together. If you just specify <code>None</code> without <code>Secure</code> the cookie will be rejected. <code>Secure</code> ensures that the browser request is sent by a secure (HTTPS) connection.</p>\n<h3 class=\"anchored\">\n  <a name=\"real-world-example-of-the-difference-between-code-strict-code-and-code-lax-code\" href=\"#real-world-example-of-the-difference-between-code-strict-code-and-code-lax-code\">Real-world example of the difference between <code>Strict</code> and <code>Lax</code></a>\n</h3>\n\n<p>The <code>None</code> attribute is pretty understandable; however, there seems to be confusion around <code>Strict</code> and <code>Lax</code>. Let's dive into a real-world example.</p>\n\n<p>Let's say you are the CEO of TalkToMe, Inc., a feature rich commenting system. You allow your users to embed TalkToMe on their websites and they gain social network integration, advanced moderation options and other extensive community functions. If TalkToMe, Inc.'s first-party cookies are set to <code>Lax</code>, your customers are still able to access their embedded comments. If TalkToMe, Inc.'s first-party cookies are set to <code>Strict</code>, your customers will not be able to access data from an external site.</p>\n<h3 class=\"anchored\">\n  <a name=\"chrome-80-code-samesite-code-update\" href=\"#chrome-80-code-samesite-code-update\">Chrome 80 <code>SameSite</code> update</a>\n</h3>\n\n<p>With the Chrome 51 update, Google gave website developers power to set rules around how cookies are shared; however, many developers don't follow the recommended practice. Instead of leaving the user's cookies exposed to potential security vulnerabilities (allowing third-party requests by default), the Chrome 80 update takes the power back and sets all cookies to <code>SameSite=Lax</code> by default. In other words, Chrome has decided to make all cookies limited to first-party context by default, and will require developers to mark a cookie as needing third-party visibility using <code>SameSite=None</code> explicitly.</p>\n\n<blockquote>\n<p>“We’ve been focused on giving users transparency and choice over how they are tracked on the web through easy to use controls.”</p>\n<p>- Ben Galbraith, Director, Chrome Product Management</p>\n</blockquote>\n<h3 class=\"anchored\">\n  <a name=\"will-this-change-break-anything\" href=\"#will-this-change-break-anything\">Will this change break anything?</a>\n</h3>\n\n<p>This <code>SameSite</code> update requires explicit labeling for third-party cookies. Cookies that aren’t labeled appropriately may cease to function in Chrome. Even more than that: all cookies previously set may no longer be accessible.</p>\n<h3 class=\"anchored\">\n  <a name=\"how-many-users-will-this-change-affect\" href=\"#how-many-users-will-this-change-affect\">How many users will this change affect?</a>\n</h3>\n\n<p>According to the online traffic monitor <a href=\"https://gs.statcounter.com/browser-market-share\">StatCounter</a>, Chrome is the most popular web browser, and this change will affect <strong>64%</strong> of the world’s internet users in 2020. Keep reading to find out how you can keep this change from affecting your users!</p>\n<h3 class=\"anchored\">\n  <a name=\"will-my-website-be-affected\" href=\"#will-my-website-be-affected\">Will my website be affected?</a>\n</h3>\n\n<p>If either of the following is true, you will be affected and you must update your cookies:</p>\n\n<ul>\n<li>If your website integrates with external services for advertising, content recommendations, third-party widgets, social media embeds, or any custom integration that relies on cookies</li>\n<li>If your website uses non-secure (HTTP rather than HTTPS) browser access</li>\n</ul>\n<h2 class=\"anchored\">\n  <a name=\"prepare-for-chrome-80-updates\" href=\"#prepare-for-chrome-80-updates\">Prepare for Chrome 80 updates</a>\n</h2>\n<h3 class=\"anchored\">\n  <a name=\"step-1-enabling-code-samesite-code-chrome-flags-and-test-to-see-if-your-site-faces-potential-code-samesite-code-errors\" href=\"#step-1-enabling-code-samesite-code-chrome-flags-and-test-to-see-if-your-site-faces-potential-code-samesite-code-errors\">Step 1: Enabling <code>SameSite</code> Chrome flags and test to see if your site faces potential <code>SameSite</code> errors</a>\n</h3>\n\n<p>As of Chrome 76, you can enable the new <code>#same-site-by-default-cookies</code> flag and test your site before the February 4, 2020 deadline.</p>\n\n<p>Let's enable the flag:</p>\n\n<ol>\n<li>Go to chrome://flags/\n<img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1580746066-image-16.png\" alt=\"image-16\">\n</li>\n<li>Enable #same-site-by-default-cookies and #cookies-without-same-site-must-be-secure\n<img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1580746263-Screen%20Shot%202020-01-09%20at%206.28.20%20PM.png\" alt=\"Screen Shot 2020-01-09 at 6\">\n</li>\n<li>Restart the browser for the changes to take effect.\n<img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1580746297-Screen%20Shot%202020-01-09%20at%206.26.57%20PM.png\" alt=\"Screen Shot 2020-01-09 at 6\">\n</li>\n<li>Visit your website and see if you can spot error messages in the console of your browser's dev tools.\n<img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1580746311-Screen%20Shot%202019-12-17%20at%204.44.39%20PM.png\" alt=\"Screen Shot 2019-12-17 at 4\">\n</li>\n</ol>\n\n<p>If you see error messages like the one above, this means your site is not ready for the 2020 Chrome 80 release. You should continue reading to learn how to set your cookies.</p>\n<h3 class=\"anchored\">\n  <a name=\"step-2-fixing-cookie-errors-using-appropriate-attributes\" href=\"#step-2-fixing-cookie-errors-using-appropriate-attributes\">Step 2: Fixing cookie errors using appropriate attributes</a>\n</h3>\n<h4 class=\"anchored\">\n  <a name=\"common-use-cases-auditing-your-cookie-usage\" href=\"#common-use-cases-auditing-your-cookie-usage\">Common use cases: Auditing your cookie usage</a>\n</h4>\n\n<p><a href=\"https://www.chromium.org/updates/same-site\">Chrome</a>, <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Cookies#SameSite_cookies\">Firefox</a>, <a href=\"https://blogs.windows.com/msedgedev/2018/05/17/samesite-cookies-microsoft-edge-internet-explorer/\">Edge</a>, and other browsers will also change their default cookie behavior to the following:</p>\n\n<ol>\n<li>Cookies without a <code>SameSite</code> attribute will be treated as <code>SameSite=Lax</code> (See variants below), meaning all cookies will be restricted to first-party context only. <strong>If you need third-party access, you will need to update your cookies.</strong>\n</li>\n<li>Cookies needing third-party access must specify <code>SameSite=None; Secure</code> to enable access.</li>\n</ol>\n\n<p>If you don't know whether you provide cookies that are intended for cross-site usage, some common use-cases are</p>\n\n<ul>\n<li>You present ads on your website.</li>\n<li>You present content in an <code>&lt;iframe&gt;</code>.</li>\n<li>You present content within a WebView.</li>\n<li>You present images from another site on your website.</li>\n<li>You embed content shared from other sites, such as videos, maps, code samples, chat widgets and social post.</li>\n<li>You use third-party services on your website like Facebook, Twitter, Instagram, LinkedIn, Gravatar, Google Calendar, User Tracking (CrazyEgg, Google Analytics, etc.), CRM and/or reservations, booking, anti-fraud and payments services.</li>\n</ul>\n\n<p>💡 NOTE:\nCookie warnings triggered from domains you don't control will need to be set appropriately by the domain owner. If you are getting a warning like this from Google, Google will have to set this cookie appropriately. If the warning messages list a domain you control, you will need to add the correct attributes.  </p>\n\n<pre><code>(index):1 A cookie associated with a resource at http://google.com/ was set with \nSameSite=None but without Secure. A future release of Chrome will only deliver \ncookies marked SameSite=None if they are also marked Secure. You can review cookies\n in developer tools under Application&gt;Storage&gt;Cookies and see more details at \n https://www.chromestatus.com/feature/5633521622188032.\n</code></pre>\n<h4 class=\"anchored\">\n  <a name=\"knowing-which-attribute-to-use\" href=\"#knowing-which-attribute-to-use\">Knowing which attribute to use</a>\n</h4>\n\n<p>First, a quick recap of <code>SameSite</code> attributes:</p>\n\n<table>\n<thead>\n<tr>\n<th>Value</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Strict</strong></td>\n<td>Cookies with this setting can be accessed only when visiting the domain from which it was initially set. In other words, <code>Strict</code> completely blocks a cookie being sent to <code>a.com</code> when it is being sent from a page on <code>b.com</code> (i.e. <code>b.com</code> is in the URL bar). Even when clicking a top-level link on a third-party domain to your site, the browser will refuse to send the cookie. This option would be best for applications that require high security, such as banks.</td>\n</tr>\n<tr>\n<td><strong>Lax</strong></td>\n<td>Unlike <code>None</code> where cookies are always sent, <code>Lax</code> cookies are only sent on same-site request like <code>Strict</code>. However,  <code>Lax</code> allows top-level navigation access with a safe HTTP method, like HTTP <code>GET</code>. The cookie will not be sent with cross-domain <code>POST</code> requests or when loading the site in a cross-origin frame, but it will be sent when you navigate to the site via a standard top-level <code>&lt;a href=...&gt;</code> link.</td>\n</tr>\n<tr>\n<td><strong>None</strong></td>\n<td>Cookies with this setting will work the same way as cookies work today. Cookies will be able to be used across sites. 💡Note that you need both the <code>None</code> and <code>Secure</code> attributes together. If you just specify <code>None</code> without <code>Secure</code> the cookie will be rejected. <code>Secure</code> ensures that the browser request is sent by a secure (HTTPS) connection.</td>\n</tr>\n</tbody>\n</table>\n\n<p><strong>🍪 When to use <code>SameSite=Strict</code></strong></p>\n\n<p>Use when the domain in the URL bar equals the cookie’s domain (first-party) AND the link isn’t coming from a third-party.</p>\n\n<pre><code>Set-Cookie: first_party_var=value; SameSite=Strict\n</code></pre>\n\n<p><strong>🍪 When to use <code>SameSite=Lax</code></strong></p>\n\n<p>Use when the domain in the URL bar equals the cookie’s domain (first-party). Note: Third party content (images, iframes, etc.) is allowed.</p>\n\n<pre><code>Set-Cookie: first_party_var=value; SameSite=Lax\n</code></pre>\n\n<p><strong>🍪 When to use <code>SameSite=None; Secure</code></strong></p>\n\n<p>Use when you don't need cross-domain limitations.</p>\n\n<pre><code>Set-Cookie: third_party_var=value; SameSite=None; Secure\n</code></pre>\n\n<p><strong>Common scenarios</strong></p>\n\n<table>\n<thead>\n<tr>\n<th>When to...</th>\n<th>Scenario</th>\n<th>Attribute</th>\n<th>If you do nothing</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Use <code>SameSite=Strict</code></strong></td>\n<td>Your website offers banking services or your website needs a very secure environment</td>\n<td>Update your <code>SameSite</code> attribute to <code>SameSite=Strict</code>  to add a layer of protection from web threats.</td>\n<td>Your site may be susceptible to potential web vulnerabilities and data leaks.</td>\n</tr>\n<tr>\n<td><strong>Use <code>SameSite=Lax</code></strong></td>\n<td>You have a social community website and you offer embedded chat widgets</td>\n<td>Update your <code>SameSite</code> attribute to <code>SameSite=Lax</code>\n</td>\n<td>You'll be good to go. Chrome's default behavior will be <code>SameSite=Lax</code>. Even if <code>SameSite</code> is not set, the default is still <code>SameSite=Lax</code>\n</td>\n</tr>\n<tr>\n<td><strong>Use <code>SameSite=None</code></strong></td>\n<td>Your website offers data analytics services <strong>OR</strong> your website offers retargeting, advertising and conversion tracking.</td>\n<td>Update your <code>SameSite</code> attribute to <code>SameSite=None; Secure</code> to ensure Chrome doesn't reject your third-party cookies.</td>\n<td>Your cookies will no longer work on Feb 4, 2020.</td>\n</tr>\n<tr>\n<td><strong>\"Speak to a representative\"</strong></td>\n<td>You've monetized your website with third-party ad programs <strong>OR</strong> you're utilizing third-party services like Google Calendar, Cloudflare, Facebook, Twitter, Instagram, LinkedIn, Gravatar, User Tracking services, CRM, reservations plugin, anti-fraud, third-party fonts, image/video hosting and/or payments services.</td>\n<td>Speak with the ad program company to ensure they have a plan to update their cookies. You can't update cookies on a domain you don't control.</td>\n<td>You may see a decline in the ad revenue you receive and or business engagement.</td>\n</tr>\n</tbody>\n</table>\n<h4 class=\"anchored\">\n  <a name=\"now-set-your-cookies\" href=\"#now-set-your-cookies\">Now, set your cookies</a>\n</h4>\n\n<p>Most server-side applications support <code>SameSite</code> attributes; however, there are a few clients who don't support it (see <a href=\"https://www.chromium.org/updates/same-site/incompatible-clients\"><strong>Known Incompatible Clients</strong></a>).</p>\n\n<ul>\n<li>\n<strong>For Server-Side Applications</strong>: Support for <code>SameSite=None</code> in languages, libraries, and frameworks\n\n<ul>\n<li><a href=\"https://github.com/GoogleChromeLabs/samesite-examples/blob/master/php.md\">PHP</a></li>\n<li><a href=\"https://github.com/GoogleChromeLabs/samesite-examples/blob/master/javascript-nodejs.md\">NodeJS</a></li>\n<li><a href=\"https://github.com/GoogleChromeLabs/samesite-examples/blob/master/python.md\">Python</a></li>\n<li><a href=\"https://github.com/GoogleChromeLabs/samesite-examples/blob/master/python-flask.md\">Python Flask</a></li>\n<li>\n<a href=\"https://api.rubyonrails.org/classes/ActionDispatch/Cookies.html\">Ruby on Rails</a>\n\n<ul>\n<li>Recent pull request: <a href=\"https://github.com/rails/rails/pull/28297\">https://github.com/rails/rails/pull/28297</a>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<strong>Directly</strong>: <code>document.cookie</code>\n\n<ul>\n<li><a href=\"https://github.com/GoogleChromeLabs/samesite-examples/blob/master/javascript.md\">JavaScript</a></li>\n</ul>\n</li>\n</ul>\n<h2 class=\"anchored\">\n  <a name=\"need-more-help\" href=\"#need-more-help\">Need more help?</a>\n</h2>\n\n<p>Rowan Merewood, Developer Advocate for Chrome, listed a few helpful ways to get help with setting cookies. Keep in mind, this will be a new update so if you run into an issue, it may be the first time anyone has encountered the issue. It is best to just raise the issue, be vocal, and publicly address your concerns because someone else is very likely to encounter the same issue!</p>\n\n<ul>\n<li>Raise an issue on the <a href=\"https://github.com/GoogleChromeLabs/samesite-examples\"><code>SameSite</code> examples repo on GitHub</a>.</li>\n<li>Post a question on the <a href=\"https://stackoverflow.com/questions/tagged/samesite\">\"samesite\" tag on StackOverflow</a>.</li>\n<li>For issues with Chromium's behavior, raise a bug via the <a href=\"https://bit.ly/2lJMd5c\">[<code>SameSite</code> cookies] issue template</a>.</li>\n<li>Follow Chrome's progress on the <a href=\"https://www.chromium.org/updates/same-site\"><code>SameSite</code> updates page</a>.</li>\n</ul>","PublishedAt":"2020-02-03 21:30:00+00:00","OriginURL":"https://blog.heroku.com/chrome-changes-samesite-cookie","SourceName":"Heroku"}},{"node":{"ID":366,"Title":"Terrier: An Open-Source Tool for Identifying and Analyzing Container and Image Components","Description":"<style scoped>\n@media only screen and\n  (min-width: 415px) {\n    #cover-image { width: 70%; }\n}\n\n@media only screen and\n  (max-width: 414px) \n  and (orientation: portrait) { \n    #cover-image { width: 100%; }\n}\n</style>\n\n<p>As part of our Blackhat Europe talk <a href=\"https://www.blackhat.com/eu-19/briefings/schedule/#reverse-engineering-and-exploiting-builds-in-the-cloud-17287\">“Reverse Engineering and Exploiting Builds in the Cloud”</a> we publicly released a new tool called Terrier. </p>\n\n<figure style=\"text-align: center;\">\n<img id=\"cover-image\" src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1578942799-terrier-twitter.png\" alt=\"Announcing Terrier: An open-source tool for identifying and analysing container and image components\">\n<figcaption>Announcing Terrier: An open-source tool for identifying and analysing container and image components.</figcaption>\n</figure>\n\n<p>In this blog post, I am going to show you how Terrier can help you identify and verify container and image components for a wide variety of use-cases, be it from a supply-chain perspective or forensics perspective. Terrier can be found on Github <a href=\"https://github.com/heroku/terrier\">https://github.com/heroku/terrier</a>.</p>\n<h2 class=\"anchored\">\n  <a name=\"containers-and-images\" href=\"#containers-and-images\">Containers and images</a>\n</h2>\n\n<p>In this blog post, I am not going to go into too much detail about containers and images (you can learn more <a href=\"https://www.opencontainers.org/about\">here</a>) however it is important to highlight a few characteristics of containers and images that make them interesting in terms of Terrier. Containers are run from images and currently the Open Containers Initiative (OCI) is the most popular format for images. The remainder of this blog post refers to OCI images as images.</p>\n\n<p>Essentially images are tar archives that container multiple tar archives and meta-information that represent the “layers” of an image. The OCI format of images makes images relatively simple to work with which makes analysis relatively simple. If you only had access to a terminal and the tar command, you could pretty much get what you need from the image’s tar archive.</p>\n\n<p>When images are utilised at runtime for a container, their contents become the contents of the running container and the layers are essentially extracted to a location on the container’s runtime host. The container runtime host is the host that is running and maintaining the containers. This location is typically <code>/var/lib/docker/overlay2/&lt;containerID&gt;/</code>. This location contains a few folders of interest, particularly the \"merged\" folder. The \"merged\" folder contains the contents of the image and any changes that have occurred in the container since its creation. For example, if the image contained a location such as <code>/usr/chris/stuff</code> and after creating a container from this image I created a file called <code>helloworld.txt</code> at the location <code>/usr/chris/stuff</code>. This would result in the following valid path on the container runtime host <code>/var/lib/docker/overlay2/&lt;containerID&gt;/merged/usr/chris/stuff/helloworld.txt</code>.</p>\n<h2 class=\"anchored\">\n  <a name=\"what-does-terrier-do\" href=\"#what-does-terrier-do\">What does Terrier do?</a>\n</h2>\n\n<p>Now that we have a brief understanding of images and containers, we can look at what Terrier does. Often it is the case that you would like to determine if an image or container contains a specific file. This requirement may be due to a forensic analysis need or to identify and prevent a certain supply-chain attack vector. Regardless of the requirement, having the ability to determine the presence of a specific file in an image or container is useful.</p>\n<h3 class=\"anchored\">\n  <a name=\"identifying-files-in-oci-images\" href=\"#identifying-files-in-oci-images\">Identifying files in OCI images</a>\n</h3>\n\n<p>Terrier can be used to determine if a specific image contains a specific file. In order to do this, you need the following:</p>\n\n<ol>\n<li>An OCI Image i.e TAR archive</li>\n<li>A SHA256 hash of a specific file/s</li>\n</ol>\n\n<p>The first point can be easily achieved with Docker by using the following command:</p>\n\n<pre><code class=\"language-term\">$ docker save imageid -o myImage.tar\n</code></pre>\n\n<p>The command above uses a Docker image ID which can be obtained using the following command:</p>\n\n<pre><code class=\"language-term\">$ docker images\n</code></pre>\n\n<p>Once you have your image exported as a tar archive, you will then need to establish the SHA256 hash of the particular file you would like to identify in the image. There are multiple ways to achieve this but in this example, we are going to use the hash of the Golang binary <em>go1.13.4 linux/amd64</em> which can be achieved with following command on a Linux host:</p>\n\n<pre><code class=\"language-term\">$ cat /usr/local/go/bin/go | sha256sum\n</code></pre>\n\n<p>The command above should result in the following SHA256 hash: <code>82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd</code></p>\n\n<p>Now that we have a hash, we can use this hash to determine if the Golang binary is in the image <code>myImage.tar</code>. To achieve this, we need to populate a configuration file for Terrier. Terrier makes use of YAML configuration files and below is our config file that we save as <code>cfg.yml</code>:</p>\n\n<pre><code class=\"language-yaml\">mode: image\nimage: myImage.tar\n\nhashes:\n    - hash: '82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd'\n</code></pre>\n\n<p>The config file above has multiple entries which allow us to specify the <code>mode</code> that Terrier will operate in and in this case, we are working with an image file (tar archive) so the mode is <code>image</code>. The image file we are working with is <code>myImage.tar</code> and the hash we are looking to identify is in the <code>hashes</code> list.</p>\n\n<p>We are now ready to run Terrier and this can be done with the following command:</p>\n\n<pre><code class=\"language-term\">$ ./terrier\n</code></pre>\n\n<p>The command above should result in output similar to the following:</p>\n\n<pre><code class=\"language-term\">$ ./terrier \n[+] Loading config:  cfg.yml\n[+] Analysing Image\n[+] Docker Image Source:  myImage.tar\n[*] Inspecting Layer:  34a9e0f17132202a82565578a3c2dae1486bb198cde76928c8c2c5c461e11ccf\n[*] Inspecting Layer:  6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560\n[*] Inspecting Layer:  6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759\n[!] Found file '6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759/usr/local/go/bin/go' with hash: 82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd\n[*] Inspecting Layer:  a6e646c34d2d2c2f4ab7db95e4c9f128721f63c905f107887839d3256f1288e1\n[*] Inspecting Layer:  aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545\n[*] Inspecting Layer:  d4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c\n[*] Inspecting Layer:  dbf9da5e4e5e1ecf9c71452f6b67b2b0225cec310a20891cc5dedbfd4ead667c\n</code></pre>\n\n<p>We have identified a file <code>/usr/local/go/bin/go</code> located at layer <code>6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759</code> that has the same SHA256 hash as the one we provided. We now have verification that the image “myImage.tar” contains a file with the SHA256 hash we provided.</p>\n\n<p>This example can be extended upon and you can instruct Terrier to search for multiple hashes. In this case, we are going to search for a malicious file. Recently a malicious Python library was identified in the wild and went by the name “Jeilyfish”. Terrier could be used to check if a Docker image of yours contained this malicious package. To do this, we can determine the SHA256 of one of the malicious Python files that contains the backdoor:</p>\n\n<pre><code class=\"language-term\">$ cat jeIlyfish-0.7.1/jeIlyfish/_jellyfish.py | sha256sum\ncf734865dd344cd9b0b349cdcecd83f79a751150b5fd4926f976adddb93d902c\n</code></pre>\n\n<p>We then update our Terrier config to include the hash calculated above.</p>\n\n<pre><code class=\"language-yaml\">mode: image\nimage: myImage.tar\n\nhashes:\n    - hash: '82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd'\n    - hash: 'cf734865dd344cd9b0b349cdcecd83f79a751150b5fd4926f976adddb93d902c'\n</code></pre>\n\n<p>We then run Terrier against and analyse the results:</p>\n\n<pre><code class=\"language-term\">$ ./terrier \n[+] Loading config:  cfg.yml\n[+] Analysing Image\n[+] Docker Image Source:  myImage.tar\n[*] Inspecting Layer:  34a9e0f17132202a82565578a3c2dae1486bb198cde76928c8c2c5c461e11ccf\n[*] Inspecting Layer:  6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560\n[*] Inspecting Layer:  6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759\n[!] Found file '6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759/usr/local/go/bin/go' with hash: 82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd\n[*] Inspecting Layer:  a6e646c34d2d2c2f4ab7db95e4c9f128721f63c905f107887839d3256f1288e1\n[*] Inspecting Layer:  aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545\n[*] Inspecting Layer:  d4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c\n[*] Inspecting Layer:  dbf9da5e4e5e1ecf9c71452f6b67b2b0225cec310a20891cc5dedbfd4ead667c\n</code></pre>\n\n<p>The results above indicate that our image did not contain the malicious Python package.</p>\n\n<p>There is no limit as to how many hashes you can search for however it should be noted that Terrier performs all its actions in-memory for performance reasons so you might hit certain limits if you do not have enough accessible memory.</p>\n<h3 class=\"anchored\">\n  <a name=\"identifying-and-verifying-specific-files-in-oci-images\" href=\"#identifying-and-verifying-specific-files-in-oci-images\">Identifying and verifying specific files in OCI images</a>\n</h3>\n\n<p>Terrier can also be used to determine if a specific image contains a specific file <em>at a specific location</em>. This can be useful to ensure that an image is using a specific component i.e binary, shared object or dependency.  This can also be seen as “pinning” components by ensuring that you are images are using specific components i.e a specific version of cURL.</p>\n\n<p>In order to do this, you need the following:</p>\n\n<ol>\n<li>An OCI Image i.e TAR archive</li>\n<li>A SHA256 hash of a specific file/s</li>\n<li>The path and name of the specific file/s</li>\n</ol>\n\n<p>The first point can be easily achieved with Docker by using the following command:</p>\n\n<pre><code class=\"language-term\">$ docker save imageid -o myImage.tar\n</code></pre>\n\n<p>The command above utilises a Docker image id which can be obtained using the following command:</p>\n\n<pre><code class=\"language-term\">$ docker images\n</code></pre>\n\n<p>Once you have your image exported as a tar archive, you will need to determine the path of the file you would like to identify and verify in the image. For example, if we would like to ensure that our images are making use of a specific version of cURL, we can run the following commands in a container or some other environment that resembles the image.</p>\n\n<pre><code class=\"language-term\">$ which curl\n/usr/bin/curl\n</code></pre>\n\n<p>We now have the path to cURL and can now generate the SHA256 of this instance of cURL because in this case, we trust this instance of cURL. We could determine the hash by other means for example many binaries are released with a corresponding hash from the developer which can be acquired from the developer’s website.</p>\n\n<pre><code class=\"language-term\">$ cat /usr/bin/curl | sha256sum \n9a43cb726fef31f272333b236ff1fde4beab363af54d0bc99c304450065d9c96\n</code></pre>\n\n<p>With this information, we can now populate our config file for Terrier:</p>\n\n<pre><code class=\"language-yaml\">mode: image\nimage: myImage.tar\nfiles:\n  - name: '/usr/bin/curl'\n    hashes:\n      - hash: '9a43cb726fef31f272333b236ff1fde4beab363af54d0bc99c304450065d9c96'\n</code></pre>\n\n<p>We’ve saved the above config as <code>cfg.yml</code> and when we run Terrier with this config, we get the following output:</p>\n\n<pre><code class=\"language-term\">$ ./terrier\n[+] Loading config:  cfg.yml\n[+] Analysing Image\n[+] Docker Image Source:  myImage.tar\n[*] Inspecting Layer:  34a9e0f17132202a82565578a3c2dae1486bb198cde76928c8c2c5c461e11ccf\n[*] Inspecting Layer:  34a9e0f17132202a82565578a3c2dae1486bb198cde76928c8c2c5c461e11ccf\n[*] Inspecting Layer:  6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560\n[*] Inspecting Layer:  6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560\n[*] Inspecting Layer:  6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759\n[*] Inspecting Layer:  6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759\n[*] Inspecting Layer:  a6e646c34d2d2c2f4ab7db95e4c9f128721f63c905f107887839d3256f1288e1\n[*] Inspecting Layer:  a6e646c34d2d2c2f4ab7db95e4c9f128721f63c905f107887839d3256f1288e1\n[*] Inspecting Layer:  aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545\n[*] Inspecting Layer:  aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545\n[*] Inspecting Layer:  d4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c\n[*] Inspecting Layer:  d4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c\n[*] Inspecting Layer:  dbf9da5e4e5e1ecf9c71452f6b67b2b0225cec310a20891cc5dedbfd4ead667c\n[*] Inspecting Layer:  dbf9da5e4e5e1ecf9c71452f6b67b2b0225cec310a20891cc5dedbfd4ead667c\n[!] All components were identified: (1/1)\n[!] All components were identified and verified: (1/1)\n$ echo $?\n0\n</code></pre>\n\n<p>The output above indicates that the file <code>/usr/bin/curl</code> was successfully identified and verified, meaning that the image contained a file at the location <code>/usr/bin/curl</code> and that the SHA256 of that file matched the hash we provided in the config. Terrier also makes use of return codes and if we analyse the return code from the output above, we can see that the value is <code>0</code> which indicates a success. If Terrier cannot identify or verify all the provided files, a return code of <code>1</code> is returned which indicates a failure. The setting of return codes is particularly useful in testing environments or CI/CD environments.</p>\n\n<p>We can also run Terrier with verbose mode enable to get more information:</p>\n\n<pre><code class=\"language-term\">$ ./terrier \n[+] Loading config:  cfg.yml\n[+] Analysing Image\n[+] Docker Image Source:  myImage.tar\n[*] Inspecting Layer:  34a9e0f17132202a82565578a3c2dae1486bb198cde76928c8c2c5c461e11ccf\n[*] Inspecting Layer:  6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560\n        [!] Identified  instance of '/usr/bin/curl' at: 6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560/usr/bin/curl \n        [!] Verified matching instance of '/usr/bin/curl' at: 6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560/usr/bin/curl with hash: 9a43cb726fef31f272333b236ff1fde4beab363af54d0bc99c304450065d9c96\n[*] Inspecting Layer:  6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759\n[*] Inspecting Layer:  a6e646c34d2d2c2f4ab7db95e4c9f128721f63c905f107887839d3256f1288e1\n[*] Inspecting Layer:  aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545\n[*] Inspecting Layer:  d4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c\n[*] Inspecting Layer:  dbf9da5e4e5e1ecf9c71452f6b67b2b0225cec310a20891cc5dedbfd4ead667c\n[!] All components were identified: (1/1)\n[!] All components were identified and verified: (1/1)\n</code></pre>\n\n<p>The output above provides some more detailed information such as which layer the cURL files was located at. If you wanted more information, you could enable the <strong>veryveryverbose</strong> option in the config file but beware, this is a lot of output and grep will be your friend.</p>\n\n<p>There is no limit for how many hashes you can specify for a file. This can be useful for when you want to allow more than one version of a specific file i.e multiple versions of cURL. An example config of multiple hashes for a file might look like:</p>\n\n<pre><code class=\"language-yaml\">mode: image\nimage: myImage.tar\nfiles:\n  - name: '/usr/bin/curl'\n    hashes:\n      - hash: '9a43cb726fef31f272333b236ff1fde4beab363af54d0bc99c304450065d9c96'\n      - hash: 'aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545'\n      - hash: '6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759'\n      - hash: 'd4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c'\n</code></pre>\n\n<p>The config above allows Terrier to verify if the identified cURL instance is one of the provided hashes. There is also no limit for the amount of files Terrier can attempt to identify and verify.</p>\n\n<p>Terrier’s Github repo also contains a useful script called <code>convertSHA.sh</code> which can be used to convert a list of SHA256 hashes and filenames into a Terrier config file. This is useful when converting the output from other tools into a Terrier friendly format. For example, we could have the following contents of a file:</p>\n\n<pre><code>8946690bfe12308e253054ea658b1552c02b67445763439d1165c512c4bc240d ./bin/uname\n6de8254cfd49543097ae946c303602ffd5899b2c88ec27cfcd86d786f95a1e92 ./bin/gzexe\n74ff9700d623415bc866c013a1d8e898c2096ec4750adcb7cd0c853b4ce11c04 ./bin/wdctl\n61c779de6f1b9220cdedd7dfee1fa4fb44a4777fff7bd48d12c21efb87009877 ./bin/dmesg\n7bdde142dc5cb004ab82f55adba0c56fc78430a6f6b23afd33be491d4c7c238b ./bin/which\n3ed46bd8b4d137cad2830974a78df8d6b1d28de491d7a23d305ad58742a07120 ./bin/mknod\ne8ca998df296413624b2bcf92a31ee3b9852f7590f759cc4a8814d3e9046f1eb ./bin/mv\na91d40b349e2bccd3c5fe79664e70649ef0354b9f8bd4658f8c164f194b53d0f ./bin/chown\n091abe52520c96a75cf7d4ff38796fc878cd62c3a75a3fd8161aa3df1e26bebd ./bin/uncompress\nc5ebd611260a9057144fd1d7de48dbefc14e16240895cb896034ae05a94b5750 ./bin/echo\nd4ba9ffb5f396a2584fec1ca878930b677196be21aee16ee6093eb9f0a93bf8f ./bin/df\n5fb515ff832650b2a25aeb9c21f881ca2fa486900e736dfa727a5442a6de83e5 ./bin/tar\n6936c9aa8e17781410f286bb1cbc35b5548ea4e7604c1379dc8e159d91a0193d ./bin/zforce\n8d641329ea7f93b1caf031b70e2a0a3288c49a55c18d8ba86cc534eaa166ec2e ./bin/gzip\n0c1a1f53763ab668fb085327cdd298b4a0c1bf2f0b51b912aa7bc15392cd09e7 ./bin/su\n20c358f7ee877a3fd2138ecce98fada08354810b3e9a0e849631851f92d09cc4 ./bin/bzexe\n01764d96697b060b2a449769073b7cf2df61b5cb604937e39dd7a47017e92ee0 ./bin/znew\n0d1a106dc28c3c41b181d3ba2fc52086ede4e706153e22879e60e7663d2f6aad ./bin/login\nfb130bda68f6a56e2c2edc3f7d5b805fd9dcfbcc26fb123a693b516a83cfb141 ./bin/dir\n0e7ca63849eebc9ea476ea1fefab05e60b0ac8066f73c7d58e8ff607c941f212 ./bin/bzmore\n14dc8106ec64c9e2a7c9430e1d0bef170aaad0f5f7f683c1c1810b466cdf5079 ./bin/zless\n9cf4cda0f73875032436f7d5c457271f235e59c968c1c101d19fc7bf137e6e37 ./bin/chmod\nc5f12f157b605b1141e6f97796732247a26150a0a019328d69095e9760b42e38 ./bin/sleep\nb9711301d3ab42575597d8a1c015f49fddba9a7ea9934e11d38b9ff5248503a8 ./bin/zfgrep\n0b2840eaf05bb6802400cc5fa793e8c7e58d6198334171c694a67417c687ffc7 ./bin/stty\nd9393d0eca1de788628ad0961b74ec7a648709b24423371b208ae525f60bbdad ./bin/bunzip2\nd2a56d64199e674454d2132679c0883779d43568cd4c04c14d0ea0e1307334cf ./bin/mkdir\n1c48ade64b96409e6773d2c5c771f3b3c5acec65a15980d8dca6b1efd3f95969 ./bin/cat\n09198e56abd1037352418279eb51898ab71cc733642b50bcf69d8a723602841e ./bin/true\n97f3993ead63a1ce0f6a48cda92d6655ffe210242fe057b8803506b57c99b7bc ./bin/zdiff\n0d06f9724af41b13cdacea133530b9129a48450230feef9632d53d5bbb837c8c ./bin/ls\nda2da96324108bbe297a75e8ebfcb2400959bffcdaa4c88b797c4d0ce0c94c50 ./bin/zegrep\n</code></pre>\n\n<p>The file contents above are trusted SHA256 hashes for specific files. If we would like to use this list for ensuring that a particular image is making use of the files listed above, we can do the following:</p>\n\n<pre><code class=\"language-term\">$ ./convertSHA.sh trustedhashes.txt terrier.yml\n</code></pre>\n\n<p>The script above takes the input file <code>trustedhashes.txt</code> which contains our trusted hashes listed above and converts them into a Terrier friendly config file called <code>terrier.yml</code> which looks like the following:</p>\n\n<pre><code class=\"language-yaml\">mode: image\nimage: myImage.tar\nfiles:\n  - name: '/bin/uname'\n    hashes:\n       - hash: '8946690bfe12308e253054ea658b1552c02b67445763439d1165c512c4bc240d'\n  - name: '/bin/gzexe'\n    hashes:\n       - hash: '6de8254cfd49543097ae946c303602ffd5899b2c88ec27cfcd86d786f95a1e92'\n  - name: '/bin/wdctl'\n    hashes:\n       - hash: '74ff9700d623415bc866c013a1d8e898c2096ec4750adcb7cd0c853b4ce11c04'\n  - name: '/bin/dmesg'\n    hashes:\n       - hash: '61c779de6f1b9220cdedd7dfee1fa4fb44a4777fff7bd48d12c21efb87009877'\n  - name: '/bin/which'\n    hashes:\n       - hash: '7bdde142dc5cb004ab82f55adba0c56fc78430a6f6b23afd33be491d4c7c238b'\n  - name: '/bin/mknod'\n</code></pre>\n\n<p>The config file <code>terrier.yml</code> is ready to be used:</p>\n\n<pre><code class=\"language-term\">$ ./terrier -cfg=terrier.yml\n[+] Loading config:  terrier.yml\n[+] Analysing Image\n[+] Docker Image Source:  myImage.tar\n[*] Inspecting Layer:  34a9e0f17132202a82565578a3c2dae1486bb198cde76928c8c2c5c461e11ccf\n[*] Inspecting Layer:  6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560\n[*] Inspecting Layer:  6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759\n[*] Inspecting Layer:  a6e646c34d2d2c2f4ab7db95e4c9f128721f63c905f107887839d3256f1288e1\n[*] Inspecting Layer:  aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545\n[*] Inspecting Layer:  d4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c\n[*] Inspecting Layer:  dbf9da5e4e5e1ecf9c71452f6b67b2b0225cec310a20891cc5dedbfd4ead667c\n[!] Not all components were identifed: (4/31)\n[!] Component not identified:  /bin/uncompress\n[!] Component not identified:  /bin/bzexe\n[!] Component not identified:  /bin/bzmore\n[!] Component not identified:  /bin/bunzip2\n$ echo $?\n1\n</code></pre>\n\n<p>As we can see from the output above, Terrier was unable to identify 4/31 of the components provided in the config. The return code is also 1 which indicates a failure. If we were to remove the components that are not in the provided image, the output from the previous command would look like the following:</p>\n\n<pre><code class=\"language-term\">$ ./terrier -cfg=terrier.yml\n[+] Loading config: terrier.yml\n[+] Analysing Image\n[+] Docker Image Source: myImage.tar\n[*] Inspecting Layer: 34a9e0f17132202a82565578a3c2dae1486bb198cde76928c8c2c5c461e11ccf\n[*] Inspecting Layer: 6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560\n[*] Inspecting Layer: 6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759\n[*] Inspecting Layer: a6e646c34d2d2c2f4ab7db95e4c9f128721f63c905f107887839d3256f1288e1\n[*] Inspecting Layer: aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545\n[*] Inspecting Layer: d4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c\n[*] Inspecting Layer: dbf9da5e4e5e1ecf9c71452f6b67b2b0225cec310a20891cc5dedbfd4ead667c\n[!] All components were identified: (27/27)\n[!] Not all components were verified: (26/27)\n[!] Component not verified: /bin/cat\n[!] Component not verified: /bin/chmod\n[!] Component not verified: /bin/chown\n[!] Component not verified: /bin/df\n[!] Component not verified: /bin/dir\n[!] Component not verified: /bin/dmesg\n[!] Component not verified: /bin/echo\n[!] Component not verified: /bin/gzexe\n[!] Component not verified: /bin/gzip\n[!] Component not verified: /bin/login\n[!] Component not verified: /bin/ls\n[!] Component not verified: /bin/mkdir\n[!] Component not verified: /bin/mknod\n[!] Component not verified: /bin/mv\n[!] Component not verified: /bin/sleep\n[!] Component not verified: /bin/stty\n[!] Component not verified: /bin/su\n[!] Component not verified: /bin/tar\n[!] Component not verified: /bin/true\n[!] Component not verified: /bin/uname\n[!] Component not verified: /bin/wdctl\n[!] Component not verified: /bin/zdiff\n[!] Component not verified: /bin/zfgrep\n[!] Component not verified: /bin/zforce\n[!] Component not verified: /bin/zless\n[!] Component not verified: /bin/znew\n$ echo $?\n1\n</code></pre>\n\n<p>The output above indicates that Terrier was able to identify all the components provided but many were not verifiable, the hashes did not match and once again, the return code is <code>1</code> to indicate this failure.</p>\n<h3 class=\"anchored\">\n  <a name=\"identifying-files-in-containers\" href=\"#identifying-files-in-containers\">Identifying files in containers</a>\n</h3>\n\n<p>The previous sections focused on identifying files in images, which can be referred to as a form of “static analysis,” however it is also possible to perform this analysis to running containers. In order to do this, you need the following:</p>\n\n<ol>\n<li>Location of the container’s <code>merged</code> folder </li>\n<li>A SHA256 hash of a specific file/s</li>\n</ol>\n\n<p>The <code>merged</code> folder is Docker specific, in this case, we are using it because this is where the contents of the Docker container reside, this might be another location if it were LXC.</p>\n\n<p>The location of the container’s <code>merged</code> folder can be determined by running the following commands. First obtain the container’s ID:</p>\n\n<pre><code class=\"language-term\">$ docker ps\nCONTAINER ID        IMAGE                    COMMAND               CREATED             STATUS              PORTS               NAMES\nb9e676fd7b09        golang                   \"bash\"                20 hours ago        Up 20 hours                             cocky_robinson\n</code></pre>\n\n<p>Once you have the container’s ID, you can run the following command which will help you identify the location of the container’s <code>merged</code> folder on the underlying host.</p>\n\n<pre><code class=\"language-term\">$ docker exec b9e676fd7b09 mount | grep diff\noverlay on / type overlay (rw,relatime,lowerdir=/var/lib/docker/overlay2/l/7ZDEFE6PX4C3I3LGIGGI5MWQD4:\n/var/lib/docker/overlay2/l/EZNIFFIXOVO2GIT5PTBI754HC4:/var/lib/docker/overlay2/l/UWKXP76FVZULHGRKZMVYJHY5IK:\n/var/lib/docker/overlay2/l/DTQQUTRXU4ZLLQTMACWMJYNRTH:/var/lib/docker/overlay2/l/R6DE2RY63EJABTON6HVSFRFICC:\n/var/lib/docker/overlay2/l/U4JNTFLQEKMFHVEQJ5BQDLL7NO:/var/lib/docker/overlay2/l/FEBURQY25XGHJNPSFY5EEPCFKA:\n/var/lib/docker/overlay2/l/ICNMAZ44JY5WZQTFMYY4VV6OOZ,\nupperdir=/var/lib/docker/overlay2/04f84ddd30a7df7cd3f8b1edeb4fb89d476ed84cf3f76d367e4ebf22cd1978a4/diff,\nworkdir=/var/lib/docker/overlay2/04f84ddd30a7df7cd3f8b1edeb4fb89d476ed84cf3f76d367e4ebf22cd1978a4/work)          \n</code></pre>\n\n<p>From the results above, we are interested in two entries, <code>upperdir</code> and <code>workdir</code> because these two entries will provide us with the path to the container’s <code>merged</code> folder. From the results above, we can determine that the container’s <code>merged</code> directory is located at <code>/var/lib/docker/overlay2/04f84ddd30a7df7cd3f8b1edeb4fb89d476ed84cf3f76d367e4ebf22cd1978a4/</code> on the underlying host.</p>\n\n<p>Now that we have the location, we need some files to identify and in this case, we are going to reuse the SHA256 hashes from the previous section. Let’s now go ahead and populate our Terrier configuration with this new information.</p>\n\n<pre><code class=\"language-yaml\">mode: container\npath: merged\n#image: myImage.tar\n\nhashes:\n    - hash: '82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd'\n    - hash: 'cf734865dd344cd9b0b349cdcecd83f79a751150b5fd4926f976adddb93d902c'\n</code></pre>\n\n<p>The configuration above shows that we have changed the <code>mode</code> from <code>image</code> to <code>container</code> and we have added the <code>path</code> to our <code>merged</code> folder. We have kept the two hashes from the previous section. </p>\n\n<p>If we run Terrier with this configuration from the location <code>/var/lib/docker/overlay2/04f84ddd30a7df7cd3f8b1edeb4fb89d476ed84cf3f76d367e4ebf22cd1978a4/</code>, we get the following output:</p>\n\n<pre><code class=\"language-term\">$ ./terrier\n[+] Loading config: cfg.yml\n[+] Analysing Container\n[!] Found matching instance of '82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd' at: merged/usr/local/go/bin/go with hash:82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd\n</code></pre>\n\n<p>From the output above, we know that the container (<code>b9e676fd7b09</code>) does not contain the malicious Python package but it does contain an instance of the Golang binary which is located at <code>merged/usr/local/go/bin/go</code>.</p>\n<h3 class=\"anchored\">\n  <a name=\"identifying-and-verifying-specific-files-in-containers\" href=\"#identifying-and-verifying-specific-files-in-containers\">Identifying and verifying specific files in containers</a>\n</h3>\n\n<p>And as you might have guessed, Terrier can also be used to verify and identify files at specific paths in containers. To do this, we need the following:</p>\n\n<ol>\n<li>Location of the container’s <code>merged</code> folder </li>\n<li>A SHA256 hash of a specific file/s</li>\n<li>The path and name of the specific file/s</li>\n</ol>\n\n<p>The points above can be determined using the same procedures described in the previous sections. Below is an example Terrier config file that we could use to identify and verify components in a running container:</p>\n\n<pre><code class=\"language-yaml\">mode: container\npath: merged\nverbose: true\nfiles:\n  - name: '/usr/bin/curl'\n    hashes:\n      - hash: '9a43cb726fef31f272333b236ff1fde4beab363af54d0bc99c304450065d9c96'\n  - name: '/usr/local/go/bin/go'\n    hashes:\n      - hash: '82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd'\n</code></pre>\n\n<p>If we run Terrier with the above config, we get the following output:</p>\n\n<pre><code class=\"language-term\">$ ./terrier\n[+] Loading config: cfg.yml\n[+] Analysing Container\n[!] Found matching instance of '/usr/bin/curl' at: merged/usr/bin/curl with hash:9a43cb726fef31f272333b236ff1fde4beab363af54d0bc99c304450065d9c96\n[!] Found matching instance of '/usr/local/go/bin/go' at: merged/usr/local/go/bin/go with hash:82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91\ndd3ff92dd\n[!] All components were identified: (2/2)\n[!] All components were identified and verified: (2/2)\n$ echo $?\n0\n</code></pre>\n\n<p>From the output above, we can see that Terrier was able to successfully identify and verify all the files in the running container. The return code is also <code>0</code> which indicates a successful execution of Terrier.</p>\n<h3 class=\"anchored\">\n  <a name=\"using-terrier-with-ci-cd\" href=\"#using-terrier-with-ci-cd\">Using Terrier with CI/CD</a>\n</h3>\n\n<p>In addition to Terrier being used as a standalone CLI tool, Terrier can also be integrated easily with existing CI/CD technologies such as GitHub Actions and CircleCI. Below are two example configurations that show how Terrier can be used to identify and verify certain components of Docker files in a pipeline and prevent the pipeline from continuing if all verifications do not pass. This can be seen as an extra mitigation for supply-chain attacks.</p>\n\n<p>Below is a CircleCI example configuration using Terrier to verify the contents of an image.</p>\n\n<pre><code class=\"language-yaml\">version: 2\njobs:\nbuild:\n  machine: true\n  steps:\n    - checkout\n    - run:\n       name: Build Docker Image\n       command: |\n             docker build -t builditall .\n    - run:\n       name: Save Docker Image Locally\n       command: |\n             docker save builditall -o builditall.tar\n    - run:\n       name: Verify Docker Image Binaries\n       command: |\n             ./terrier\n</code></pre>\n\n<p>Below is a Github Actions example configuration using Terrier to verify the contents of an image.</p>\n\n<pre><code class=\"language-yaml\">name: Go\non: [push]\njobs:\nbuild:\n  name: Build\n  runs-on: ubuntu-latest\n  steps:\n\n  - name: Get Code\n    uses: actions/checkout@master\n  - name: Build Docker Image\n    run: |\n      docker build -t builditall .\n  - name: Save Docker Image Locally\n    run: |\n      docker save builditall -o builditall.tar\n  - name: Verify Docker Image Binaries\n    run: |\n      ./terrier\n</code></pre>\n<h2 class=\"anchored\">\n  <a name=\"conclusion\" href=\"#conclusion\">Conclusion</a>\n</h2>\n\n<p>In this blog post, we have looked at how to perform multiple actions on Docker (and OCI) containers and images via Terrier. The actions performed allowed us to identify specific files according to their hashes in images and containers. The actions performed have also allowed us to identify and verify multiple components in images and containers. These actions performed by Terrier are useful when attempting to prevent certain supply-chain attacks.</p>\n\n<p>We have also seen how Terrier can be used in a DevOps pipeline via GitHub Actions and CircleCI.</p>\n\n<p>Learn more about Terrier on GitHub at <a href=\"https://github.com/heroku/terrier\">https://github.com/heroku/terrier</a>.</p>","PublishedAt":"2020-01-14 21:30:00+00:00","OriginURL":"https://blog.heroku.com/terrier-open-source-identifying-analyzing-containers","SourceName":"Heroku"}},{"node":{"ID":367,"Title":"Know Your Database Types","Description":"<p><em>This blog post is adapted from <a href=\"https://www.youtube.com/watch?v=7QgNDhtaQMQ&amp;t=16m07s\">a lightning talk</a> by Ben Fritsch at Ruby on Ice 2019.</em></p>\n\n<p>There can be a number of reasons why your application performs poorly, but perhaps none are as challenging as issues stemming from your database. If your database's response times tend to be high, it can cause a strain on your network and your users’ patience. The usual culprit for a slow database is an inefficient query being executed somewhere in your application logic. Usually, you can implement a fix in a number of common ways, by:</p>\n\n<ul>\n<li>\n<a href=\"https://jaketrent.com/post/find-kill-locks-postgres/\">reducing the amount of open locks</a> (or more detail about database lock debugging in this other blog post by Heroku Engineer Richard Schneeman, <a href=\"https://blog.heroku.com/curious-case-table-locking-update-query\">The Curious Case of the Table-Locking UPDATE Query</a>)</li>\n<li><a href=\"http://www.postgresqltutorial.com/postgresql-indexes/postgresql-create-index/\">defining indexes for faster <code>WHERE</code> lookups</a></li>\n<li><a href=\"https://www.datadoghq.com/blog/100x-faster-postgres-performance-by-changing-1-line/\">rewriting the query to use more efficient statements</a></li>\n</ul>\n\n<p>...But what if your problem isn't resolved by any of these actions?</p>\n\n<p>Let's talk about a problem that can occur from the underlying database schema, and how to solve it.</p>\n\n<div class=\"embedded-video-wrapper\">\n<iframe title=\"Know Your Database Types\" src=\"https://www.youtube-nocookie.com/embed/7QgNDhtaQMQ?start=967\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n<h2 class=\"anchored\">\n  <a name=\"the-problem\" href=\"#the-problem\">The problem</a>\n</h2>\n\n<p>Consider this PostgreSQL database schema:</p>\n\n<pre><code class=\"language-sql\">CREATE TABLE table (\n  app_uuid uuid NOT NULL,\n  json_field json\n)\n</code></pre>\n\n<p>Postgres lets you mark a column's data type as <code>json</code>. This, as opposed to simply unstructured <code>text</code>, allows for more flexible querying and data validation.</p>\n\n<p>As part of our application’s behavior, we receive and store payloads that look like this:</p>\n\n<pre><code class=\"language-json\">{\"data\": \"very large string\", \"url\": \"https://heroku.com\"}\n</code></pre>\n\n<p>If I want to fetch the <code>url</code> values for a specific <code>app_uuid</code>, I would write a query like this:</p>\n\n<pre><code class=\"language-sql\">SELECT (table.large_json_field -&gt;&gt; 'url'::text) AS url,\nFROM table\nWHERE (\"app_uuid\" = $app_uuid)\nORDER BY  \"created_at\" DESC LIMIT 200;\n</code></pre>\n\n<p>The average execution time of this query was 10ms, although there were outliers reaching as high as 1200ms. This was becoming unacceptable, and I dug in to see exactly what was going on.</p>\n<h2 class=\"anchored\">\n  <a name=\"the-investigation\" href=\"#the-investigation\">The investigation</a>\n</h2>\n\n<p>If you need to look into slow queries, the <a href=\"https://www.postgresql.org/docs/12/sql-explain.html\"><code>EXPLAIN ANALYZE</code></a> statement is a good place to start. It will provide you with some internal metrics about how your query is planned and executed. I learned that there is also the option to use <code>EXPLAIN (ANALYZE, BUFFERS)</code>, which looks like this:</p>\n\n<pre><code class=\"language-sql\">EXPLAIN (ANALYZE, BUFFERS)\nSELECT (table.large_json_field -&gt;&gt; 'url'::text) AS url,\nFROM table\nWHERE (\"app_uuid\" = $app_uuid)\nORDER BY  \"created_at\" DESC LIMIT 200;\n</code></pre>\n\n<p><code>BUFFERS</code> provides stats on the I/O subsystem of Postgres, identifying whether information is being loaded from cache (memory) or directly from disk. It is much slower for Postgres to read data outside of the cache.</p>\n\n<p>The result of that informative query was the following information:</p>\n\n<pre><code>-&gt;   Index Cond: (app_uuid = $app_uuid::uuid)\n        Buffers: shared hit=7106\n\nPlanning time: 0.187 ms\nExecution time: 1141.296 ms\n</code></pre>\n\n<p>By default, Postgres has <a href=\"https://www.cybertec-postgresql.com/en/postgresql-block-sizes-getting-started/\">a block size of 8kb</a>. According to the Postgres planner, the query to fetch a <code>url</code> key uses over 7,100 blocks, which means that we've loaded (and discarded) about 60MB of JSON data (7,106 * 8kb), just to fetch the URLs we wanted. Even though the query took less than a millisecond to plan, it takes over a second to execute!</p>\n<h2 class=\"anchored\">\n  <a name=\"the-solution\" href=\"#the-solution\">The solution</a>\n</h2>\n\n<p>The fix for this is simple and direct. Rather than relying on a single <code>json</code> column, I converted it into two separate <code>text</code> fields: one for <code>data</code> and one for <code>url</code>. This brought the query time down from 1200ms to 10ms as we were able to scope our query to the exact piece of information we needed.</p>\n\n<p>We realized that in this use case, storing JSON was no longer a requirement for us. When we started building this feature about three or four years ago, a <code>json</code> data type was the best choice we had, given the information we had. Our system hasn't changed, but our understanding of how the system was being used did. We had been storing the same JSON structure for years, but we were essentially storing unstructured data in our database. As a result of our database design, reading information became expensive for queries which only required a small piece of information.</p>\n\n<p>I encourage you to audit your database schema for columns with data types that are no longer necessary. As our application and knowledge of users' behaviors evolves, so too should our database structure. Of course, don't change tables just for the sake of changing something! We were able to continue operating with this problem for years without any tremendous strain on our systems. If it's not a problem, don't fix it.</p>\n\n<p>Want to learn more about Postgres? Check out another article we wrote on dev.to: <a href=\"https://dev.to/heroku/postgres-is-underrated-it-handles-more-than-you-think-4ff3\">Postgres Is Underrated—It Handles More than You Think</a>.</p>","PublishedAt":"2019-12-18 18:07:49+00:00","OriginURL":"https://blog.heroku.com/know-your-database-types","SourceName":"Heroku"}},{"node":{"ID":368,"Title":"The Curious Case of the Table-Locking UPDATE Query","Description":"<blockquote>\n<p>Update: On closer inspection, the lock type was not on the table, but on a tuple. For more information on this locking mechanism see the <a href=\"https://github.com/postgres/postgres/blob/master/src/backend/access/heap/README.tuplock\">internal Postgresql tuple locking documentation</a>. Postgres does not have lock promotion as suggested in the debugging section of this post.</p>\n</blockquote>\n\n<p>I maintain an internal-facing service at Heroku that does metadata processing. It's not real-time, so there's plenty of slack for when things go wrong. Recently I discovered that the system was getting bogged down to the point where no jobs were being executed at all. After hours of debugging, I found the problem was an <code>UPDATE</code> on a single row on a single table was causing the entire table to lock, which caused a lock queue and ground the whole process to a halt. This post is a story about how the problem was debugged and fixed and why such a seemingly simple query caused so much harm.</p>\n<h2 class=\"anchored\">\n  <a name=\"no-jobs-processing\" href=\"#no-jobs-processing\">No jobs processing</a>\n</h2>\n\n<p>I started debugging when the backlog on our system began to grow, and the number of jobs being processed fell to nearly zero. The system has been running in production for years, and while there have been occasional performance issues, nothing stood out as a huge problem. I checked our datastores, and they were well under their limits, I checked our error tracker and didn't see any smoking guns. My best guess was the database where the results were being stored was having problems.</p>\n\n<p>The first thing I did was run <code>heroku pg:diagnose</code>, which shows \"red\" (critical) and \"yellow\" (important but less critical) issues. It showed that I had queries that had been running for DAYS:</p>\n\n<pre><code>68698   5 days 18:01:26.446979  UPDATE \"table\" SET &lt;values&gt; WHERE (\"uuid\" = '&lt;uuid&gt;')\n</code></pre>\n\n<p>Which seemed odd. The query in question was a simple update, and it's not even on the most massive table in the DB. When I checked <code>heroku pg:outliers</code> from the <a href=\"https://github.com/heroku/heroku-pg-extras\">pg extras CLI plugin</a> I was surprised to see this update taking up 80%+ of the time even though it is smaller than the largest table in the database by a factor of 200. So what gives?</p>\n\n<p>Running the update statement manually didn't reproduce the issue, so I was fresh out of ideas. If it had, then I could have run with <code>EXPLAIN ANALYZE</code> to see why it was so slow. Luckily I work with some pretty fantastic database engineers, and I pinged them for possible ideas. They mentioned that there might be a locking issue with the database. The idea was strange to me since it had been running relatively unchanged for an extremely long time and only now started to see problems, but I decided to look into it.</p>\n\n<pre><code class=\"language-sql\">SELECT\n  S.pid,\n  age(clock_timestamp(), query_start),\n  query,\n  L.mode,\n  L.locktype,\n  L.granted\nFROM pg_stat_activity S\ninner join pg_locks L on S.pid = L.pid\norder by L.granted, L.pid DESC;\n-----------------------------------\npid      | 127624\nage      | 2 days 01:45:00.416267\nquery    | UPDATE \"table\" SET &lt;values&gt; WHERE (\"uuid\" = '&lt;uuid&gt;')\nmode     | AccessExclusiveLock\nlocktype | tuple\ngranted  | f\n</code></pre>\n\n<p>I saw a ton of queries that were hung for quite some time, and most of them pointed to my seemingly teeny <code>UPDATE</code> statement.</p>\n<h2 class=\"anchored\">\n  <a name=\"all-about-locks\" href=\"#all-about-locks\">All about locks</a>\n</h2>\n\n<p>Up until this point, I basically knew nothing about how PostgreSQL uses locking other than in an explicit advisory lock, which can be used via a gem like <a href=\"https://github.com/heroku/pg_lock\">pg_lock</a> (That I maintain). Luckily Postgres has excellent docs around locks, but it's a bit much if you're new to the field: <a href=\"https://www.postgresql.org/docs/11/explicit-locking.html#LOCKING-TABLES\">Postgresql Lock documentation</a></p>\n\n<p>Looking up the name of the lock from before <code>Access Exclusive Lock</code> I saw that it locks the whole table:</p>\n\n<blockquote>\n<p>ACCESS EXCLUSIVE\nConflicts with locks of all modes (ACCESS SHARE, ROW SHARE, ROW EXCLUSIVE, SHARE UPDATE EXCLUSIVE, SHARE, SHARE ROW EXCLUSIVE, EXCLUSIVE, and ACCESS EXCLUSIVE). This mode guarantees that the holder is the only transaction accessing the table in any way.\nAcquired by the DROP TABLE, TRUNCATE, REINDEX, CLUSTER, VACUUM FULL, and REFRESH MATERIALIZED VIEW (without CONCURRENTLY) commands. Many forms of ALTER TABLE also acquire a lock at this level (see ALTER TABLE). This is also the default lock mode for LOCK TABLE statements that do not specify a mode explicitly.</p>\n</blockquote>\n\n<p>From the docs, this lock is not typically triggered by an <code>UPDATE</code>, so what gives? Grepping through the docs showed me that an <code>UPDATE</code> should trigger a <code>ROW SHARE</code> lock:</p>\n\n<pre><code>ROW EXCLUSIVE\nConflicts with the SHARE, SHARE ROW EXCLUSIVE, EXCLUSIVE, and ACCESS EXCLUSIVE lock modes.\n\nThe commands UPDATE, DELETE, and INSERT acquire this lock mode on the target table (in addition to ACCESS SHARE locks on any other referenced tables). In general, this lock mode will be acquired by any command that modifies data in a table.\n</code></pre>\n\n<blockquote>\n<p>A database engineer directly told me what kind of lock an <code>UPDATE</code> should use, but you could find it in the docs if you don't have access to some excellent database professionals.</p>\n</blockquote>\n\n<p>Mostly what happens when you try to <code>UPDATE</code> is that Postgres will acquire a lock on the row that you want to change. If you have two update statements running at the same time on the same row, then the second must wait for the first to process. So why on earth, if an <code>UPDATE</code> is supposed only to take out a row lock, was my query taking out a lock against the whole table?</p>\n<h2 class=\"anchored\">\n  <a name=\"unmasking-a-locking-mystery\" href=\"#unmasking-a-locking-mystery\">Unmasking a locking mystery</a>\n</h2>\n\n<p>I would love to tell you that I have a really great debugging tool to tell you about here, but I mostly duck-duck-go-ed (searched) a ton and eventually found <a href=\"https://grokbase.com/t/postgresql/pgsql-general/124s02j3jy/updates-sharelocks-rowexclusivelocks-and-deadlocks\">this forum post</a>. In the post someone is complaining about a similar behavior, they're using an update but are seeing more aggressive lock being used sometimes.</p>\n\n<p>Based on the responses to the forum it sounded like if there is more than a few <code>UPDATE</code> queries that are trying to modify the same row at the same time what happens is that one of the queries will try to acquire the lock, see it is taken then it will instead acquire a larger lock on the table. Postgres queues locks, so if this happens for multiple rows with similar contention, then multiple queries would be taking out locks on the whole table, which somewhat could explain the behavior I was seeing. It seemed plausible, but why was there such a problem?</p>\n\n<p>I combed over my codebase and couldn't find anything. Then as I was laying down to go to bed that evening, I had a moment of inspiration where I remembered that we were updating the database in parallel for the same UUID using threads:</p>\n\n<pre><code class=\"language-ruby\">@things.map do |thing|\n  Concurrent::Promise.execute(executor: :fast) do\n    store_results!(thing)\n  end\nend.each(&amp;:value!)\n</code></pre>\n\n<p>In every loop, we were creating a promise that would concurrently update values (using a thread pool). Due to a design decision from years ago, each loop causes an <code>UPDATE</code> to the same row in the database for each job being run. This programming pattern was never a problem before because, as I mentioned earlier, there's another table with more than 200x the number of records, so we've never had any issues with this scheme until recently.</p>\n\n<p>With this new theory, I removed the concurrency, which meant that each <code>UPDATE</code> call would be sequential instead of in parallel:</p>\n\n<pre><code class=\"language-ruby\">@things.map do |thing|\n  store_results!(thing)\nend\n</code></pre>\n\n<p>While the code is less efficiently in the use of IO on the Ruby program, it means that the chance that the same row will try to be updated at the same time is drastically decreased.</p>\n\n<p>I manually killed the long-running locked queries using <code>SELECT pg_cancel_backend(&lt;pid&gt;);</code> and I deployed this change (in the morning after a code review).</p>\n\n<p>Once the old stuck queries were aborted, and the new code was in place, then the system promptly got back up and running, churning through plenty of backlog.</p>\n<h2 class=\"anchored\">\n  <a name=\"locks-and-stuff\" href=\"#locks-and-stuff\">Locks and stuff</a>\n</h2>\n\n<p>While this somewhat obscure debugging story might not be directly relevant to your database, here are some things you can take away from this article. Your database has locks (think mutexes but with varying scope), and those locks can mess up your day if they're doing something different than you're expecting. You can see the locks that your database is currently using by running the <code>heroku pg:locks</code> command (may need to install the <code>pg:extras</code> plugin). You can also see which queries are taking out which locks using the SQL query I posted earlier.</p>\n\n<p>The next thing I want to cover is documentation. If it weren't for several very experienced Postgres experts and a seemingly random forum post about how multiple <code>UPDATE</code> statements can trigger a more aggressive lock type, then I never would have figured this out. If you're familiar with the Postgres documentation, is this behavior written down anywhere? If so, then could we make it easier to find or understand somehow? If it's not written down, can you help me document it? I don't mind writing documentation, but I'm not totally sure what the expected behavior is. For instance, why does a lock queue for a row that goes above a specific threshold trigger a table lock? And what exactly is that threshold? I'm sure this behavior makes total sense from an implementation point of view, but as an end-user, I would like it to be spelled out and officially documented.</p>\n\n<p>I hope you either learned a thing or two or at least got a kick out of my misery. This issue was a pain to debug, but in hindsight, a quirky bug to blog about. Thanks for reading!</p>\n\n<p>And to learn about another potential database issue, check out this other blog post by Heroku Engineer Ben Fritsch, <a href=\"https://blog.heroku.com/know-your-database-types\">Know Your Database Types</a>.</p>\n\n<p>Special thanks to <a href=\"https://github.com/mble\">Matthew Blewitt</a> and <a href=\"https://github.com/andscoop\">Andy Cooper</a> for helping me debug this!</p>","PublishedAt":"2019-12-18 18:07:00+00:00","OriginURL":"https://blog.heroku.com/curious-case-table-locking-update-query","SourceName":"Heroku"}},{"node":{"ID":369,"Title":"Let It Crash: Best Practices for Handling Node.js Errors on Shutdown","Description":"<p><em>This blog post is adapted from a talk given by Julián Duque at NodeConf EU 2019 titled \"<a href=\"https://youtu.be/Fguac8pIAtU\">Let it crash!</a>.\"</em></p>\n\n<p>Before coming to Heroku, I did some consulting work as a Node.js solutions architect. My job was to visit various companies and make sure that they were successful in designing production-ready Node applications. Unfortunately, I witnessed many different problems when it came to error handling, especially on process shutdown. When an error occurred, there was often not enough visibility on why it happened, a lack of logging details, and bouts of downtime as applications attempted to recover from crashes.</p>\n\n<div class=\"embedded-video-wrapper\">\n<iframe title=\"Let it Crash!\" src=\"https://www.youtube-nocookie.com/embed/Fguac8pIAtU\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n\n\n<p>We started to assemble a collection of best practices and recommendations on error handling, to ensure they were aligned with the overall Node.js community. In this post, I'll walk through some of the background on the Node.js process lifecycle and some strategies to properly handle graceful shutdown and quickly restart your application after a catastrophic error terminates your program. </p>\n<h2 class=\"anchored\">\n  <a name=\"the-node-js-process-lifecycle\" href=\"#the-node-js-process-lifecycle\">The Node.js process lifecycle</a>\n</h2>\n\n<p>Let's first explore briefly how Node.js operates. A Node.js process is very lightweight and has a small memory footprint. Because crashes are an inevitable part of programming, your primary goal when architecting an application is to keep the startup process very lean, so that your application can quickly boot up. If your startup operations include CPU intensive work or synchronous operations, it might affect the ability of your Node.js processes to quickly restart.</p>\n\n<p>A strategy you can use here is to prebuild as much as possible. That might mean preparing data or compiling assets during the building process. It may increase your deployment times, but it's better to spend more time outside of the startup process. Ultimately, this ensures that when a crash does happen, you can exit a process and start a new one without much downtime.</p>\n<h3 class=\"anchored\">\n  <a name=\"node-js-exit-methods\" href=\"#node-js-exit-methods\">Node.js exit methods</a>\n</h3>\n\n<p>Let's take a look at several ways you can terminate a Node.js process and the differences between them.</p>\n\n<p>The most common function to use is <a href=\"https://nodejs.org/api/process.html#process_process_exit_code\"><code>process.exit()</code></a>, which takes a single argument, an integer. If the argument is <code>0</code>, it represents a successful exit state. If it's greater than that, it indicates that an error occurred; <code>1</code> is a common exit code for failures here.</p>\n\n<p>Another option is <a href=\"https://nodejs.org/api/process.html#process_process_abort\"><code>process.abort()</code></a>. When this method is called, the Node.js process terminates immediately. More importantly, if your operating system allows it, Node will also generate a core dump file, which contains a ton of useful information about the process. You can use this core dump to do some postmortem debugging using tools like <a href=\"https://github.com/nodejs/llnode\"><code>llnode</code></a>.</p>\n<h3 class=\"anchored\">\n  <a name=\"node-js-exit-events\" href=\"#node-js-exit-events\">Node.js exit events</a>\n</h3>\n\n<p>As Node.js is built on top of JavaScript, it has an event loop, which allows you to listen for events that occur and act on them. When Node.js exits, it also emits several types of events.</p>\n\n<p>One of these is <code>beforeExit</code>, and as its name implies, it is emitted right before a Node process exits. You can provide an event handler which can make asynchronous calls, and the event loop will continue to perform the work until it's all finished. It's important to note that this event is <em>not</em> emitted on <code>process.exit()</code> calls or <code>uncaughtException</code>s; we'll get into when you might use this event a little later.</p>\n\n<p>Another event is <code>exit</code>, which is emitted only when <code>process.exit()</code> is explicitly called. As it fires after the event loop has been terminated, you can't do any asynchronous work in this handler.</p>\n\n<p>The code sample below illustrates the differences between the two events:</p>\n\n<pre><code class=\"language-js\">process.on('beforeExit', code =&gt; {\n  // Can make asynchronous calls\n  setTimeout(() =&gt; {\n    console.log(`Process will exit with code: ${code}`)\n    process.exit(code)\n  }, 100)\n})\n\nprocess.on('exit', code =&gt; {\n  // Only synchronous calls\n  console.log(`Process exited with code: ${code}`)\n})\n</code></pre>\n<h3 class=\"anchored\">\n  <a name=\"os-signal-events\" href=\"#os-signal-events\">OS signal events</a>\n</h3>\n\n<p>Your operating system emits events to your Node.js process, too, depending on the circumstances occurring outside of your program. These are referred to as <a href=\"https://en.wikipedia.org/wiki/Signal_(IPC)\">signals</a>. Two of the more common signals are <code>SIGTERM</code> and <code>SIGINT</code>.</p>\n\n<p><code>SIGTERM</code> is normally sent by a process monitor to tell Node.js to expect a successful termination. If you're running <code>systemd</code> or <code>upstart</code> to manage your Node application, and you stop the service, it sends a <code>SIGTERM</code> event so that you can handle the process shutdown.</p>\n\n<p><code>SIGINT</code> is emitted when a Node.js process is interrupted, usually as the result of a control-C (<code>^-C</code>) keyboard event. You can also capture that event and do some work around it.</p>\n\n<p>Here is an example showing how you may act on these signal events:</p>\n\n<pre><code class=\"language-js\">process.on('SIGTERM', signal =&gt; {\n  console.log(`Process ${process.pid} received a SIGTERM signal`)\n  process.exit(0)\n})\n\nprocess.on('SIGINT', signal =&gt; {\n  console.log(`Process ${process.pid} has been interrupted`)\n  process.exit(0)\n})\n</code></pre>\n\n<p>Since these two events are considered a successful termination, we call <code>process.exit</code> and pass an argument of <code>0</code> because it is something that is expected.</p>\n<h3 class=\"anchored\">\n  <a name=\"javascript-error-events\" href=\"#javascript-error-events\">JavaScript error events</a>\n</h3>\n\n<p>At last, we arrive at higher-level error types: the error events thrown by JavaScript itself.</p>\n\n<p>When a JavaScript error is not properly handled, an <code>uncaughtException</code> is emitted. These suggest the programmer has made an error, and they should be treated with the utmost priority. Usually, it means a bug occurred on a piece of logic that needed more testing, such as calling a method on a <code>null</code> type.</p>\n\n<p>An <code>unhandledRejection</code> error is a newer concept. It is emitted when a <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise\">promise</a> is not satisfied; in other words, a promise was rejected (it failed), and there was no handler attached to respond. These errors can indicate an operational error or a programmer error, and they should also be treated as high priority.</p>\n\n<p>In both of these cases, you should do something counterintuitive and <strong>let your program crash</strong>! Please don't try to be clever and introduce some complex logic trying to prevent a process restart. Doing so will almost always leave your application in a bad state, whether that's having a memory leak or leaving sockets hanging. It's simpler to let it crash, start a new process from scratch, and continue receiving more requests.</p>\n\n<p>Here's some code indicating how you might best handle these events:</p>\n\n<pre><code class=\"language-js\">process.on('uncaughtException', err =&gt; {\n  console.log(`Uncaught Exception: ${err.message}`)\n  process.exit(1)\n})\n</code></pre>\n\n<p>We’re explicitly “crashing” the Node.js process here! Don’t be afraid of this! It is more likely than not unsafe to continue. The <a href=\"https://nodejs.org/api/process.html#process_warning_using_uncaughtexception_correctly\">Node.js documentation</a> says,</p>\n\n<blockquote>\n<p>Unhandled exceptions inherently mean that an application is in an undefined state...The correct use of 'uncaughtException' is to perform synchronous cleanup of allocated resources (e.g. file descriptors, handles, etc) before shutting down the process. It is not safe to resume normal operation after 'uncaughtException'.</p>\n</blockquote>\n\n<pre><code class=\"language-js\">process.on('unhandledRejection', (reason, promise) =&gt; {\n  console.log('Unhandled rejection at ', promise, `reason: ${err.message}`)\n  process.exit(1)\n})\n</code></pre>\n\n<p><code>unhandledRejection</code> is such a common error, that the Node.js maintainers have decided it should really crash the process, and they warn us that in a future version of Node.js <code>unhandledRejection</code>s will crash the process.</p>\n\n<blockquote>\n<p>[DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.</p>\n</blockquote>\n<h2 class=\"anchored\">\n  <a name=\"run-more-than-one-process\" href=\"#run-more-than-one-process\">Run more than one process</a>\n</h2>\n\n<p>Even if your process startup time is extremely quick, running just a single process is a risk to safe and uninterrupted application operation. We recommend running more than one process and to use a load balancer to handle the scheduling. That way, if one of the processes crashes, there is another process that is alive and able to receive new requests. This is going to give you a little bit more leverage and prevent downtime.</p>\n\n<p>Use whatever you have on-hand for the load balancing. You can configure a reverse proxy like nginx or HAProxy to do this. If you're on Heroku, you can <a href=\"https://devcenter.heroku.com/articles/how-heroku-works#http-routing\">scale your application</a> to increase the number of dynos. If you're on Kubernetes, you can use <a href=\"https://kubernetes.io/docs/concepts/services-networking/ingress/\">Ingress</a> or other load balancer strategies for your application.</p>\n<h2 class=\"anchored\">\n  <a name=\"monitor-your-processes\" href=\"#monitor-your-processes\">Monitor your processes</a>\n</h2>\n\n<p>You should have process monitoring in-place, something running in your operating system or an application environment that's constantly checking if your Node.js process is alive or not. If the process crashes due to a failure, the process monitor is in charge of restarting the process.</p>\n\n<p>Our recommendation is to always use the native process monitoring that's available on your operating system. For example, if you're running on Unix or Linux, you can use the <a href=\"https://en.wikipedia.org/wiki/Systemd\"><code>systemd</code></a> or <a href=\"https://en.wikipedia.org/wiki/Upstart_(software)\"><code>upstart</code></a> commands. If you're using containers, Docker has a <a href=\"https://docs.docker.com/engine/reference/run/#restart-policies---restart\"><code>--restart</code> flag</a>, and Kubernetes has <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy\"><code>restartPolicy</code></a>, both of which are useful.</p>\n\n<p>If you can't use any existing tools, use a Node.js process monitor like <a href=\"https://pm2.keymetrics.io/\">PM2</a> or <a href=\"https://github.com/foreversd/forever\">forever</a> as a last resort. These tools are okay for development environments, but I can't really recommend them for production use.</p>\n\n<p>If your application is running on Heroku, don’t worry—we take care of the restart for you!</p>\n<h2 class=\"anchored\">\n  <a name=\"graceful-shutdowns\" href=\"#graceful-shutdowns\">Graceful shutdowns</a>\n</h2>\n\n<p>Let's say we have a server running. It's receiving requests and establishing connections with clients. But what happens if the process crashes?  If we're not performing a graceful shutdown, some of those sockets are going to hang around and keep waiting for a response until a timeout has been reached. That unnecessary time spent consumes resources, eventually leading to downtime and a degraded experience for your users.</p>\n\n<p>It's best to explicitly stop receiving connections, so that the server can disconnect connections while it's recovering. Any new connections will go to the other Node.js processes running through the load balancer</p>\n\n<p>To do this, you can call <a href=\"https://nodejs.org/api/http.html#http_server_close_callback\"><code>server.close()</code></a>, which tells the server to stop accepting new connections. Most Node servers implement this class, and it accepts a callback function as an argument.</p>\n\n<p>Now, imagine that your server has many clients connected, and the majority of them have not experienced an error or crashed. How can you close the server while not abruptly disconnecting valid clients? We'll need to use a timeout to build a system to indicate that if all the connections don't close within a certain limit, we will completely shutdown the server. We do this because we want to give existing, healthy clients time to finish up but don't want the server to wait for an excessively long time to shutdown.</p>\n\n<p>Here's some sample code of what that might look like:</p>\n\n<pre><code class=\"language-js\">process.on('&lt;signal or error event&gt;', _ =&gt; {\n  server.close(() =&gt; {\n    process.exit(0)\n  })\n  // If server hasn't finished in 1000ms, shut down process\n  setTimeout(() =&gt; {\n    process.exit(0)\n  }, 1000).unref() // Prevents the timeout from registering on event loop\n})\n</code></pre>\n<h2 class=\"anchored\">\n  <a name=\"logging\" href=\"#logging\">Logging</a>\n</h2>\n\n<p>Chances are you have already implemented a robust logging strategy for your running application, so I won't get into it too much about that here. Just remember to log with the same rigorous quality and amount of information for when the application shuts down!</p>\n\n<p>If a crash occurs, log as much relevant information as possible, including the errors and stack trace. Rely on libraries like <a href=\"https://github.com/pinojs/pino\"><code>pino</code></a> or <a href=\"https://github.com/winstonjs/winston\"><code>winston</code></a> in your application, and store these logs using one of their transports for better visibility. You can also take a look at <a href=\"https://elements.heroku.com/addons#logging\">our various logging add-ons</a> to find a provider which matches your application’s needs.</p>\n<h2 class=\"anchored\">\n  <a name=\"make-sure-everything-is-still-good\" href=\"#make-sure-everything-is-still-good\">Make sure everything is still good</a>\n</h2>\n\n<p>Last, and certainly not least, we recommend that you add a health check route. This is a simple endpoint that returns a <code>200</code> status code if your application is running:</p>\n\n<pre><code class=\"language-js\">// Add a health check route in express\napp.get('/_health', (req, res) =&gt; {\n  res.status(200).send('ok')\n})\n</code></pre>\n\n<p>You can have a separate service continuously monitor that route. You can configure this in a number of ways, whether by using a reverse proxy, such as nginx or HAProxy, or a load balancer, like ELB or ALB.</p>\n\n<p>Any application that acts as the top layer of your Node.js process can be used to constantly monitor that the health check is returning. These will also give you way more visibility around the health of your Node.js processes, and you can rest easy knowing that your Node processes are running properly. There are some great great monitoring services to help you with this in the <a href=\"https://elements.heroku.com/addons#monitoring\">Add-ons section of our Elements Marketplace</a>.</p>\n<h2 class=\"anchored\">\n  <a name=\"putting-it-all-together\" href=\"#putting-it-all-together\">Putting it all together</a>\n</h2>\n\n<p>Whenever I work on a new Node.js project, I use the same function to ensure that my crashes are logged and my recoveries are guaranteed. It looks something like this:</p>\n\n<pre><code class=\"language-js\">function terminate (server, options = { coredump: false, timeout: 500 }) {\n  // Exit function\n  const exit = code =&gt; {\n    options.coredump ? process.abort() : process.exit(code)\n  }\n\n  return (code, reason) =&gt; (err, promise) =&gt; {\n    if (err &amp;&amp; err instanceof Error) {\n    // Log error information, use a proper logging library here :)\n    console.log(err.message, err.stack)\n    }\n\n    // Attempt a graceful shutdown\n    server.close(exit)\n    setTimeout(exit, options.timeout).unref()\n  }\n}\n\nmodule.exports = terminate\n</code></pre>\n\n<p>Here, I've created a module called <code>terminate</code>. I pass the instance of that server that I'm going to be closing, and some configuration options, such as whether I want to enable core dumps, as well as the timeout. I usually use an environment variable to control when I want to enable a core dump. I enable them only when I am going to do some performance testing on my application or whenever I want to replicate the error.</p>\n\n<p>This exported function can then be set to listen to our error events:</p>\n\n<pre><code class=\"language-js\">const http = require('http')\nconst terminate = require('./terminate')\nconst server = http.createServer(...)\n\nconst exitHandler = terminate(server, {\n  coredump: false,\n  timeout: 500\n})\n\nprocess.on('uncaughtException', exitHandler(1, 'Unexpected Error'))\nprocess.on('unhandledRejection', exitHandler(1, 'Unhandled Promise'))\nprocess.on('SIGTERM', exitHandler(0, 'SIGTERM'))\nprocess.on('SIGINT', exitHandler(0, 'SIGINT'))\n</code></pre>\n<h2 class=\"anchored\">\n  <a name=\"additional-resources\" href=\"#additional-resources\">Additional resources</a>\n</h2>\n\n<p>There are a number of existing npm modules that pretty much solve the aforementioned issues in a similar ways. You can check these out as well:</p>\n\n<ul>\n<li><a href=\"https://github.com/godaddy/terminus\">@godaddy/terminus</a></li>\n<li><a href=\"https://github.com/hunterloftis/stoppable\">stoppable</a></li>\n<li><a href=\"https://github.com/sebhildebrandt/http-graceful-shutdown\">http-graceful-shutdown</a></li>\n</ul>\n\n<p>Hopefully, this information will simplify your life and enable your Node app to run better and safer in production!</p>","PublishedAt":"2019-12-17 18:12:00+00:00","OriginURL":"https://blog.heroku.com/best-practices-nodejs-errors","SourceName":"Heroku"}}]}},"pageContext":{"source":"Heroku"}},"staticQueryHashes":["3649515864"]}